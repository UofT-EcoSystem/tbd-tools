commit adf0d997d821a82bfbe36f40c68c519d56a4cb52
Author: Abhishek Tiwari <abtiwari94@gmail.com>
Date:   Tue Mar 20 15:46:02 2018 -0400

    patched memprofiler to v0.12.0

diff --git a/example/speech_recognition/deepspeech.cfg b/example/speech_recognition/deepspeech.cfg
index 4f0f496..ff519b4 100644
--- a/example/speech_recognition/deepspeech.cfg
+++ b/example/speech_recognition/deepspeech.cfg
@@ -2,13 +2,13 @@
 # method can be one of the followings - train,predict,load
 mode = train
 #ex: gpu0,gpu1,gpu2,gpu3
-context = gpu0,gpu1,gpu2
+context = gpu0
 #context = gpu0
 # checkpoint prefix, check point will be saved under checkpoints folder with prefix
 prefix = deep_bucket
 # when mode is load or predict, model will be loaded from the file name with model_file under checkpoints
 model_file = deep_bucketn_epoch0n_batch-0018
-batch_size = 12
+batch_size = 3
 #batch_size=4
 # log will be saved by the log_filename
 log_filename = deep_bucket.log
@@ -24,9 +24,9 @@ kvstore_option = device
 
 [data]
 max_duration = 16.0
-train_json = ./train_corpus_all.json
-test_json = ./test_corpus.json
-val_json = ./test_corpus.json
+train_json = /home/serailhydra/ba-dls-deepspeech/train_corpus.json
+test_json = /home/serailhydra/ba-dls-deepspeech/test_corpus.json
+val_json = /home/serailhydra/ba-dls-deepspeech/test_corpus.json
 
 language = en
 width = 161
diff --git a/example/speech_recognition/default.cfg b/example/speech_recognition/default.cfg
index 127c492..0013c89 100644
--- a/example/speech_recognition/default.cfg
+++ b/example/speech_recognition/default.cfg
@@ -7,7 +7,7 @@ context = gpu0
 prefix = test_fc
 # when mode is load or predict, model will be loaded from the file name with model_file under checkpoints
 model_file = test_fc-0040
-batch_size = 2
+batch_size = 3
 # log will be saved by the log_filename
 log_filename = test.log
 # checkpoint set n to save checkpoints after n epoch
diff --git a/example/speech_recognition/stt_io_bucketingiter.py b/example/speech_recognition/stt_io_bucketingiter.py
index 41b93f3..3e16cce 100644
--- a/example/speech_recognition/stt_io_bucketingiter.py
+++ b/example/speech_recognition/stt_io_bucketingiter.py
@@ -76,9 +76,31 @@ class BucketSTTIter(mx.io.DataIter):
                             "Must be train/validation/test")
         # if sortagrad
         if sort_by_duration:
+            pass
             durations, audio_paths, texts = datagen.sort_by_duration(durations,
                                                                      audio_paths,
                                                                      texts)
+
+            print(durations[0])
+            print(audio_paths[0])
+            print(texts[0])
+
+            durations_l = list(durations)
+            audio_paths_l = list(audio_paths)
+            texts_l = list(texts)
+
+            for i in range(len(durations) / 2):
+                durations_l[i], durations_l[len(durations) - i - 1] = durations_l[len(durations) - i - 1], durations_l[i]
+                audio_paths_l[i], audio_paths_l[len(durations) - i - 1] = audio_paths_l[len(durations) - i - 1], audio_paths_l[i]
+                texts_l[i], texts_l[len(durations) - i - 1] = texts_l[len(durations) - i - 1], texts_l[i]
+
+            durations = tuple(durations_l)
+            audio_paths = tuple(audio_paths_l)
+            texts = tuple(texts_l)
+
+            print(durations[0])
+            print(audio_paths[0])
+            print(texts[0])
         else:
             durations = durations
             audio_paths = audio_paths
diff --git a/include/mxnet/base.h b/include/mxnet/base.h
index cceee70..fec5cf6 100644
--- a/include/mxnet/base.h
+++ b/include/mxnet/base.h
@@ -34,6 +34,9 @@
 #include <nnvm/tuple.h>
 #include <nnvm/symbolic.h>
 #include <string>
+#include <stdio.h>
+#include <stdlib.h>
+#include <execinfo.h>
 
 /*!
  *\brief whether to use opencv support
@@ -122,6 +125,8 @@
  */
 #define PROFILER_MESSAGE_FUNCNAME PROFILER_MESSAGE(__FUNCTION__)
 
+#define MXNET_MEMORY_PROFILER_ON 1
+
 /*! \brief namespace of mxnet */
 namespace mxnet {
 /*! \brief mxnet cpu */
@@ -145,12 +150,22 @@ struct Context {
     kGPU = gpu::kDevMask,
     kCPUPinned = 3
   };
+#if MXNET_MEMORY_PROFILER_ON
+  /*! \brief the name of the output of the execution (for memory profiling) */
+  std::wstring name = L"warning!,ctx_source_unclear";
+
+  inline void set_name(std::wstring name_) {
+    name = name_;
+  }
+#endif
+
   /*! \brief the device type we run the op on */
   DeviceType dev_type;
   /*! \brief device id we are going to run it on */
   int32_t dev_id;
   /*! \brief default constructor */
-  Context() : dev_type(kCPU), dev_id(0) {}
+  Context() : dev_type(kCPU), dev_id(0) {
+  }
   /*!
    * \brief Get corresponding device mask
    * \return cpu::kDevMask or gpu::kDevMask
@@ -208,7 +223,14 @@ struct Context {
    * \param dev_type device type.
    * \param dev_id device id. -1 for current device.
    */
-  inline static Context Create(DeviceType dev_type, int32_t dev_id = -1);
+//  inline static Context Create(DeviceType dev_type, int32_t dev_id = -1);
+#if MXNET_MEMORY_PROFILER_ON
+  inline static Context Create(DeviceType dev_type,
+                               int32_t dev_id = -1,
+                               std::string context_name = std::string("untagged"));
+#else
+    inline static Context Create(DeviceType dev_type, int32_t dev_id = -1);
+#endif
   /*! \return CPU Context */
   inline static Context CPU(int32_t dev_id = 0);
   /*!
@@ -255,6 +277,16 @@ struct RunContext {
   inline const Context& get_ctx() const {
     return ctx;
   }
+#if MXNET_MEMORY_PROFILER_ON
+  inline void set_name(std::wstring name) {
+    ctx.set_name(name);
+  }
+  inline void set_name(char* name) {
+    std::vector<char> sname(name, name + strlen(name));
+    std::wstring wname(sname.begin(), sname.end());
+    set_name(wname);
+  }
+#endif
 };
 }  // namespace mxnet
 
@@ -268,9 +300,21 @@ inline bool Context::operator<(const Context &b) const {
     return dev_type < b.dev_type;
   }
 }
+
+#if MXNET_MEMORY_PROFILER_ON
+inline Context Context::Create(DeviceType dev_type,
+                               int32_t dev_id,
+                               std::string name) {
+#else
 inline Context Context::Create(DeviceType dev_type, int32_t dev_id) {
+#endif
+
   Context ctx;
   ctx.dev_type = dev_type;
+#if MXNET_MEMORY_PROFILER_ON
+  ctx.set_name(std::wstring(name.begin(),name.end()));
+#endif
+
   if (dev_id < 0) {
     ctx.dev_id = 0;
     if (dev_type != kCPU) {
@@ -299,11 +343,18 @@ inline Context Context::GPU(int32_t dev_id) {
 
 inline Context Context::FromString(std::string str) {
   Context ret;
+//  fprintf(stderr,"[trace_base] in FromString : %s\n", str.c_str());
   try {
     std::string::size_type l = str.find('(');
     CHECK_NE(l, std::string::npos);
     std::string::size_type r = str.find(')');
+#if MXNET_MEMORY_PROFILER_ON
+    /*Set the memory profiling tag from the context string*/
+    // @TODO: @ABHISHEK: Possible unsafe because no check on total length
+    // come up with better solution
+#else
     CHECK_EQ(r, str.length()-1);
+#endif
 
     std::string type = str.substr(0, l);
     int id = std::stoi(str.substr(l+1, r-l-1));
@@ -311,6 +362,10 @@ inline Context Context::FromString(std::string str) {
       ret = CPU(id);
     } else if (type == "gpu") {
       ret = GPU(id);
+#if MXNET_MEMORY_PROFILER_ON
+      std::string tag = str.substr(r+1, str.length()-1);
+      ret.set_name(std::wstring(tag.begin(),tag.end()));
+#endif
     } else if (type == "cpu_pinned") {
       ret = CPUPinned(id);
     } else {
diff --git a/include/mxnet/c_api.h b/include/mxnet/c_api.h
index 55b840d..e0beff1 100644
--- a/include/mxnet/c_api.h
+++ b/include/mxnet/c_api.h
@@ -180,6 +180,8 @@ typedef int (*CustomFunctionBwdFunc)(int /*num_ograds*/, int /*num_igrads*/, voi
                                      void* /*state*/);
 typedef int (*CustomFunctionDelFunc)(void* /*state*/);
 
+MXNET_DLL int MXShowAvailableGPUMem();
+
 /*!
  * \brief return str message of the last error
  *  all function in this file will return 0 when success
@@ -286,7 +288,13 @@ MXNET_DLL int MXNDArrayCreateEx(const mx_uint *shape,
                               int dev_id,
                               int delay_alloc,
                               int dtype,
-                              NDArrayHandle *out);
+#if MXNET_MEMORY_PROFILER_ON
+                              NDArrayHandle *out,
+                              char const* name
+#else
+                              NDArrayHandle *out
+#endif
+                              );
 
 
 /*!
diff --git a/include/mxnet/executor.h b/include/mxnet/executor.h
index 85d3477..103018d 100644
--- a/include/mxnet/executor.h
+++ b/include/mxnet/executor.h
@@ -128,9 +128,15 @@ class Executor {
   static Executor* SimpleBind(nnvm::Symbol symbol,
                               const Context& default_ctx,
                               const std::map<std::string, Context>& group2ctx,
+#if MXNET_MEMORY_PROFILER_ON
+                              std::vector<Context>& in_arg_ctxes,
+                              std::vector<Context>& arg_grad_ctxes,
+                              std::vector<Context>& aux_state_ctxes,
+#else
                               const std::vector<Context>& in_arg_ctxes,
                               const std::vector<Context>& arg_grad_ctxes,
                               const std::vector<Context>& aux_state_ctxes,
+#endif
                               const std::unordered_map<std::string, TShape>& arg_shape_map,
                               const std::unordered_map<std::string, int>& arg_dtype_map,
                               const std::unordered_map<std::string, int>& arg_stype_map,
diff --git a/include/mxnet/ndarray.h b/include/mxnet/ndarray.h
index 84ee9fa..c18963e 100644
--- a/include/mxnet/ndarray.h
+++ b/include/mxnet/ndarray.h
@@ -37,6 +37,7 @@
 #include "./base.h"
 #include "./storage.h"
 #include "./engine.h"
+
 #if MKL_EXPERIMENTAL == 1
 #include <mkl_memory.h>
 #endif
@@ -171,7 +172,6 @@ class NDArray {
 #endif
   }
 
-
   /*!
    * \return the shape of current NDArray.
    */
@@ -229,8 +229,14 @@ class NDArray {
   /*!
    * \return the data TBlob
    */
+#if MXNET_MEMORY_PROFILER_ON
+  inline const TBlob& data() const {
+    /*@ABHISHEK: added the if condition to mirror the v0.12.0 code*/
+    if (storage_type() == kDefaultStorage) CheckAndAlloc(ptr_->shandle.ctx.name);
+#else
   inline const TBlob& data() const {
     if (storage_type() == kDefaultStorage) CheckAndAlloc();
+#endif
     SetTBlob();
     return tblob_;
   }
@@ -534,10 +540,19 @@ class NDArray {
    * \brief Allocate the space if it is delayed allocated.
    * This is an internal function used by system that normal user should not use
    */
+  /*@ABHISHEK: when hongyu modified this file, it had 2 definitions for CheckAndAlloc, now it has 8,
+   * I will make similar modifications to all 8 invocations.*/
+#if MXNET_MEMORY_PROFILER_ON
+  inline void CheckAndAlloc(std::wstring name) const {
+    CHECK_EQ(storage_type(), kDefaultStorage);
+    ptr_->CheckAndAlloc(name);
+  }
+#else
   inline void CheckAndAlloc() const {
     CHECK_EQ(storage_type(), kDefaultStorage);
     ptr_->CheckAndAlloc();
   }
+#endif
 
   /*!
    * \brief Allocate the space if the allocation has been delayed
@@ -548,32 +563,67 @@ class NDArray {
    * with CheckAndAlloc(const std::vector<TShape> &aux_shapes), since
    * TShape tmp = some_shape is equivalent to TShape tmp = {some_shape}.
    */
+#if MXNET_MEMORY_PROFILER_ON
+  void ReshapeAndAlloc(const TShape& shape, std::wstring name) {
+    CHECK_EQ(storage_type(), kDefaultStorage);
+    CHECK(!is_none());
+    shape_ = shape;
+    ptr_->CheckAndAlloc(shape.Size() * mshadow::mshadow_sizeof(dtype_), name);
+  }
+#else
   void ReshapeAndAlloc(const TShape& shape) {
     CHECK_EQ(storage_type(), kDefaultStorage);
     CHECK(!is_none());
     shape_ = shape;
     ptr_->CheckAndAlloc(shape.Size() * mshadow::mshadow_sizeof(dtype_));
   }
+#endif
 
   /* !
    * \brief Alloc memory for non-default storage
    * aux_shape is only known at run time
    */
+  /*@ABHISHEK: Adding similar mods as Hongyu to all these CheckAndAlloc calls*/
+#if MXNET_MEMORY_PROFILER_ON
+  inline void CheckAndAlloc(const std::vector<TShape> &aux_shapes, std::wstring name) const {
+    CHECK_NE(storage_type(), kDefaultStorage)
+             << "CheckAndAlloc(aux_shapes) is not intended for kDefaultStorage";
+    ptr_->CheckAndAlloc(shape_, aux_shapes, dtype_, name);
+  }
+#else
   inline void CheckAndAlloc(const std::vector<TShape> &aux_shapes) const {
     CHECK_NE(storage_type(), kDefaultStorage)
              << "CheckAndAlloc(aux_shapes) is not intended for kDefaultStorage";
     ptr_->CheckAndAlloc(shape_, aux_shapes, dtype_);
   }
+#endif
+#if MXNET_MEMORY_PROFILER_ON
+  inline void CheckAndAllocData(const TShape &storage_shape, std::wstring name) const {
+    CHECK_NE(storage_type(), kDefaultStorage)
+             << "CheckAndAllocData is not intended for kDefaultStorage";
+    ptr_->CheckAndAllocData(storage_shape, dtype_, name);
+  }
+#else
   inline void CheckAndAllocData(const TShape &storage_shape) const {
     CHECK_NE(storage_type(), kDefaultStorage)
              << "CheckAndAllocData is not intended for kDefaultStorage";
     ptr_->CheckAndAllocData(storage_shape, dtype_);
   }
+#endif
+#if MXNET_MEMORY_PROFILER_ON
+  inline void CheckAndAllocAuxData(size_t i, const TShape &aux_shape, std::wstring name) const {
+    CHECK_NE(storage_type(), kDefaultStorage)
+             << "CheckAndAllocAuxData is not intended for kDefaultStorage";
+    ptr_->CheckAndAllocAuxData(i, aux_shape, name);
+  }
+#else
   inline void CheckAndAllocAuxData(size_t i, const TShape &aux_shape) const {
     CHECK_NE(storage_type(), kDefaultStorage)
              << "CheckAndAllocAuxData is not intended for kDefaultStorage";
     ptr_->CheckAndAllocAuxData(i, aux_shape);
   }
+#endif
+
   /*!
    * \brief Save list of ndarray into the Stream.x
    * \param fo The stream of output.
@@ -645,7 +695,12 @@ class NDArray {
       var = Engine::Get()->NewVariable();
       shandle.size = size * mshadow::mshadow_sizeof(dtype);
       shandle.ctx = ctx_;
+      /*@ABHISHEK* Are we passing the correct thing here?*/
+#if MXNET_MEMORY_PROFILER_ON
+      if (!delay_alloc_) this->CheckAndAlloc(ctx_.name);
+#else
       if (!delay_alloc_) this->CheckAndAlloc();
+#endif
     }
 
     Chunk(const TBlob &data, int dev_id)
@@ -675,13 +730,22 @@ class NDArray {
       var = Engine::Get()->NewVariable();
       // aux_handles always reflect the correct number of aux data
       for (size_t i = 0; i < aux_shapes.size(); i++) {
+        /*@ABHISHEK: Need to modify all calls to and definitions of this helper function as well*/
+#if MXNET_MEMORY_PROFILER_ON
+        CheckAndAllocAuxData(i, aux_shapes[i], ctx_.name);
+#else
         CheckAndAllocAuxData(i, aux_shapes[i]);
+#endif
         // this line is needed in case when aux_shapes[i].Size() = 0
         // aux_handles[i] will not be updated and take only default value.
         aux_handles[i].ctx = ctx;
       }
       if (!delay_alloc) {
+#if MXNET_MEMORY_PROFILER_ON
+        CheckAndAllocData(storage_shape, dtype, ctx_.name);
+#else
         CheckAndAllocData(storage_shape, dtype);
+#endif
       }
     }
 
@@ -729,43 +793,85 @@ class NDArray {
     }
 
     /*! \brief check if delay alloc is on, do alloc if not yet done */
+#if MXNET_MEMORY_PROFILER_ON
+    inline void CheckAndAlloc(std::wstring name) {
+      if (delay_alloc) {
+        shandle = Storage::Get()->Alloc(shandle.size, shandle.ctx, name);
+        delay_alloc = false;
+      }
+    }
+#else
     inline void CheckAndAlloc(void) {
       if (delay_alloc) {
         shandle = Storage::Get()->Alloc(shandle.size, shandle.ctx);
         delay_alloc = false;
       }
     }
+#endif
+
 
     /*! \brief Check and alloc memory for a dense ndarray */
     // size is the number of bytes
+    /*@ABHISHEK: new helpers in v0.12.0*/
+#if MXNET_MEMORY_PROFILER_ON
+    void CheckAndAlloc(uint64_t dbytes, std::wstring name) {
+#else
     void CheckAndAlloc(uint64_t dbytes) {
+#endif
       CHECK_EQ(kDefaultStorage, storage_type)
               << "CheckAndAlloc(dbytes) is not intended for kDefaultStorage";
       if (delay_alloc) {
+#if MXNET_MEMORY_PROFILER_ON
+        shandle = Storage::Get()->Alloc(dbytes, shandle.ctx, name);
+#else
         shandle = Storage::Get()->Alloc(dbytes, shandle.ctx);
+#endif
         delay_alloc = false;
       } else if (shandle.size < dbytes) {
         // free storage if necessary and alloc again
         if (shandle.size > 0) Storage::Get()->Free(shandle);
         // init storage
+#if MXNET_MEMORY_PROFILER_ON
+        shandle = Storage::Get()->Alloc(dbytes, shandle.ctx, name);
+#else
         shandle = Storage::Get()->Alloc(dbytes, shandle.ctx);
+#endif
       }
     }
 
+    /*@ABHISHEK: new helper, needs same modifications as other CheckAndAlloc functions*/
+#if MXNET_MEMORY_PROFILER_ON
+    inline void CheckAndAlloc(const TShape &shape, const std::vector<TShape> &aux_shapes,
+                              int dtype, std::wstring name) {
+#else
     inline void CheckAndAlloc(const TShape &shape, const std::vector<TShape> &aux_shapes,
                               int dtype) {
+#endif
       // calculate size, perform allocation
       if (kRowSparseStorage == storage_type) {
         // For row sparse, aux_shape indicates the number of rows to allocate
         auto aux_shape = aux_shapes[rowsparse::kIdx];
+#if MXNET_MEMORY_PROFILER_ON
+        CheckAndAllocAuxData(rowsparse::kIdx, aux_shape, name);
+        TShape storage_shape(shape);
+        storage_shape[0] = aux_shape[0];
+        CheckAndAllocData(storage_shape, dtype, name);
+#else
         CheckAndAllocAuxData(rowsparse::kIdx, aux_shape);
         TShape storage_shape(shape);
         storage_shape[0] = aux_shape[0];
         CheckAndAllocData(storage_shape, dtype);
+#endif
       } else if (kCSRStorage == storage_type) {
+#if MXNET_MEMORY_PROFILER_ON
+        CheckAndAllocAuxData(csr::kIndPtr, aux_shapes[csr::kIndPtr], name);
+        CheckAndAllocAuxData(csr::kIdx, aux_shapes[csr::kIdx], name);
+        CheckAndAllocData(aux_shapes[csr::kIdx], dtype, name);
+#else
         CheckAndAllocAuxData(csr::kIndPtr, aux_shapes[csr::kIndPtr]);
         CheckAndAllocAuxData(csr::kIdx, aux_shapes[csr::kIdx]);
         CheckAndAllocData(aux_shapes[csr::kIdx], dtype);
+#endif
       } else {
         LOG(FATAL) << "Storage type " << storage_type << " not implemented for CheckAndAlloc";
       }
@@ -774,14 +880,23 @@ class NDArray {
     // storage shape is also updated
     // if data is already allocated, try reuse the storage. Otherwise, free the current one
     // and allocate new storage
+    /*@ABHISHEK: Another helper*/
+#if MXNET_MEMORY_PROFILER_ON
+    inline void CheckAndAllocData(const TShape &shape, int dtype, std::wstring name) {
+#else
     inline void CheckAndAllocData(const TShape &shape, int dtype) {
+#endif
       CHECK_NE(aux_shapes.size(), 0) << "data is expected to be allocated after aux_data";
       auto dbytes = shape.Size() * mshadow::mshadow_sizeof(dtype);
       if (shandle.size < dbytes) {
         // free storage if necessary and alloc again
         if (shandle.size > 0) Storage::Get()->Free(shandle);
         // init storage
+#if MXNET_MEMORY_PROFILER_ON
+        shandle = Storage::Get()->Alloc(dbytes, ctx, name);
+#else
         shandle = Storage::Get()->Alloc(dbytes, ctx);
+#endif
       }
       // init shape
       storage_shape = shape;
@@ -793,7 +908,12 @@ class NDArray {
     // aux shape is also updated
     // if aux data is already allocated, try reuse the storage. Otherwise, free the current one
     // and allocate new storage
+  /*@ABHISHEK: Another helper*/
+#if MXNET_MEMORY_PROFILER_ON
+    inline void CheckAndAllocAuxData(size_t i, const TShape &shape, std::wstring name) {
+#else
     inline void CheckAndAllocAuxData(size_t i, const TShape &shape) {
+#endif
       CHECK_EQ(shape.ndim(), 1) << "shape must be 1D in CheckAndAllocAuxData";
       CHECK_NE(storage_type, kUndefinedStorage)
         << "storage type cannot be kUndefinedStorage in CheckAndAllocAuxData";
@@ -807,7 +927,11 @@ class NDArray {
         // free storage if necessary and alloc again
         if (aux_handles[i].size > 0) Storage::Get()->Free(aux_handles[i]);
         // init aux storage
+#if MXNET_MEMORY_PROFILER_ON
+        aux_handles[i] = Storage::Get()->Alloc(aux_bytes, ctx, name);
+#else
         aux_handles[i] = Storage::Get()->Alloc(aux_bytes, ctx);
+#endif
       }
       // init shape
       set_aux_shape(i, shape);
diff --git a/include/mxnet/op_attr_types.h b/include/mxnet/op_attr_types.h
index 9c512ee..615fccd 100644
--- a/include/mxnet/op_attr_types.h
+++ b/include/mxnet/op_attr_types.h
@@ -80,6 +80,11 @@ struct OpContext {
   inline mshadow::Stream<xpu>* get_stream() const {
     return run_ctx.get_stream<xpu>();
   }
+#if MXNET_MEMORY_PROFILER_ON
+  inline void set_name(const std::wstring name) {
+    run_ctx.set_name(name);
+  }
+#endif
 };
 
 /*! \brief the execution type of the operator */
diff --git a/include/mxnet/resource.h b/include/mxnet/resource.h
index 1ca1fc6..13fb096 100644
--- a/include/mxnet/resource.h
+++ b/include/mxnet/resource.h
@@ -105,10 +105,17 @@ struct Resource {
    * \tparam ndim the number of dimension of the tensor requested.
    */
   template<typename xpu, int ndim>
+#if MXNET_MEMORY_PROFILER_ON
+  inline mshadow::Tensor<xpu, ndim, real_t> get_space(
+      mshadow::Shape<ndim> shape, mshadow::Stream<xpu> *stream, std::wstring name = L"") const {
+    return get_space_typed<xpu, ndim, real_t>(shape, stream, name);
+  }
+#else
   inline mshadow::Tensor<xpu, ndim, real_t> get_space(
       mshadow::Shape<ndim> shape, mshadow::Stream<xpu> *stream) const {
     return get_space_typed<xpu, ndim, real_t>(shape, stream);
   }
+#endif
   /*!
    * \brief Get cpu space requested as mshadow Tensor.
    *  The caller can request arbitrary size.
@@ -133,6 +140,20 @@ struct Resource {
    * \tparam ndim the number of dimension of the tensor requested.
    */
   template<typename xpu, int ndim, typename DType>
+#if MXNET_MEMORY_PROFILER_ON
+  inline mshadow::Tensor<xpu, ndim, DType> get_space_typed(
+      mshadow::Shape<ndim> shape, mshadow::Stream<xpu> *stream, std::wstring name = L"") const {
+    CHECK_EQ(req.type, ResourceRequest::kTempSpace);
+    if (std::is_same<xpu, gpu>::value)
+      return mshadow::Tensor<xpu, ndim, DType>(
+          reinterpret_cast<DType*>(get_space_internal(shape.Size() * sizeof(DType), name)),
+          shape, shape[ndim - 1], stream);
+    else
+      return mshadow::Tensor<xpu, ndim, DType>(
+          reinterpret_cast<DType*>(get_space_internal(shape.Size() * sizeof(DType))),
+          shape, shape[ndim - 1], stream);
+  }
+#else
   inline mshadow::Tensor<xpu, ndim, DType> get_space_typed(
       mshadow::Shape<ndim> shape, mshadow::Stream<xpu> *stream) const {
     CHECK_EQ(req.type, ResourceRequest::kTempSpace);
@@ -140,6 +161,7 @@ struct Resource {
         reinterpret_cast<DType*>(get_space_internal(shape.Size() * sizeof(DType))),
         shape, shape[ndim - 1], stream);
   }
+#endif
   /*!
    * \brief Get CPU space as mshadow Tensor in specified type.
    * The caller can request arbitrary size.
@@ -161,7 +183,11 @@ struct Resource {
    * \param size The size of the space.
    * \return The allocated space.
    */
+#if MXNET_MEMORY_PROFILER_ON
+  void* get_space_internal(size_t size, std::wstring name = L"") const;
+#else
   void* get_space_internal(size_t size) const;
+#endif
   /*!
    * \brief internal function to get cpu space from resources.
    * \param size The size of space.
diff --git a/include/mxnet/storage.h b/include/mxnet/storage.h
index 7e3af8e..8d0fa66 100644
--- a/include/mxnet/storage.h
+++ b/include/mxnet/storage.h
@@ -57,7 +57,11 @@ class Storage {
    * \param ctx Context information about the device and ID.
    * \return Handle struct.
    */
+#if MXNET_MEMORY_PROFILER_ON
+  virtual Handle Alloc(size_t size, Context ctx, std::wstring name = L"") = 0;
+#else
   virtual Handle Alloc(size_t size, Context ctx) = 0;
+#endif
   /*!
    * \brief Free storage.
    * \param handle Handle struect.
diff --git a/make/config.mk b/make/config.mk
index d47d4d6..6ac1504 100644
--- a/make/config.mk
+++ b/make/config.mk
@@ -37,7 +37,7 @@ USE_PROFILER =
 USE_SIGNAL_HANDLER =
 
 # the additional link flags you want to add
-ADD_LDFLAGS =
+ADD_LDFLAGS = -ldl
 
 # the additional compile flags you want to add
 ADD_CFLAGS =
diff --git a/plugin/warpctc/warpctc-inl.h b/plugin/warpctc/warpctc-inl.h
index d492656..26d8c45 100644
--- a/plugin/warpctc/warpctc-inl.h
+++ b/plugin/warpctc/warpctc-inl.h
@@ -194,9 +194,13 @@ class WarpCTCOp : public Operator {
                                       input_lengths.size(), info,
                                       &alloc_bytes),
                    "Error: get_workspace_size in inf_test");
-
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1> ctc_workspace = ctx.requested[warpctc_enum::kTmp].get_space<xpu>(
+        mshadow::Shape1(alloc_bytes), s, L"workspace:warpctc");
+#else
     Tensor<xpu, 1> ctc_workspace = ctx.requested[warpctc_enum::kTmp].get_space<xpu>(
         mshadow::Shape1(alloc_bytes), s);
+#endif
 
     std::vector<float> costs(minibatch);
     throw_on_error(compute_ctc_loss(activations,
diff --git a/python/mxnet/_ctypes/ndarray.py b/python/mxnet/_ctypes/ndarray.py
index 0d02c04..23fdcea 100644
--- a/python/mxnet/_ctypes/ndarray.py
+++ b/python/mxnet/_ctypes/ndarray.py
@@ -31,7 +31,7 @@ from ..base import check_call
 
 class NDArrayBase(object):
     """Base data structure for ndarray"""
-    __slots__ = ["handle", "writable"]
+    __slots__ = ["handle", "writable", "tag"]
     # pylint: disable= no-member
 
     def __init__(self, handle, writable=True):
@@ -46,6 +46,7 @@ class NDArrayBase(object):
             assert isinstance(handle, NDArrayHandle)
         self.handle = handle
         self.writable = writable
+        self.tag = None
 
     def __del__(self):
         check_call(_LIB.MXNDArrayFree(self.handle))
@@ -64,6 +65,15 @@ def _set_ndarray_class(cls):
 
 def _imperative_invoke(handle, ndargs, keys, vals, out):
     """ctypes implementation of imperative invoke wrapper"""
+    # @Abhishek
+    # import inspect as ins
+    # print("[py_trace]*****************")
+    # frameno = 0
+    # for frameinfo in ins.stack():
+    #     frameno += 1
+    #     print(frameno, ':', frameinfo)
+    # print("[py_trace]*****************")
+
     if out is not None:
         original_output = out
         if isinstance(out, NDArrayBase):
diff --git a/python/mxnet/context.py b/python/mxnet/context.py
index 9798b48..1c4ac37 100644
--- a/python/mxnet/context.py
+++ b/python/mxnet/context.py
@@ -40,6 +40,9 @@ class Context(object):
     device_id : int (default=0)
         The device id of the device, needed for GPU.
 
+    name : str (default='untagged')
+        This tag is used for gpu memory profiling, since each ndarray allocated
+        on the GPU has an associated context, it is easy to tag and profile.
     Note
     ----
     Context can also be used as a way to change the default context.
@@ -64,13 +67,18 @@ class Context(object):
     default_ctx = None
     devtype2str = {1: 'cpu', 2: 'gpu', 3: 'cpu_pinned'}
     devstr2type = {'cpu': 1, 'gpu': 2, 'cpu_pinned': 3}
-    def __init__(self, device_type, device_id=0):
+    def __init__(self, device_type, device_id=0, name="untagged"):
         if isinstance(device_type, Context):
             self.device_typeid = device_type.device_typeid
             self.device_id = device_type.device_id
         else:
             self.device_typeid = Context.devstr2type[device_type]
             self.device_id = device_id
+        # Change 'name' to 'name_tag' when using property called
+        # 'name' otherwise you will get infinite recursive loop
+        # @ABHISHEK: We are creating this field here in Context but
+        # it must be set by Ndarray::set_context_name() function.
+        self.name = name
         self._old_ctx = None
 
     @property
@@ -92,6 +100,7 @@ class Context(object):
 
     def __hash__(self):
         """Compute hash value of context for dictionary lookup"""
+        # @ABHISHEK: should I include name tag for hash value calculation
         return hash((self.device_typeid, self.device_id))
 
     def __eq__(self, other):
@@ -103,7 +112,7 @@ class Context(object):
             self.device_id == other.device_id
 
     def __str__(self):
-        return '%s(%d)' % (self.device_type, self.device_id)
+        return '%s(%d)_%s' % (self.device_type, self.device_id, self.name)
 
     def __repr__(self):
         return self.__str__()
diff --git a/python/mxnet/executor_manager.py b/python/mxnet/executor_manager.py
index 33c6c97..d5c27d2 100644
--- a/python/mxnet/executor_manager.py
+++ b/python/mxnet/executor_manager.py
@@ -196,6 +196,7 @@ def _bind_exec(sym, ctx, input_shapes, param_names, need_grad=False,
 
         aux_arrays = [a for a in base_exec.aux_arrays]
 
+    self.logger.info('@executor_manager:_bind_exec_bind:bind')
     executor = sym.bind(ctx=ctx, args=arg_arrays, args_grad=grad_arrays,
                         aux_states=aux_arrays,
                         grad_req=grad_req, shared_exec=base_exec)
diff --git a/python/mxnet/metric.py b/python/mxnet/metric.py
index 5b0780a..ded7dc3 100644
--- a/python/mxnet/metric.py
+++ b/python/mxnet/metric.py
@@ -642,6 +642,7 @@ class Perplexity(EvalMetric):
         preds : list of `NDArray`
             Predicted values.
         """
+        # print("[mem] Annotation: Perplexity.Update")
         assert len(labels) == len(preds)
         loss = 0.
         num = 0
@@ -658,6 +659,7 @@ class Perplexity(EvalMetric):
             num += pred.size
         self.sum_metric += loss
         self.num_inst += num
+        # print("[mem] Annotation: end")
 
     def get(self):
         """Returns the current evaluation result.
diff --git a/python/mxnet/model.py b/python/mxnet/model.py
index 2444ca0..24f0fc5 100644
--- a/python/mxnet/model.py
+++ b/python/mxnet/model.py
@@ -133,6 +133,25 @@ def _update_params(param_arrays, grad_arrays, updater, num_device,
             # state for the same index but on diff devs, TODO(mli)
             # use a better solution later
             w, g = p
+            # Since 'context' is defined as a property of Ndarray class, and since
+            # the ndarray property getter calls MXNDarraygetcontext,
+            # accessing 'w.context' always reads from c++ core which doesnt have
+            # the name tag
+
+            # When we want to set the context tag we say w.context.name = 'tag'
+            # this sets the tag but this information is not propagated to c++ core
+            # Next time when we try to read this tag with
+            # print('tag:', w.context.name), instead of reading the saved tag,
+            # the python side immediately issues a call to C++ core, reads the context
+            # information and initalizes and returns a new context object, since
+            # the tag was not propagated to C++ while setting, this newly created
+            # context object is created with the default tag ('untagge') hence we lose
+            # the tag we saved while setting it.
+
+            # We set the tag here, so even if different optimizers are used the tag
+            # will be propagated properly without any issues
+            w.set_context_tag('optimizer_weight_update_' + param_names[index])
+            g.set_context_tag('optimizer_grad_update_' + param_names[index])
             updater(index*num_device+k, g, w)
 
 
@@ -573,6 +592,7 @@ class FeedForward(BASE_ESTIMATOR):
             if arg_shapes == pred_shapes:
                 return
         # for now only use the first device
+        logger.info('@model.py:simple_bind')
         pred_exec = self.symbol.simple_bind(
             self.ctx[0], grad_req='null', type_dict=type_dict, **dict(input_shapes))
         pred_exec.copy_params_from(self.arg_params, self.aux_params)
diff --git a/python/mxnet/module/base_module.py b/python/mxnet/module/base_module.py
index bae166e..8243645 100644
--- a/python/mxnet/module/base_module.py
+++ b/python/mxnet/module/base_module.py
@@ -456,6 +456,7 @@ class BaseModule(object):
         """
         assert num_epoch is not None, 'please specify number of epochs'
 
+        self.logger.info('@base_module:bind')
         self.bind(data_shapes=train_data.provide_data, label_shapes=train_data.provide_label,
                   for_training=True, force_rebind=force_rebind)
         if monitor is not None:
diff --git a/python/mxnet/module/bucketing_module.py b/python/mxnet/module/bucketing_module.py
index f3c7ecb..88983e9 100644
--- a/python/mxnet/module/bucketing_module.py
+++ b/python/mxnet/module/bucketing_module.py
@@ -31,6 +31,7 @@ from ..initializer import Uniform
 
 from .base_module import BaseModule, _check_input_names
 from .module import Module
+from ..base import _LIB
 
 class BucketingModule(BaseModule):
     """This module helps to deal efficiently with varying-length inputs.
@@ -58,6 +59,7 @@ class BucketingModule(BaseModule):
                  fixed_param_names=None, state_names=None):
         super(BucketingModule, self).__init__(logger=logger)
 
+
         assert default_bucket_key is not None
         self._default_bucket_key = default_bucket_key
         self._sym_gen = sym_gen
@@ -320,6 +322,7 @@ class BucketingModule(BaseModule):
                         context=self._context, work_load_list=self._work_load_list,
                         fixed_param_names=self._fixed_param_names,
                         state_names=self._state_names)
+        self.logger.info('@bucketing_module:bind.')
         module.bind(data_shapes, label_shapes, for_training, inputs_need_grad,
                     force_rebind=False, shared_module=None, grad_req=grad_req)
         self._curr_module = module
@@ -350,6 +353,7 @@ class BucketingModule(BaseModule):
                             work_load_list=self._work_load_list,
                             fixed_param_names=self._fixed_param_names,
                             state_names=self._state_names)
+            self.logger.info('@bucketing_module:switch_bucket:bind')
             module.bind(data_shapes, label_shapes, self._curr_module.for_training,
                         self._curr_module.inputs_need_grad,
                         force_rebind=False, shared_module=self._buckets[self._default_bucket_key])
diff --git a/python/mxnet/module/executor_group.py b/python/mxnet/module/executor_group.py
index 0f3c079..f508bcf 100755
--- a/python/mxnet/module/executor_group.py
+++ b/python/mxnet/module/executor_group.py
@@ -585,6 +585,7 @@ class DataParallelExecutorGroup(object):
         """Internal utility function to bind the i-th executor.
         This function utilizes simple_bind python interface.
         """
+        print("executor_group:_bind_ith_exec begin")
         shared_exec = None if shared_group is None else shared_group.execs[i]
         context = self.contexts[i]
         shared_data_arrays = self.shared_data_arrays[i]
@@ -597,6 +598,7 @@ class DataParallelExecutorGroup(object):
         if label_shapes is not None:
             input_types.update({x.name: x.dtype for x in label_shapes})
 
+        self.logger.info('@executor_group:simple_bind')
         executor = self.symbol.simple_bind(ctx=context, grad_req=self.grad_req,
                                            type_dict=input_types, shared_arg_names=self.param_names,
                                            shared_exec=shared_exec,
diff --git a/python/mxnet/module/module.py b/python/mxnet/module/module.py
index 4c20a6f..f676bf5 100644
--- a/python/mxnet/module/module.py
+++ b/python/mxnet/module/module.py
@@ -87,6 +87,8 @@ class Module(BaseModule):
 
         arg_names = symbol.list_arguments()
         input_names = data_names + label_names + state_names
+        # @ABHISHEK: This might present a good lead for where to set tags,
+        # all names are here
         self._param_names = [x for x in arg_names if x not in input_names]
         self._fixed_param_names = fixed_param_names
         self._aux_names = symbol.list_auxiliary_states()
@@ -374,6 +376,7 @@ class Module(BaseModule):
             essentially corresponds to a different bucket -- a module with different symbol
             but with the same sets of parameters (e.g. unrolled RNNs with different lengths).
         """
+        self.logger.info("[py_trace] module.py bind() start.")
         # force rebinding is typically used when one want to switch from
         # training to prediction phase.
         if force_rebind:
@@ -415,6 +418,7 @@ class Module(BaseModule):
                                                      fixed_param_names=self._fixed_param_names,
                                                      grad_req=grad_req,
                                                      state_names=self._state_names)
+        self.logger.info("[py_trace] module.py created DataParallelExecutorGroup.")
         self._total_exec_bytes = self._exec_group._total_exec_bytes
         if shared_module is not None:
             self.params_initialized = True
diff --git a/python/mxnet/ndarray/ndarray.py b/python/mxnet/ndarray/ndarray.py
index 1cd9f40..9a871ae 100644
--- a/python/mxnet/ndarray/ndarray.py
+++ b/python/mxnet/ndarray/ndarray.py
@@ -123,6 +123,18 @@ def _new_alloc_handle(shape, ctx, delay_alloc, dtype=mx_real_t):
         A new empty `NDArray` handle.
     """
     hdl = NDArrayHandle()
+    # The opitimizer update allocs which are the major
+    # allocs from python are not coming through this path
+    # but via _imperative_invoke() so we need a way now to
+    # propagate this info through that path.
+
+    # @ABHISHEK: Im leaving these modifications here incase
+    # we later want to propagate the tag along this path
+    # Currently the optimizer allocations happen via 'MXImperativeInvokeEx'
+    # API and python sends the context info encoded in a string
+    # and reconstructs the context object on C++ side by parsing this string
+    # BOTTOMLNE: THIS MODIFICATION TO MXNDArrayCreateEx is NOT REQUIRED.
+    # we can reverse this and still have good memory profiling.
     check_call(_LIB.MXNDArrayCreateEx(
         c_array(mx_uint, shape),
         mx_uint(len(shape)),
@@ -130,7 +142,9 @@ def _new_alloc_handle(shape, ctx, delay_alloc, dtype=mx_real_t):
         ctypes.c_int(ctx.device_id),
         ctypes.c_int(int(delay_alloc)),
         ctypes.c_int(int(_DTYPE_NP_TO_MX[np.dtype(dtype).type])),
-        ctypes.byref(hdl)))
+        ctypes.byref(hdl),
+        # @TODO: @ABHISHEK this will break when MXNET_MEM_PROFILER is not set on C++ side!!
+        ctypes.create_string_buffer(str.encode(ctx.name))))
     return hdl
 
 
@@ -1451,7 +1465,20 @@ fixed-size items.
         dev_id = ctypes.c_int()
         check_call(_LIB.MXNDArrayGetContext(
             self.handle, ctypes.byref(dev_typeid), ctypes.byref(dev_id)))
-        return Context(Context.devtype2str[dev_typeid.value], dev_id.value)
+        # @ABHISHEK: return the Context with existing tag
+        if self.tag:
+            return Context(Context.devtype2str[dev_typeid.value], dev_id.value, self.tag)
+        else:
+            return Context(Context.devtype2str[dev_typeid.value], dev_id.value)
+
+    # @context.setter
+    # @ABHISHEK: for mem profiling
+    def set_context_tag(self, tag):
+        # Need this extra field to prevent infinite recursive loop
+        # Without this field we will have to check 'if self.context.name is not None'
+        # __inside__ Context() property getter, which will call itself
+        # because it sees 'self.context.name' -> will loop
+        self.tag = tag
 
     @property
     def dtype(self):
@@ -3030,6 +3057,9 @@ def zeros(shape, ctx=None, dtype=None, **kwargs):
         ctx = Context.default_ctx
     dtype = mx_real_t if dtype is None else dtype
     # pylint: disable= no-member, protected-access
+    # @ABHISHEK: Here we propagate ctx tag from python to C++
+    # the _internal._zeros is a function in gen_internal and it
+    # passes the ctx info as a string to _imperative_invoke
     return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, **kwargs)
     # pylint: enable= no-member, protected-access
 
diff --git a/python/mxnet/ndarray/sparse.py b/python/mxnet/ndarray/sparse.py
index a31ec01..31ebb46 100644
--- a/python/mxnet/ndarray/sparse.py
+++ b/python/mxnet/ndarray/sparse.py
@@ -21,6 +21,7 @@
 
 from __future__ import absolute_import
 from __future__ import division
+
 try:
     from __builtin__ import slice as py_slice
     from __builtin__ import sum as py_sum
@@ -409,7 +410,8 @@ class CSRNDArray(BaseSparseNDArray):
         NDArray
             This CSRNDArray's indices array.
         """
-        return self._aux_data(1)
+        return self.\
+            _aux_data(1)
 
     @property
     def indptr(self):
diff --git a/python/mxnet/ndarray/utils.py b/python/mxnet/ndarray/utils.py
index 6f3b0ff..2b9b17d 100644
--- a/python/mxnet/ndarray/utils.py
+++ b/python/mxnet/ndarray/utils.py
@@ -61,7 +61,6 @@ def zeros(shape, ctx=None, dtype=None, stype=None, **kwargs):
     >>> mx.nd.zeros((1,2), mx.cpu(), 'float16', stype='row_sparse').asnumpy()
     array([[ 0.,  0.]], dtype=float16)
     """
-
     if stype is None or stype == 'default':
         return _zeros_ndarray(shape, ctx, dtype, **kwargs)
     else:
diff --git a/python/mxnet/optimizer.py b/python/mxnet/optimizer.py
index 66c261b..75b59b5 100644
--- a/python/mxnet/optimizer.py
+++ b/python/mxnet/optimizer.py
@@ -221,6 +221,7 @@ class Optimizer(object):
         weight_master_copy = None
         if self.multi_precision and weight.dtype == numpy.float16:
             weight_master_copy = weight.astype(numpy.float32)
+            # @ABHISHEK: Will lose tag here because tag is with weight.context and
             return (weight_master_copy,) + (self.create_state(index, weight_master_copy),)
         if weight.dtype == numpy.float16 and not self.multi_precision:
             warnings.warn("Accumulating with float16 in optimizer can lead to "
diff --git a/src/c_api/c_api.cc b/src/c_api/c_api.cc
index 1d348a5..505a62b 100644
--- a/src/c_api/c_api.cc
+++ b/src/c_api/c_api.cc
@@ -44,6 +44,7 @@
 #include "./c_api_common.h"
 #include "../operator/custom/custom-inl.h"
 #include "../engine/profiler.h"
+#include <cuda_runtime.h>
 
 using namespace mxnet;
 
@@ -82,6 +83,14 @@ inline int MXAPIGetFunctionRegInfo(const FunRegType *e,
   API_END();
 }
 
+int MXShowAvailableGPUMem() {
+  API_BEGIN();
+  size_t free, total;
+  cudaMemGetInfo(&free, &total);
+  fprintf(stderr, "[LOG] %ld bytes GPU memory available (%d MBs)", free, free >> 20);
+  API_END();
+}
+
 // NOTE: return value is added in API_END
 int MXRandomSeed(int seed) {
   API_BEGIN();
@@ -144,6 +153,7 @@ int MXGetVersion(int *out) {
 
 int MXNDArrayCreateNone(NDArrayHandle *out) {
   API_BEGIN();
+//  fprintf(stderr, "[trace_capi] createnone\n");
   *out = new NDArray();
   API_END();
 }
@@ -155,6 +165,7 @@ int MXNDArrayCreate(const mx_uint *shape,
                     int delay_alloc,
                     NDArrayHandle *out) {
   API_BEGIN();
+//  fprintf(stderr, "[trace_capi] create\n");
   *out = new NDArray(
       TShape(shape, shape + ndim),
       Context::Create(static_cast<Context::DeviceType>(dev_type), dev_id),
@@ -162,17 +173,30 @@ int MXNDArrayCreate(const mx_uint *shape,
   API_END();
 }
 
+//@TODO: for completeness make similar mem profiling modifications to other NDarray create APIs
+//@TODO: check if there is a better way to do this string passing, check for security issues
 int MXNDArrayCreateEx(const mx_uint *shape,
-                    mx_uint ndim,
-                    int dev_type,
-                    int dev_id,
-                    int delay_alloc,
-                    int dtype,
-                    NDArrayHandle *out) {
+                      mx_uint ndim,
+                      int dev_type,
+                      int dev_id,
+                      int delay_alloc,
+                      int dtype,
+#if MXNET_MEMORY_PROFILER_ON
+                      NDArrayHandle *out,
+                      char const* context_name = "untagged"
+#else
+                      NDArrayHandle *out
+#endif
+                      ) {
   API_BEGIN();
   *out = new NDArray(
       TShape(shape, shape + ndim),
+#if MXNET_MEMORY_PROFILER_ON
+      Context::Create(static_cast<Context::DeviceType>(dev_type),
+                      dev_id, std::string(context_name)),
+#else
       Context::Create(static_cast<Context::DeviceType>(dev_type), dev_id),
+#endif
       delay_alloc != 0,
       dtype);
   API_END();
@@ -201,6 +225,7 @@ int MXNDArrayCreateSparseEx(int storage_type,
     aux_shapes.emplace_back(shape_start, shape_start + aux_ndims[i]);
     shape_start += aux_ndims[i];
   }
+//  fprintf(stderr, "[trace_capi] createsparseex\n");
   *out = new NDArray(
       NDArrayStorageType(storage_type),
       TShape(shape, shape + ndim),
@@ -221,6 +246,7 @@ int MXNDArrayLoadFromRawBytes(const void *buf,
   if (!ptr->Load(&strm)) {
     throw dmlc::Error("Invalid NDArray serialization format");
   }
+//  fprintf(stderr, "[trace_capi] loadfromrawbytes\n");
   *out = ptr;
   API_END_HANDLE_ERROR(delete ptr);
 }
@@ -328,6 +354,7 @@ int MXNDArrayLoad(const char* fname,
   }
   ret->ret_handles.resize(data.size());
   for (size_t i = 0; i < data.size(); ++i) {
+//    fprintf(stderr, "[trace_capi] load\n");
     NDArray *ptr = new NDArray();
     *ptr = data[i];
     ret->ret_handles[i] = ptr;
@@ -353,6 +380,7 @@ int MXNDArraySlice(NDArrayHandle handle,
                    mx_uint slice_begin,
                    mx_uint slice_end,
                    NDArrayHandle *out) {
+//  fprintf(stderr, "[trace_capi] slice\n");
   NDArray *ptr = new NDArray();
   API_BEGIN();
   *ptr = static_cast<NDArray*>(handle)->SliceWithRecord(
@@ -364,6 +392,7 @@ int MXNDArraySlice(NDArrayHandle handle,
 int MXNDArrayAt(NDArrayHandle handle,
                 mx_uint idx,
                 NDArrayHandle *out) {
+//  fprintf(stderr, "[trace_capi] at\n");
   NDArray *ptr = new NDArray();
   API_BEGIN();
   *ptr = static_cast<NDArray*>(handle)->AtWithRecord(idx);
@@ -402,6 +431,7 @@ MXNET_DLL int MXNDArrayReshape(NDArrayHandle handle,
   if (pos >= 0) {
     new_shape[pos] = arr->shape().Size() / size;
   }
+//  fprintf(stderr, "[trace_capi] reshape\n");
   *ptr = arr->ReshapeWithRecord(new_shape);
   *out = ptr;
   API_END_HANDLE_ERROR(delete ptr);
@@ -481,6 +511,7 @@ int MXNDArrayGetAuxNDArray(NDArrayHandle handle,
                            NDArrayHandle *out) {
   API_BEGIN();
   NDArray *arr = static_cast<NDArray*>(handle);
+//  fprintf(stderr, "[trace_capi] getauxndarray\n");
   *out = new NDArray(arr->aux_ndarray(i));
   API_END();
 }
@@ -494,6 +525,7 @@ int MXNDArrayGetDataNDArray(NDArrayHandle handle,
                             NDArrayHandle *out) {
   API_BEGIN();
   NDArray *arr = static_cast<NDArray*>(handle);
+//  fprintf(stderr, "[trace_capi] getdatandarray\n");
   *out = new NDArray(arr->data_ndarray());
   API_END();
 }
@@ -522,6 +554,7 @@ int MXNDArrayGetGrad(NDArrayHandle handle, NDArrayHandle *out) {
   if (ret.is_none()) {
     *out = NULL;
   } else {
+//    fprintf(stderr, "[trace_capi] getgrad\n");
     *out = new NDArray(ret);
   }
   API_END();
@@ -530,6 +563,7 @@ int MXNDArrayGetGrad(NDArrayHandle handle, NDArrayHandle *out) {
 int MXNDArrayDetach(NDArrayHandle handle, NDArrayHandle *out) {
   API_BEGIN();
   NDArray *arr = static_cast<NDArray*>(handle);
+//  fprintf(stderr, "[trace_capi] arraydetach\n");
   *out = new NDArray(arr->Detach());
   API_END();
 }
@@ -689,6 +723,7 @@ int MXDataIterNext(DataIterHandle handle, int *out) {
 int MXDataIterGetLabel(DataIterHandle handle, NDArrayHandle *out) {
   API_BEGIN();
   const DataBatch& db = static_cast<IIterator<DataBatch>* >(handle)->Value();
+//  fprintf(stderr, "[trace_capi] dataietergetlabel\n");
   NDArray* pndarray = new NDArray();
   // temp hack to make label 1D
   // TODO(tianjun) make label 1D when label_width=0
@@ -713,6 +748,7 @@ int MXDataIterGetIndex(DataIterHandle handle, uint64_t **out_index, uint64_t *ou
 int MXDataIterGetData(DataIterHandle handle, NDArrayHandle *out) {
   API_BEGIN();
   const DataBatch& db = static_cast<IIterator<DataBatch>* >(handle)->Value();
+//  fprintf(stderr, "[trace_capi] dataitergetdata\n");
   NDArray* pndarray = new NDArray();
   *pndarray = db.data[0];
   *out = pndarray;
@@ -872,6 +908,7 @@ int MXKVStorePullRowSparseEx(KVStoreHandle handle,
 void MXKVStoreSetUpdaterImpl(KVStoreHandle handle,
                              MXKVStoreUpdater updater,
                              void* updater_handle) {
+//  fprintf(stderr, "[trace_capi] kvstoresetupdaterimpl\n");
   MXKVStoreUpdater * updater_temp = updater;
   void* updater_handle_temp = updater_handle;
   std::function<void(int, const NDArray&, NDArray*)> updt
@@ -897,6 +934,7 @@ int MXKVStoreSetUpdaterEx(KVStoreHandle handle,
                           MXKVStoreUpdater updater,
                           MXKVStoreStrUpdater str_updater,
                           void* updater_handle) {
+//  fprintf(stderr, "[trace_capi] kvstoresetupdaterex\n");
   API_BEGIN();
   // set updater with int keys
   MXKVStoreSetUpdaterImpl(handle, updater, updater_handle);
diff --git a/src/c_api/c_api_executor.cc b/src/c_api/c_api_executor.cc
index 8be3965..73d04db 100644
--- a/src/c_api/c_api_executor.cc
+++ b/src/c_api/c_api_executor.cc
@@ -456,6 +456,7 @@ int MXExecutorSimpleBind(SymbolHandle symbol_handle,
     if (nd.is_none()) {
       LOG(FATAL) << "Input argument NDArray cannot be un-allocated";
     }
+    //fprintf(stderr, "[trace_capi] in_arg_vec\n");
     ret->ret_handles.push_back(new NDArray(nd));
   }
   if (in_arg_vec.size() > 0) {
@@ -468,6 +469,7 @@ int MXExecutorSimpleBind(SymbolHandle symbol_handle,
     if (nd.is_none()) {
       ret->ret_handles.push_back(nullptr);
     } else {
+      //fprintf(stderr, "[trace_capi] arg_grad_vec\n");
       ret->ret_handles.push_back(new NDArray(nd));
     }
   }
@@ -480,6 +482,7 @@ int MXExecutorSimpleBind(SymbolHandle symbol_handle,
     if (nd.is_none()) {
       LOG(FATAL) << "Auxiliary argument NDArray cannot be un-allocated";
     }
+    //fprintf(stderr, "[trace_capi] aux_state_vec\n");
     ret->ret_handles.push_back(new NDArray(nd));
   }
   if (aux_state_vec.size() > 0) {
@@ -497,6 +500,7 @@ int MXExecutorSimpleBind(SymbolHandle symbol_handle,
       if (kv.second.is_none()) {
         LOG(FATAL) << "Shared data NDArray cannot be un-allocated";
       }
+      //fprintf(stderr, "[trace_capi] shared_buffer_map\n");
       ret->ret_handles.push_back(new NDArray(kv.second));
       ret->ret_vec_str.emplace_back(kv.first);
       ret->ret_vec_charp.push_back(ret->ret_vec_str.back().c_str());
diff --git a/src/c_api/c_api_ndarray.cc b/src/c_api/c_api_ndarray.cc
index 88f14ab..141af48 100644
--- a/src/c_api/c_api_ndarray.cc
+++ b/src/c_api/c_api_ndarray.cc
@@ -35,6 +35,7 @@
 #include "./c_api_common.h"
 #include "../common/utils.h"
 #include "../common/exec_utils.h"
+#include <stdlib.h>
 
 using namespace mxnet;
 
diff --git a/src/common/utils.h b/src/common/utils.h
index e0604de..0332318 100644
--- a/src/common/utils.h
+++ b/src/common/utils.h
@@ -43,6 +43,11 @@
 #include <algorithm>
 #include <functional>
 
+#include <dlfcn.h>    // for dladdr
+#include <cxxabi.h>   // for __cxa_demangle
+#include <string>
+#include <sstream>
+
 namespace mxnet {
 namespace common {
 
@@ -348,6 +353,41 @@ FCompType GetFCompute(const nnvm::Op* op, const std::string& name,
   }
 }
 
+// https://gist.github.com/fmela/591333
+inline void CustomBacktrace(int skip = 1) {
+  void *callstack[128];
+  const int nMaxFrames = sizeof(callstack) / sizeof(callstack[0]);
+  char buf[1024];
+  int nFrames = backtrace(callstack, nMaxFrames);
+  char **symbols = backtrace_symbols(callstack, nFrames);
+
+  std::ostringstream trace_buf;
+  for (int i = skip; i < nFrames; i++) {
+//      fprintf(stderr, "[cb] %s\n", symbols[i]);
+
+    Dl_info info;
+    if (dladdr(callstack[i], &info) && info.dli_sname) {
+      char *demangled = NULL;
+      int status = -1;
+      if (info.dli_sname[0] == '_')
+        demangled = abi::__cxa_demangle(info.dli_sname, NULL, 0, &status);
+      snprintf(buf, sizeof(buf), "%-3d %*p %s + %zd\n",
+               i, int(2 + sizeof(void*) * 2), callstack[i],
+               status == 0 ? demangled :
+               info.dli_sname == 0 ? symbols[i] : info.dli_sname,
+               (char *)callstack[i] - (char *)info.dli_saddr);
+      free(demangled);
+    } else {
+      snprintf(buf, sizeof(buf), "%-3d %*p %s\n",
+               i, int(2 + sizeof(void*) * 2), callstack[i], symbols[i]);
+    }
+    trace_buf << buf;
+  }
+  free(symbols);
+  if (nFrames == nMaxFrames)
+    trace_buf << "[truncated]\n";
+  fprintf(stderr, "[cb] %s\n", trace_buf.str().c_str());
+}
 }  // namespace common
 }  // namespace mxnet
 #endif  // MXNET_COMMON_UTILS_H_
diff --git a/src/engine/threaded_engine_perdevice.cc b/src/engine/threaded_engine_perdevice.cc
index fe5a874..67cecc4 100644
--- a/src/engine/threaded_engine_perdevice.cc
+++ b/src/engine/threaded_engine_perdevice.cc
@@ -210,6 +210,12 @@ class ThreadedEnginePerDevice : public ThreadedEngine {
     RunContext run_ctx{ctx, stream};
     auto* task_queue = &(block->task_queue);
     while (task_queue->Pop(&opr_block)) {
+#if MXNET_MEMORY_PROFILER_ON
+      if (ctx.name==L"warning!,ctx_source_unclear" || ctx.name==L"untagged") {
+        fprintf(stderr, "[trace_gpuworker] %ls to %s\n", ctx.name.c_str(), opr_block->opr_stat->opr_name);
+        run_ctx.set_name(opr_block->opr_stat->opr_name);
+      }
+#endif
       this->ExecuteOprBlock(run_ctx, opr_block);
     }
     // Catch exception for CUDA driver shutdown
diff --git a/src/executor/graph_executor.cc b/src/executor/graph_executor.cc
index 90b2b26..55f24aa 100644
--- a/src/executor/graph_executor.cc
+++ b/src/executor/graph_executor.cc
@@ -27,6 +27,8 @@
 #include <vector>
 #include <algorithm>
 
+#include <cuda_runtime.h>
+
 #include "./exec_pass.h"
 #include "./graph_executor.h"
 #include "../engine/profiler.h"
@@ -57,6 +59,7 @@ inline NDArray InitZeros(const NDArrayStorageType stype, const TShape &shape,
                                 const Context &ctx, const int dtype) {
   // NDArray with default storage
   if (stype == kDefaultStorage) {
+    /*Allocates once for source, target, target_label...already tagged in first line of ReshapeOrCreate()*/
     NDArray ret(shape, ctx, false, dtype);
     ret = 0;
     return ret;
@@ -252,6 +255,7 @@ inline ValueType get_node_attr(
  */
 nnvm::Graph GraphExecutor::InitFullGraph(nnvm::Symbol symbol,
                                          const std::vector<OpReqType>& grad_req_types) {
+  fprintf(stderr, "[trace] In InitFullGraph.\n");
   using nnvm::NodePtr;
   using nnvm::NodeEntry;
   // initial information
@@ -307,6 +311,7 @@ nnvm::Graph GraphExecutor::InitFullGraph(nnvm::Symbol symbol,
   for (const auto &e : g_grad.outputs) {
     g.outputs.push_back(e);
   }
+  fprintf(stderr, "[trace] Finished InitFullGraph.\n");
   return g;
 }
 
@@ -322,6 +327,7 @@ Graph AssignContext(Graph g,
                     const std::vector<Context>& aux_state_ctxes,
                     size_t num_forward_inputs,
                     size_t num_forward_outputs) {
+  fprintf(stderr, "[trace] In AssignContext.\n");
   const auto& idx = g.indexed_graph();
   const auto& mutable_nodes = idx.mutable_input_nodes();
   // default use default context.
@@ -340,6 +346,8 @@ Graph AssignContext(Graph g,
         << default_ctx << ". All gradients must be in global context (" << default_ctx
         << ") unless group2ctx is specified for cross-device graph.";
     }
+    /*@ABHISHEK: Why is a similar loop not used to check aux_state_ctxes*/
+    fprintf(stderr, "[trace] Finished AssignContext.\n");
     return g;
   }
 
@@ -513,6 +521,7 @@ void GraphExecutor::Init(nnvm::Symbol symbol,
                          const std::vector<NDArray>& aux_states,
                          Executor* shared_exec,
                          const nnvm::NodeEntryMap<NDArray>& feed_dict) {
+  fprintf(stderr, "[trace] In GraphExecutor::Init for regular bind flow.\n");
   // create in_arg_ctxes, arg_grad_ctxes, aux_state_ctxes
   auto get_ctx1 = [](const NDArray& nd) { return nd.ctx(); };
   auto get_ctx2 = [default_ctx](const NDArray& nd) -> Context {
@@ -613,13 +622,20 @@ void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
                                   const nnvm::ShapeVector& inferred_shapes,
                                   const nnvm::DTypeVector& inferred_dtypes,
                                   const StorageTypeVector& inferred_stypes,
+#if MXNET_MEMORY_PROFILER_ON
+                                  std::vector<Context>& in_arg_ctxes,
+                                  std::vector<Context>& arg_grad_ctxes,
+                                  std::vector<Context>& aux_state_ctxes,
+#else
                                   const std::vector<Context>& in_arg_ctxes,
                                   const std::vector<Context>& arg_grad_ctxes,
                                   const std::vector<Context>& aux_state_ctxes,
+#endif
                                   const std::vector<OpReqType>& grad_req_types,
                                   std::vector<NDArray>* in_arg_vec,
                                   std::vector<NDArray>* arg_grad_vec,
                                   std::vector<NDArray>* aux_state_vec) {
+  fprintf(stderr, "[trace] In InitArguments for regular bind (no shared data).\n");
   // initialize in_args, arg_grads, and aux_states
   // populate grad_store_
   data_entry_.resize(idx.num_node_entries());
@@ -683,8 +699,15 @@ NDArray ReshapeOrCreate(const std::string& name,
                         const TShape& dest_arg_shape,
                         const int dest_arg_dtype,
                         const NDArrayStorageType dest_arg_stype,
+#if MXNET_MEMORY_PROFILER_ON
+                        Context& ctx,
+#else
                         const Context& ctx,
+#endif
                         std::unordered_map<std::string, NDArray>* shared_buffer) {
+#if MXNET_MEMORY_PROFILER_ON
+  ctx.name = std::wstring(name.begin(), name.end());
+#endif
   if (dest_arg_dtype != kDefaultStorage) {
     return InitZeros(dest_arg_stype, dest_arg_shape, ctx, dest_arg_dtype);
   }
@@ -703,6 +726,8 @@ NDArray ReshapeOrCreate(const std::string& name,
                    << "the bucket taking the largest input for better memory sharing.";
       // the NDArrays in shared_buffer are guaranteed to be of default storage
       it->second = InitZeros(dest_arg_stype, dest_arg_shape, ctx, dest_arg_dtype);
+    size_t free_bytes, total_bytes;
+    cudaMemGetInfo(&free_bytes, &total_bytes);
       return it->second;
     }  // arg_array.shape().Size() >= arg_shape.Size()
   } else {
@@ -722,9 +747,15 @@ void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
                                   const nnvm::ShapeVector& inferred_shapes,
                                   const nnvm::DTypeVector& inferred_dtypes,
                                   const StorageTypeVector& inferred_stypes,
+#if MXNET_MEMORY_PROFILER_ON
+                                  std::vector<Context>& in_arg_ctxes,
+                                  std::vector<Context>& arg_grad_ctxes,
+                                  std::vector<Context>& aux_state_ctxes,
+#else
                                   const std::vector<Context>& in_arg_ctxes,
                                   const std::vector<Context>& arg_grad_ctxes,
                                   const std::vector<Context>& aux_state_ctxes,
+#endif
                                   const std::vector<OpReqType>& grad_req_types,
                                   const std::unordered_set<std::string>& shared_arg_names,
                                   const Executor* shared_exec,
@@ -732,6 +763,7 @@ void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
                                   std::vector<NDArray>* in_arg_vec,
                                   std::vector<NDArray>* arg_grad_vec,
                                   std::vector<NDArray>* aux_state_vec) {
+  fprintf(stderr, "[trace] In InitArguments using shared data.\n");
   // initialize in_args, arg_grads, and aux_states and populate grad_store_
   data_entry_.resize(idx.num_node_entries());
   size_t arg_top = 0, aux_top = 0;
@@ -747,7 +779,10 @@ void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
     if (mutable_nodes.count(nid)) {
       if (nullptr != shared_exec && inferred_stype == kDefaultStorage &&
           shared_exec->aux_state_map().at(arg_name).storage_type() == kDefaultStorage) {
+        /*@ABHISHEK: This block tries to reuse memory so no need to tag here.*/
         const NDArray& aux_nd = shared_exec->aux_state_map().at(arg_name);
+        aux_state_ctxes[aux_top].name = L"aux_state_re:" + std::wstring(arg_name.begin(), arg_name.end());
+        //fprintf(stderr, "[trace_aux_shared] %ls\n", aux_state_ctxes[aux_top].name.c_str());
         CHECK_EQ(inferred_shape, aux_nd.shape())
           << "Inferred shape does not match shared_exec.aux_array's shape."
              " Therefore, the allocated memory for shared_exec.aux_array cannot"
@@ -760,6 +795,9 @@ void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
           << arg_name << " for the current executor";
         aux_state_vec->emplace_back(aux_nd);
       } else {
+#if MXNET_MEMORY_PROFILER_ON
+        aux_state_ctxes[aux_top].name = L"aux_state:" + std::wstring(arg_name.begin(), arg_name.end());
+#endif
         EmplaceBackZeros(inferred_stype, inferred_shape, aux_state_ctxes[aux_top],
                          inferred_dtype, aux_state_vec);
       }  // if (has_shared_exec)
@@ -773,6 +811,8 @@ void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
             shared_exec->in_arg_map().at(arg_name).storage_type() == kDefaultStorage) {
           // try to reuse memory from shared_exec
           const NDArray& in_arg_nd = shared_exec->in_arg_map().at(arg_name);
+          in_arg_ctxes[arg_top].name = L"in_arg:" + std::wstring(arg_name.begin(), arg_name.end());
+          //fprintf(stderr, "[trace_inarg_shared] %ls\n", in_arg_ctxes[arg_top].name.c_str());
           CHECK_EQ(inferred_shape, in_arg_nd.shape())
             << "Inferred shape does not match shared_exec.arg_array's shape"
                " Therefore, the allocated memory for shared_exec.arg_array cannot"
@@ -786,6 +826,9 @@ void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
           in_arg_vec->emplace_back(in_arg_nd);
         } else {
           // doesn't have shared_exec, or non-default storage
+#if MXNET_MEMORY_PROFILER_ON
+          in_arg_ctxes[arg_top].name = L"in_arg:" + std::wstring(arg_name.begin(), arg_name.end());
+#endif
           EmplaceBackZeros(inferred_stype, inferred_shape, in_arg_ctxes[arg_top],
                            inferred_dtype, in_arg_vec);
         }
@@ -799,8 +842,13 @@ void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
           if (nullptr != shared_exec && grad_stype == kDefaultStorage &&
               shared_exec->arg_grad_map().at(arg_name).storage_type() == kDefaultStorage) {
             // try to reuse memory from shared_exec
+            arg_grad_ctxes[arg_top].name = L"arg_grad:" + std::wstring(arg_name.begin(), arg_name.end());
+            //fprintf(stderr, "[trace_arggrad_shared] %ls\n", arg_grad_ctxes[arg_top].name.c_str());
             arg_grad_vec->emplace_back(shared_exec->arg_grad_map().at(arg_name));
           } else {
+#if MXNET_MEMORY_PROFILER_ON
+            arg_grad_ctxes[arg_top].name = L"arg_grad:" + std::wstring(arg_name.begin(), arg_name.end());
+#endif
             EmplaceBackZeros(grad_stype, inferred_shape, arg_grad_ctxes[arg_top],
                              inferred_dtype, arg_grad_vec);
           }
@@ -832,6 +880,7 @@ void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
       ++arg_top;
     }
   }
+    fprintf(stderr, "[trace] Finished InitArguments using shared data.\n");
 }
 
 /*!
@@ -842,6 +891,7 @@ void GraphExecutor::FinishInitGraph(nnvm::Symbol symbol,
                                     nnvm::Graph g,
                                     Executor* shared_exec,
                                     const nnvm::NodeEntryMap<NDArray>& feed_dict) {
+  fprintf(stderr, "[trace] In FinishInitGraph.\n");
   const auto& idx = g.indexed_graph();
   const auto& vstorage_type = g.GetAttr<StorageTypeVector>("storage_type");
 
@@ -875,6 +925,7 @@ void GraphExecutor::FinishInitGraph(nnvm::Symbol symbol,
   graph_ = std::move(g);
 
   if (shared_exec != nullptr) {
+    fprintf(stderr, "[trace] passing non null ptr to InitDataEntryMemory.\n");
     this->InitDataEntryMemory(&(dynamic_cast<GraphExecutor*>(shared_exec)->data_pool_));
   } else {
     this->InitDataEntryMemory(nullptr);
@@ -897,6 +948,7 @@ void GraphExecutor::FinishInitGraph(nnvm::Symbol symbol,
   }
   this->InitCachedOps();
   this->InitOpSegs();
+  fprintf(stderr, "[trace] Finished FinishInitGraph.\n");
 }
 
 /*!
@@ -916,9 +968,15 @@ void GraphExecutor::FinishInitGraph(nnvm::Symbol symbol,
 void GraphExecutor::Init(nnvm::Symbol symbol,
                          const Context& default_ctx,
                          const std::map<std::string, Context>& ctx_map,
+#if MXNET_MEMORY_PROFILER_ON
+                         std::vector<Context>& in_arg_ctxes,
+                         std::vector<Context>& arg_grad_ctxes,
+                         std::vector<Context>& aux_state_ctxes,
+#else
                          const std::vector<Context>& in_arg_ctxes,
                          const std::vector<Context>& arg_grad_ctxes,
                          const std::vector<Context>& aux_state_ctxes,
+#endif
                          const std::unordered_map<std::string, TShape>& arg_shape_map,
                          const std::unordered_map<std::string, int>& arg_dtype_map,
                          const std::unordered_map<std::string, int>& arg_stype_map,
@@ -930,6 +988,7 @@ void GraphExecutor::Init(nnvm::Symbol symbol,
                          std::unordered_map<std::string, NDArray>* shared_buffer,
                          Executor* shared_exec,
                          const nnvm::NodeEntryMap<NDArray>& feed_dict) {
+  fprintf(stderr, "[trace] In Init for simple bind flow in which only certain inputs are provided.\n");
   nnvm::Graph g = InitGraph(symbol, default_ctx, ctx_map, in_arg_ctxes, arg_grad_ctxes,
                             aux_state_ctxes, grad_req_types);
   // The following code of shape and dtype inferences and argument
@@ -945,6 +1004,8 @@ void GraphExecutor::Init(nnvm::Symbol symbol,
   for (size_t i = 0; i < num_forward_inputs_; ++i) {
     const uint32_t nid = idx.input_nodes().at(i);
     const std::string& name = idx[nid].source->attrs.name;
+//    fprintf(stderr, "[trace_forwardinputs] forward inputs name : %ls.\n",
+//					std::wstring(name.begin(), name.end()).c_str());
     auto it1 = arg_shape_map.find(name);
     if (arg_shape_map.end() != it1) {
       arg_shapes[i] = it1->second;
@@ -992,6 +1053,7 @@ void GraphExecutor::Init(nnvm::Symbol symbol,
                   grad_req_types, shared_arg_names, shared_exec,
                   shared_buffer, in_arg_vec, arg_grad_vec, aux_state_vec);
   }
+  //fprintf(stderr, "GraphExecutor::Init after init in_args, arg_grads, and aux_states\n");
   // The above code of shape and dtype inferences and argument
   // initialization is for simple_bind only. Regular bind operation
   // should do this differently.
@@ -1000,6 +1062,7 @@ void GraphExecutor::Init(nnvm::Symbol symbol,
   // This function can be called by regular bind
   // operation flow as well.
   FinishInitGraph(symbol, g, shared_exec, feed_dict);
+  fprintf(stderr, "[trace] Finished Init for simple bind flow in which only certain inputs are provided.\n");
 }
 
 /*!
@@ -1016,6 +1079,7 @@ Graph GraphExecutor::InitGraph(nnvm::Symbol symbol,
                                const std::vector<Context>& arg_grad_ctxes,
                                const std::vector<Context>& aux_state_ctxes,
                                const std::vector<OpReqType>& grad_req_types) {
+  fprintf(stderr, "[trace] In InitGraph.\n");
   // setup gradient
   nnvm::Graph g = InitFullGraph(symbol, grad_req_types);
 
@@ -1034,11 +1098,13 @@ Graph GraphExecutor::InitGraph(nnvm::Symbol symbol,
     num_forward_nodes_ = std::max(
         num_forward_nodes_, static_cast<size_t>(idx.outputs()[i].node_id + 1));
   }
+  fprintf(stderr, "[trace] Finished InitGraph.\n");
   return g;
 }
 
 // initialize the memory of each entries
 void GraphExecutor::InitDataEntryMemory(std::vector<NDArray>* shared_pool) {
+  fprintf(stderr, "[trace] In InitDataEntryMemory.\n");
   using nnvm::DTypeVector;
   using nnvm::ShapeVector;
   using nnvm::StorageVector;
@@ -1060,6 +1126,9 @@ void GraphExecutor::InitDataEntryMemory(std::vector<NDArray>* shared_pool) {
     for (uint32_t i = 0; i < idx[nid].source->num_outputs(); ++i) {
       auto eid = idx.entry_id(nid, i);
       data_context[eid] = vctx[nid];
+#if MXNET_MEMORY_PROFILER_ON
+      data_context[eid].name = L"forward_features:" + std::wstring(idx[nid].source->attrs.name.begin(), idx[nid].source->attrs.name.end());
+#endif
       CHECK_NE(vstorage_type[nid], kUndefinedStorage);
       data_storage_type[eid] = (NDArrayStorageType) vstorage_type[nid];
     }
@@ -1084,8 +1153,10 @@ void GraphExecutor::InitDataEntryMemory(std::vector<NDArray>* shared_pool) {
     auto data_eid = idx.entry_id(nid, 0);
     // initialize based on storage_type
     if (stype != kDefaultStorage) {
+      /*@ABHISHEK: This alloc doesnt have any source unclear tags*/
       data_entry_[data_eid] = NDArray(stype, vshape[eid], data_context[eid], true, vdtype[eid]);
     } else {
+      /*@ABHISHEK: This alloc doesnt have any source unclear tags*/
       data_entry_[data_eid] = NDArray(vshape[eid], data_context[eid], false, vdtype[eid]);
     }
     if (log_verbose_) {
@@ -1105,6 +1176,8 @@ void GraphExecutor::InitDataEntryMemory(std::vector<NDArray>* shared_pool) {
       pool_info.resize(sid + 1, PoolEntry{Context::CPU(), size_t(0), kUndefinedStorage});
     }
     PoolEntry& info = pool_info[sid];
+    /*@ABHISHEK: No src unclear tags here*/
+    //fprintf(stderr, "[datacontext1] %ls\n",data_context[i].name.c_str());
     if (info.bytes == 0) {
       info = PoolEntry{data_context[i], bytes, data_storage_type[i]};
     } else {
@@ -1116,6 +1189,8 @@ void GraphExecutor::InitDataEntryMemory(std::vector<NDArray>* shared_pool) {
   if (shared_pool != nullptr) {
     for (const NDArray& nd : *shared_pool) {
       size_t bytes = nd.shape().Size() * mshadow::mshadow_sizeof(nd.dtype());
+      /*@ABHISHEK: src unclear tags start appearing here*/
+      //fprintf(stderr, "[sharedpool] %ls\n",nd.ctx().name.c_str());
       free_pool.insert(std::make_pair(bytes, nd));
     }
   }
@@ -1135,6 +1210,7 @@ void GraphExecutor::InitDataEntryMemory(std::vector<NDArray>* shared_pool) {
 
   for (size_t i : sorted_pool_index) {
     const Context& ctx = pool_info[i].ctx;
+    //fprintf(stderr, "[poolinfo] %ls\n", ctx.name.c_str());
     size_t bytes = pool_info[i].bytes;
     bool allocated = false;
     for (auto it = free_pool.lower_bound(bytes); it != free_pool.end(); ++it) {
@@ -1152,6 +1228,9 @@ void GraphExecutor::InitDataEntryMemory(std::vector<NDArray>* shared_pool) {
       TShape shape{static_cast<nnvm::dim_t>(nword)};
       // TODO(junwu): adding delay_alloc=true to create nd
       // is a temporary solution.
+      /*@ABHISHEK: from this function, this block is the only source of unclear allocs. Therefore whatever
+       * is untagged its in pool_info
+       * */
       NDArray nd(shape, ctx, true);
       data_pool_[i] = nd;
       // put the new allocated arrays to shared pool
@@ -1173,16 +1252,19 @@ void GraphExecutor::InitDataEntryMemory(std::vector<NDArray>* shared_pool) {
       const NDArray& src = data_pool_.at(storage_id);
       data_entry_[i] = src.AsArray(vshape[i], vdtype[i]);
     } else {
+      /*@ABHISHEK: This alloc doesnt have any source unclear tags*/
       data_entry_[i] = NDArray(storage_type, vshape[i], data_context[i]);
     }
     if (log_verbose_) {
       LOG(INFO) << "\tinit data entry\t" << i << "\tas " << common::stype_string(storage_type);
     }
   }
+  fprintf(stderr, "[trace] Finished InitDataEntryMemory.\n");
 }
 
 
 void GraphExecutor::InitCachedOps() {
+  fprintf(stderr, "[trace] In InitCachedOps.\n");
   // get the graph
   const auto& idx = graph_.indexed_graph();
   const auto& vstorage_inplace =
@@ -1297,6 +1379,7 @@ void GraphExecutor::InitCachedOps() {
     op_nodes_[nid].mutate_vars = mutate_vars;
     op_nodes_[nid].use_vars = use_vars;
   }
+  fprintf(stderr, "[trace] Finished InitCachedOps.\n");
 }
 
 void GraphExecutor::InitOpSegs() {
@@ -1395,6 +1478,8 @@ void GraphExecutor::ExecuteMonCallback(size_t nid) {
   for (index_t i = 0; i < opnode.exec->out_array.size(); ++i) {
     NDArray *cpy = new NDArray(opnode.exec->out_array[i]);
     std::string name = inode.source->attrs.name + "_" + output_names[i];
+    //fprintf(stderr, "[trace_executemoncallback] Untagged NDarray alloc: %ls \n.",
+    //        std::wstring(name.begin(), name.end()).c_str());
     this->monitor_callback_(name.c_str(), reinterpret_cast<void*>(cpy));
   }
 }
@@ -1537,9 +1622,15 @@ GraphExecutor::CachedSegOpr GraphExecutor::CreateCachedSegOpr(size_t topo_start,
 Executor *Executor::SimpleBind(nnvm::Symbol symbol,
                                const Context& default_ctx,
                                const std::map<std::string, Context>& group2ctx,
+#if MXNET_MEMORY_PROFILER_ON
+                               std::vector<Context>& in_arg_ctxes,
+                               std::vector<Context>& arg_grad_ctxes,
+                               std::vector<Context>& aux_state_ctxes,
+#else
                                const std::vector<Context>& in_arg_ctxes,
                                const std::vector<Context>& arg_grad_ctxes,
                                const std::vector<Context>& aux_state_ctxes,
+#endif
                                const std::unordered_map<std::string, TShape>& arg_shape_map,
                                const std::unordered_map<std::string, int>& arg_dtype_map,
                                const std::unordered_map<std::string, int>& arg_stype_map,
@@ -1550,6 +1641,7 @@ Executor *Executor::SimpleBind(nnvm::Symbol symbol,
                                std::vector<NDArray>* aux_states,
                                std::unordered_map<std::string, NDArray>* shared_buffer,
                                Executor* shared_exec) {
+  fprintf(stderr, "[trace] In SimpleBind.\n");
   auto exec = new exec::GraphExecutor();
   exec->Init(symbol, default_ctx, group2ctx,
              in_arg_ctxes, arg_grad_ctxes, aux_state_ctxes,
@@ -1557,6 +1649,9 @@ Executor *Executor::SimpleBind(nnvm::Symbol symbol,
              grad_req_types, shared_arg_names,
              in_args, arg_grads, aux_states,
              shared_buffer, shared_exec);
+#if MXNET_MEMORY_PROFILER_ON
+  fprintf(stderr, "[mem] finished binding.\n");
+#endif
   return exec;
 }
 
@@ -1568,10 +1663,12 @@ Executor *Executor::Bind(nnvm::Symbol symbol,
                          const std::vector<OpReqType> &grad_req_type,
                          const std::vector<NDArray> &aux_states,
                          Executor* shared_exec) {
+  fprintf(stderr, "[trace] In Bind.\n");
   auto exec = new exec::GraphExecutor();
   exec->Init(symbol, default_ctx, group2ctx,
              in_args, arg_grad_store, grad_req_type, aux_states,
              reinterpret_cast<Executor*>(shared_exec));
+  fprintf(stderr, "[trace] Finished Bind.\n");
   return exec;
 }
 }  // namespace mxnet
diff --git a/src/executor/graph_executor.h b/src/executor/graph_executor.h
index 8b25404..db96d3e 100644
--- a/src/executor/graph_executor.h
+++ b/src/executor/graph_executor.h
@@ -89,9 +89,15 @@ class GraphExecutor : public Executor {
   void Init(nnvm::Symbol symbol,
             const Context& default_ctx,
             const std::map<std::string, Context>& ctx_map,
+#if MXNET_MEMORY_PROFILER_ON
+            std::vector<Context>& in_arg_ctxes,
+            std::vector<Context>& arg_grad_ctxes,
+            std::vector<Context>& aux_state_ctxes,
+#else
             const std::vector<Context>& in_arg_ctxes,
             const std::vector<Context>& arg_grad_ctxes,
             const std::vector<Context>& aux_state_ctxes,
+#endif
             const std::unordered_map<std::string, TShape>& arg_shape_map,
             const std::unordered_map<std::string, int>& arg_dtype_map,
             const std::unordered_map<std::string, int>& arg_stype_map,
@@ -142,9 +148,15 @@ class GraphExecutor : public Executor {
                      const nnvm::ShapeVector& inferred_shapes,
                      const nnvm::DTypeVector& inferred_dtypes,
                      const StorageTypeVector& inferred_stypes,
+#if MXNET_MEMORY_PROFILER_ON
+                     std::vector<Context>& in_arg_ctxes,
+                     std::vector<Context>& arg_grad_ctxes,
+                     std::vector<Context>& aux_state_ctxes,
+#else
                      const std::vector<Context>& in_arg_ctxes,
                      const std::vector<Context>& arg_grad_ctxes,
                      const std::vector<Context>& aux_state_ctxes,
+#endif
                      const std::vector<OpReqType>& grad_req_types,
                      std::vector<NDArray>* in_arg_vec,
                      std::vector<NDArray>* arg_grad_vec,
@@ -155,9 +167,15 @@ class GraphExecutor : public Executor {
                      const nnvm::ShapeVector& inferred_shapes,
                      const nnvm::DTypeVector& inferred_dtypes,
                      const StorageTypeVector& inferred_stypes,
+#if MXNET_MEMORY_PROFILER_ON
+                     std::vector<Context>& in_arg_ctxes,
+                     std::vector<Context>& arg_grad_ctxes,
+                     std::vector<Context>& aux_state_ctxes,
+#else
                      const std::vector<Context>& in_arg_ctxes,
                      const std::vector<Context>& arg_grad_ctxes,
                      const std::vector<Context>& aux_state_ctxes,
+#endif
                      const std::vector<OpReqType>& grad_req_types,
                      const std::unordered_set<std::string>& shared_arg_names,
                      const Executor* shared_exec,
diff --git a/src/imperative/imperative.cc b/src/imperative/imperative.cc
index fc35c49..a93a8f3 100644
--- a/src/imperative/imperative.cc
+++ b/src/imperative/imperative.cc
@@ -19,6 +19,7 @@
 #include <unordered_set>
 #include <iostream>
 #include "./imperative_utils.h"
+//#include "../common/utils.h"
 
 namespace mxnet {
 #if DMLC_CXX11_THREAD_LOCAL
diff --git a/src/imperative/imperative_utils.h b/src/imperative/imperative_utils.h
index 85e01b1..49a5cc1 100644
--- a/src/imperative/imperative_utils.h
+++ b/src/imperative/imperative_utils.h
@@ -79,7 +79,12 @@ inline Context GetContext(const nnvm::NodeAttrs& attrs,
 }
 
 // Set the shape, dtype, storage type and dispatch mode via the attribute inference functions
-inline void SetShapeType(const Context& ctx,
+inline void SetShapeType(
+#if MXNET_MEMORY_PROFILER_ON
+                  Context& ctx,
+#else
+                  const Context& ctx,
+#endif
                   const nnvm::NodeAttrs& attrs,
                   const std::vector<NDArray*>& inputs,
                   const std::vector<NDArray*>& outputs,
@@ -88,6 +93,15 @@ inline void SetShapeType(const Context& ctx,
   static auto& infertype = nnvm::Op::GetAttr<nnvm::FInferType>("FInferType");
   static auto& inferstorage = nnvm::Op::GetAttr<FInferStorageType>("FInferStorageType");
   MXAPIThreadLocalEntry *ret = MXAPIThreadLocalStore::Get();
+#if MXNET_MEMORY_PROFILER_ON
+  if (ctx.name==L"warning!,ctx_source_unclear" || ctx.name==L"untagged") {
+//    fprintf(stderr, "[trace_setshapetype] %s to %s\n", ctx.name.c_str(), attrs.op->name.c_str());
+//    fprintf(stderr, "[''] attr.name:%s\n",attrs.name.c_str());
+//    fprintf(stderr, "[''] attr.op->name:%s\n",attrs.op->name.c_str());
+//    fprintf(stderr, "[''] attr.op->description:%s\n",attrs.op->description.c_str());
+    ctx.set_name(std::wstring(attrs.op->name.begin(),attrs.op->name.end()));
+  }
+#endif
   // infer shape
   std::vector<TShape>& in_shapes  = ret->arg_shapes;
   in_shapes.clear();
diff --git a/src/io/iter_sparse_prefetcher.h b/src/io/iter_sparse_prefetcher.h
index 3908f9b..317c901 100644
--- a/src/io/iter_sparse_prefetcher.h
+++ b/src/io/iter_sparse_prefetcher.h
@@ -96,9 +96,16 @@ class SparsePrefetcherIter : public PrefetcherIter {
             auto& indptr = batch.data[data_iter + 2];
             // allocate memory
             CHECK_EQ(indices.shape_.Size(), values.shape_.Size());
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO:, verify that tag is okay*/
+            nd.CheckAndAllocAuxData(csr::kIdx, indices.shape_, L"data:iter_sparse_prefetcher");
+            nd.CheckAndAllocData(values.shape_, L"data:iter_sparse_prefetcher");
+            nd.CheckAndAllocAuxData(csr::kIndPtr, indptr.shape_, L"data:iter_sparse_prefetcher");
+#else
             nd.CheckAndAllocAuxData(csr::kIdx, indices.shape_);
             nd.CheckAndAllocData(values.shape_);
             nd.CheckAndAllocAuxData(csr::kIndPtr, indptr.shape_);
+#endif
             // copy values, indices and indptr
             CopyFromTo(data_i.data(), values);
             CopyFromTo(data_i.aux_data(csr::kIdx), indices);
diff --git a/src/kvstore/comm.h b/src/kvstore/comm.h
index deed1a1..a23a436 100644
--- a/src/kvstore/comm.h
+++ b/src/kvstore/comm.h
@@ -273,7 +273,12 @@ class CommCPU : public Comm {
     const TBlob& idx_data = indices.data();
     const size_t row_length = src.shape().ProdShape(1, src.shape().ndim());
     const size_t num_rows_retained = idx_data.Size();
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify tag okay */
+    dst->CheckAndAlloc({Shape1(num_rows_retained)}, L"cpu_to_gpu:comm");
+#else
     dst->CheckAndAlloc({Shape1(num_rows_retained)});
+#endif
     TBlob dst_data = dst->data();
     TBlob dst_idx_data = dst->aux_data(rowsparse::kIdx);
     MSHADOW_TYPE_SWITCH(src.dtype(), DType, {
@@ -355,7 +360,12 @@ class CommCPU : public Comm {
         // the one left are unique non-zero rows
         size_t nnr = indices.size();
         // allocate memory for output
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify tag okay */
+        out->CheckAndAlloc({Shape1(nnr)}, L"reduce_sum_cpu:comm");
+#else
         out->CheckAndAlloc({Shape1(nnr)});
+#endif
         auto idx_data = out->aux_data(kIdx).FlatTo1D<cpu, IType>();
         auto val_data = out->data().FlatTo2D<cpu, DType>();
 
diff --git a/src/kvstore/kvstore_dist.h b/src/kvstore/kvstore_dist.h
index 5e62be8..c715669 100644
--- a/src/kvstore/kvstore_dist.h
+++ b/src/kvstore/kvstore_dist.h
@@ -325,7 +325,12 @@ class KVStoreDist : public KVStoreLocal {
                              (RunContext rctx, Engine::CallbackOnComplete cb) {
       // allocate memory for the buffer
       size_t num_rows = indices.shape().Size();
+#if MXNET_MEMORY_PROFILER_ON
+	  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+      recv_buf.CheckAndAlloc({mshadow::Shape1(num_rows)}, L"pull_row_sparse:kvstore_dist");
+#else
       recv_buf.CheckAndAlloc({mshadow::Shape1(num_rows)});
+#endif
 #if MKL_EXPERIMENTAL == 1
       mkl_set_tblob_eager_mode(recv_buf.data());
 #endif
diff --git a/src/kvstore/kvstore_dist_server.h b/src/kvstore/kvstore_dist_server.h
index bedb539..e1ca4bc 100644
--- a/src/kvstore/kvstore_dist_server.h
+++ b/src/kvstore/kvstore_dist_server.h
@@ -232,7 +232,12 @@ class KVStoreDistServer {
         stored = NDArray(kRowSparseStorage, dshape, Context());
         Engine::Get()->PushSync([recved, stored](RunContext ctx) {
             NDArray rsp = stored;
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+            stored.CheckAndAlloc({mshadow::Shape1(recved.shape()[0])}, L"data_handle_row_sparse:kvstore_dist_server");
+#else
             stored.CheckAndAlloc({mshadow::Shape1(recved.shape()[0])});
+#endif
             mshadow::Stream<cpu> *s = ctx.get_stream<cpu>();
             op::PopulateFullIdxRspImpl(s, &rsp);
             mshadow::Copy(rsp.data().FlatTo1D<cpu, float>(),
diff --git a/src/ndarray/ndarray.cc b/src/ndarray/ndarray.cc
index dd43338..697ca07 100644
--- a/src/ndarray/ndarray.cc
+++ b/src/ndarray/ndarray.cc
@@ -406,9 +406,16 @@ inline void CopyFromToCsrImpl(const NDArray& from, const NDArray& to, RunContext
     return;
   }
   // Allocate storage
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+  to.CheckAndAllocAuxData(csr::kIndPtr, from.aux_shape(csr::kIndPtr), L"copy_from_to_csr:ndarray");
+  to.CheckAndAllocAuxData(csr::kIdx, from.aux_shape(csr::kIdx), L"copy_from_to_csr:ndarray");
+  to.CheckAndAllocData(from.aux_shape(csr::kIdx), L"copy_from_to_csr:ndarray");
+#else
   to.CheckAndAllocAuxData(csr::kIndPtr, from.aux_shape(csr::kIndPtr));
   to.CheckAndAllocAuxData(csr::kIdx, from.aux_shape(csr::kIdx));
   to.CheckAndAllocData(from.aux_shape(csr::kIdx));
+#endif
   TBlob val = to.data();
   TBlob indptr = to.aux_data(csr::kIndPtr);
   TBlob idx = to.aux_data(csr::kIdx);
@@ -432,7 +439,12 @@ inline void CopyFromToRspImpl(const NDArray& from, const NDArray& to, RunContext
     return;
   }
   auto aux_shape = from.aux_shape(rowsparse::kIdx);
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+  to.CheckAndAlloc({aux_shape}, L"copy_from_to_rsp_imp:ndarray");
+#else
   to.CheckAndAlloc({aux_shape});
+#endif
   TBlob val = to.data();
   TBlob idx = to.aux_data(rowsparse::kIdx);
   ndarray::Copy<from_xpu, to_xpu>(from.data(), &val,
@@ -1122,12 +1134,27 @@ void NDArray::SyncCopyFromNDArray(const NDArray& src, int i, int j) {
   // else if dst is not initialized, allocate corresponding data blob for it
   auto get_dst_data = [&](const TShape& src_shape) {
     if (this->storage_type() == kDefaultStorage) {
+#if MXNET_MEMORY_PROFILER_ON
+	  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+      this->ReshapeAndAlloc(src_shape, L"sync_copy_from_ndarray:ndarray");
+#else
       this->ReshapeAndAlloc(src_shape);
+#endif
     } else if (!this->storage_initialized()) {
       if (j < 0) {
+#if MXNET_MEMORY_PROFILER_ON
+	    /*@ABHISHEK @TODO: Verify that the tag is okay*/
+        this->CheckAndAllocData(src_shape, L"sync_copy_from_ndarray:ndarray");
+#else
         this->CheckAndAllocData(src_shape);
+#endif
       } else {
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+        this->CheckAndAllocAuxData(j, src_shape, L"sync_copy_from_ndarray:ndarray");
+#else
         this->CheckAndAllocAuxData(j, src_shape);
+#endif
       }
     }
     TBlob dst_data = (j >= 0? this->aux_data(j) : this->data());
@@ -1192,7 +1219,6 @@ void NDArray::SyncCopyToCPU(void *data, size_t size) const {
   CHECK_EQ(dshape.Size(), size)
       << "Memory size do not match";
   TBlob dst(data, dshape, cpu::kDevMask, this->dtype_, 0); // NOLINT(*)
-
   if (this->ctx().dev_mask() == cpu::kDevMask) {
     this->WaitToRead();
     RunContext rctx{this->ctx(), nullptr};
diff --git a/src/ndarray/ndarray_function.cc b/src/ndarray/ndarray_function.cc
index ef0adbe..8581278 100644
--- a/src/ndarray/ndarray_function.cc
+++ b/src/ndarray/ndarray_function.cc
@@ -158,7 +158,12 @@ void ElementwiseSumRsp(mshadow::Stream<cpu>* s,
       //            allocating it directly in GetUniqueRspRowIdx
       std::vector<IType> uniq_row_idx;
       GetUniqueRspRowIdx(nds, &uniq_row_idx);
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+      out->CheckAndAlloc({mshadow::Shape1(uniq_row_idx.size())}, L"element_wise_sum_rsp:ndarray_function");
+#else
       out->CheckAndAlloc({mshadow::Shape1(uniq_row_idx.size())});
+#endif
       out->data().FlatTo2D<cpu, DType>() = static_cast<DType>(0);
       ElementwiseSumRspImpl<DType, IType>(s, nds, uniq_row_idx, out, omp_get_max_threads());
     });
diff --git a/src/ndarray/ndarray_function.cu b/src/ndarray/ndarray_function.cu
index 8accc2b..7fa26ad 100644
--- a/src/ndarray/ndarray_function.cu
+++ b/src/ndarray/ndarray_function.cu
@@ -132,9 +132,16 @@ void ElementwiseSumRspImpl(mshadow::Stream<gpu>* s,
                                     row_flg,
                                     num_rows,
                                     mshadow::Stream<gpu>::GetStream(s));
+#if MXNET_MEMORY_PROFILER_ON
+      mshadow::Tensor<gpu, 1, char> workspace = rsc
+          .get_space_typed<gpu, 1, char>(mshadow::Shape1(num_rows * sizeof(IType) +
+                                                         temp_storage_bytes), s,
+                             L"workspace:ndarray_function");
+#else
       mshadow::Tensor<gpu, 1, char> workspace = rsc
           .get_space_typed<gpu, 1, char>(mshadow::Shape1(num_rows * sizeof(IType) +
                                                          temp_storage_bytes), s);
+#endif
       row_flg = reinterpret_cast<IType*>(workspace.dptr_);
       d_temp_storage = workspace.dptr_ + num_rows*sizeof(IType);
       // Mark row_flg array with 0 for zero rows and 1 for non-zero rows
@@ -160,7 +167,13 @@ void ElementwiseSumRspImpl(mshadow::Stream<gpu>* s,
       dim_t nnr_out = 0;
       CUDA_CALL(cudaMemcpy(&nnr_out, &row_flg[num_rows-1], sizeof(dim_t),
                            cudaMemcpyDeviceToHost));
+    /*@ABHISHEK: This copies to Host, should we be tracking it?*/
+#if MXNET_MEMORY_PROFILER_ON
+    /*@ABHISHEK @TODO: Verify that the tag is okay*/
+      out->CheckAndAlloc({mshadow::Shape1(nnr_out)}, L"element_wise_sum_rsp_impl:ndarray_function");
+#else
       out->CheckAndAlloc({mshadow::Shape1(nnr_out)});
+#endif
       IType* out_row_idx = out->aux_data(kIdx).dptr<IType>();
       DType* out_data = out->data().dptr<DType>();
       // Fill row_idx array of output using row_flg
diff --git a/src/operator/batch_norm_v1-inl.h b/src/operator/batch_norm_v1-inl.h
index ebfc469..ea825fb 100644
--- a/src/operator/batch_norm_v1-inl.h
+++ b/src/operator/batch_norm_v1-inl.h
@@ -183,8 +183,13 @@ class BatchNormV1Op : public Operator {
 
     if (ctx.is_train && !param_.use_global_stats) {
       // get requested temp space
+#if MXNET_MEMORY_PROFILER_ON
+      Tensor<xpu, 2> workspace = ctx.requested[batchnorm_v1::kTempSpace].get_space<xpu>(
+          mshadow::Shape2(3, mean.shape_[0]), s, L"workspace:batch_norm_v1-inl");
+#else
       Tensor<xpu, 2> workspace = ctx.requested[batchnorm_v1::kTempSpace].get_space<xpu>(
           mshadow::Shape2(3, mean.shape_[0]), s);
+#endif
       Tensor<xpu, 1> gmean = workspace[0];
       Tensor<xpu, 1> gvar = workspace[1];
       Tensor<xpu, 1> tmp = workspace[2];
diff --git a/src/operator/contrib/ctc_loss-inl.h b/src/operator/contrib/ctc_loss-inl.h
index cb8d27a..3ac30cf 100644
--- a/src/operator/contrib/ctc_loss-inl.h
+++ b/src/operator/contrib/ctc_loss-inl.h
@@ -397,9 +397,16 @@ class CTCLossOp : public Operator {
                                             &workspace_bytes));
     workspace_size = (workspace_bytes + sizeof(real_t) - 1)/sizeof(real_t);
 
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, real_t> temp_space =
+      ctx.requested[ctc_loss::kTempSpace].get_space_typed<xpu, 1, real_t>(
+          mshadow::Shape1(workspace_size+data.shape_.FlatTo1D()[0]), s,
+      L"workspace:ctc_loss-inl");
+#else
     Tensor<xpu, 1, real_t> temp_space =
       ctx.requested[ctc_loss::kTempSpace].get_space_typed<xpu, 1, real_t>(
           mshadow::Shape1(workspace_size+data.shape_.FlatTo1D()[0]), s);
+#endif
 
     Tensor<gpu, 1, real_t> work_space(temp_space.dptr_,
                                       mshadow::Shape1(workspace_size), s);
@@ -452,9 +459,15 @@ class CTCLossOp : public Operator {
 
     // round-up so there are enough elems in memory
     int num_tmp_elems = (size_bytes + sizeof(real_t) - 1) / sizeof(real_t);
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, real_t> workspace =
+        ctx.requested[ctc_loss::kTempSpace].get_space_typed<xpu, 1, real_t>(
+            Shape1(num_tmp_elems), s, L"workspace:ctc_loss-inl");
+#else
     Tensor<xpu, 1, real_t> workspace =
         ctx.requested[ctc_loss::kTempSpace].get_space_typed<xpu, 1, real_t>(
             Shape1(num_tmp_elems), s);
+#endif
 
     compute_ctc_cost(data, costs.dptr_, grad.dptr_, packed_labels->data(),
                      label_lengths->data(), data_lengths->data(),
diff --git a/src/operator/contrib/deformable_convolution-inl.h b/src/operator/contrib/deformable_convolution-inl.h
index cc9a93a..956f7eb 100644
--- a/src/operator/contrib/deformable_convolution-inl.h
+++ b/src/operator/contrib/deformable_convolution-inl.h
@@ -124,8 +124,13 @@ class DeformableConvolutionOp : public Operator {
                out_data[conv::kOut].shape_);
     Stream<xpu>* s = ctx.get_stream<xpu>();
     // allocate workspace for col_buffer
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace = ctx.requested[conv::kTempSpace]
+      .get_space_typed<xpu, 1, DType>(Shape1(col_buffer_size_), s, L"workspace:deformable_convolution-inl");
+#else
     Tensor<xpu, 1, DType> workspace = ctx.requested[conv::kTempSpace]
       .get_space_typed<xpu, 1, DType>(Shape1(col_buffer_size_), s);
+#endif
     // calculate the shape of col_buffer
     TShape col_buffer_shape(num_spatial_axes_ + 1);
     col_buffer_shape[0] = conv_in_channels_ * param_.kernel.Size();
@@ -186,8 +191,13 @@ class DeformableConvolutionOp : public Operator {
                out_grad[conv::kOut].shape_);
     Stream<xpu> *s = ctx.get_stream<xpu>();
     // allocate workspace for col_buffer
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace = ctx.requested[conv::kTempSpace]
+      .get_space_typed<xpu, 1, DType>(Shape1(col_buffer_size_), s, L"workspace:deformable_convolution-inl");
+#else
     Tensor<xpu, 1, DType> workspace = ctx.requested[conv::kTempSpace]
       .get_space_typed<xpu, 1, DType>(Shape1(col_buffer_size_), s);
+#endif
     // calculate the shape of col_buffer
     TShape col_buffer_shape(num_spatial_axes_ + 1);
     col_buffer_shape[0] = conv_in_channels_ * param_.kernel.Size();
diff --git a/src/operator/contrib/fft-inl.h b/src/operator/contrib/fft-inl.h
index 129aaa3..e4da435 100644
--- a/src/operator/contrib/fft-inl.h
+++ b/src/operator/contrib/fft-inl.h
@@ -98,9 +98,15 @@ class FFTOp : public Operator {
           Shape2(n_ffts, dim_*2), s);
 
     // need temp space to pad the data into complex numbers due to cufft interface
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace =
+            ctx.requested[fft::kTempSpace].get_space_typed<xpu, 1, DType>(
+                Shape1(param_.compute_size*dim_*2), s, L"workspace:fft-inl");
+#else
     Tensor<xpu, 1, DType> workspace =
             ctx.requested[fft::kTempSpace].get_space_typed<xpu, 1, DType>(
                 Shape1(param_.compute_size*dim_*2), s);
+#endif
     Tensor<xpu, 2, DType> complex_data = Tensor<xpu, 2, DType>(workspace.dptr_,
                                               Shape2(param_.compute_size, dim_*2), s);
     // start fft
@@ -159,9 +165,15 @@ class FFTOp : public Operator {
     Tensor<xpu, 2, DType> grad = out_grad[fft::kOutComplex].get_with_shape<xpu, 2, DType>(
           Shape2(n_ffts, dim_*2), s);
     // need temp space to pad the data into complex numbers due to cufft interface
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace =
+            ctx.requested[fft::kTempSpace].get_space_typed<xpu, 1, DType>(
+                Shape1(param_.compute_size*dim_*2), s, L"workspace:fft-inl");
+#else
     Tensor<xpu, 1, DType> workspace =
             ctx.requested[fft::kTempSpace].get_space_typed<xpu, 1, DType>(
                 Shape1(param_.compute_size*dim_*2), s);
+#endif
     Tensor<xpu, 2, DType> complex_data = Tensor<xpu, 2, DType>(workspace.dptr_,
                                               Shape2(param_.compute_size, dim_*2), s);
 
diff --git a/src/operator/contrib/ifft-inl.h b/src/operator/contrib/ifft-inl.h
index 1b0e5e5..588e52b 100644
--- a/src/operator/contrib/ifft-inl.h
+++ b/src/operator/contrib/ifft-inl.h
@@ -94,9 +94,15 @@ class IFFTOp : public Operator {
     Tensor<xpu, 2, DType> out = out_data[ifft::kOut].get_with_shape<xpu, 2, DType>(
           Shape2(n_iffts, dim_), s);
     // need temp space to store the intermediate complex matrices
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace =
+            ctx.requested[ifft::kTempSpace].get_space_typed<xpu, 1, DType>(
+                Shape1(param_.compute_size*dim_*2), s, L"workspace:ifft-inl");
+#else
     Tensor<xpu, 1, DType> workspace =
             ctx.requested[ifft::kTempSpace].get_space_typed<xpu, 1, DType>(
                 Shape1(param_.compute_size*dim_*2), s);
+#endif
     Tensor<xpu, 2, DType> complex_data = Tensor<xpu, 2, DType>(workspace.dptr_,
                                               Shape2(param_.compute_size, dim_*2), s);
     // start ifft
@@ -156,9 +162,15 @@ class IFFTOp : public Operator {
     Tensor<xpu, 2, DType> grad = out_grad[ifft::kOut].get_with_shape<xpu, 2, DType>(
           Shape2(n_iffts, dim_), s);
     // need temp space to pad the data into complex numbers due to cufft interface
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace =
+            ctx.requested[ifft::kTempSpace].get_space_typed<xpu, 1, DType>(
+                Shape1(param_.compute_size*dim_*2), s, L"workspace:ifft-inl");
+#else
     Tensor<xpu, 1, DType> workspace =
             ctx.requested[ifft::kTempSpace].get_space_typed<xpu, 1, DType>(
                 Shape1(param_.compute_size*dim_*2), s);
+#endif
     Tensor<xpu, 2, DType> complex_data = Tensor<xpu, 2, DType>(workspace.dptr_,
                                               Shape2(param_.compute_size, dim_*2), s);
     // start fft
diff --git a/src/operator/contrib/multi_proposal.cu b/src/operator/contrib/multi_proposal.cu
index 082de6a..871f271 100644
--- a/src/operator/contrib/multi_proposal.cu
+++ b/src/operator/contrib/multi_proposal.cu
@@ -53,6 +53,8 @@
     CHECK_EQ(error, cudaSuccess) << " " << cudaGetErrorString(error); \
 } while (0)
 
+const int MB_Size = 1024 * 1024;
+
 namespace mshadow {
 namespace cuda {
 namespace multi_proposal {
@@ -338,6 +340,15 @@ void _nms(const mshadow::Tensor<gpu, 2>& boxes,
   FRCNN_CUDA_CHECK(cudaMalloc(&mask_dev,
                               boxes_num * col_blocks * sizeof(uint64_t)));
 
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (mask_workspace).\n", boxes_num * col_blocks * sizeof(uint64_t) / MB_Size);
+  size_t free_byte;
+  size_t total_byte;
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
   dim3 blocks(DIVUP(boxes_num, threadsPerBlock),
               DIVUP(boxes_num, threadsPerBlock));
   dim3 threads(threadsPerBlock);
@@ -471,6 +482,16 @@ class MultiProposalGPUOp : public Operator{
     float* workspace_proposals_ptr = NULL;
     FRCNN_CUDA_CHECK(cudaMalloc(&workspace_proposals_ptr,
                                 sizeof(float) * num_images * count_anchors * 5));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (proposal_workspace).\n", sizeof(float) * num_images * count_anchors * 5 / MB_Size);
+  size_t free_byte;
+  size_t total_byte;
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
     Tensor<xpu, 3> workspace_proposals(workspace_proposals_ptr,
                                        Shape3(num_images, count_anchors, 5));
     FRCNN_CUDA_CHECK(cudaMemcpy(workspace_proposals.dptr_, &anchors[0],
@@ -511,20 +532,52 @@ class MultiProposalGPUOp : public Operator{
     // Copy score to a continuous memory
     float* score_ptr = NULL;
     FRCNN_CUDA_CHECK(cudaMalloc(&score_ptr, sizeof(float) * count_anchors));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (proposal_score).\n", sizeof(float) * count_anchors / MB_Size);
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
     Tensor<xpu, 1> score(score_ptr, Shape1(count_anchors));
     int* order_ptr = NULL;
     FRCNN_CUDA_CHECK(cudaMalloc(&order_ptr, sizeof(int) * count_anchors));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (proposal_order).\n", sizeof(int) * count_anchors / MB_Size);
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
     Tensor<xpu, 1, int> order(order_ptr, Shape1(count_anchors));
 
     float* workspace_ordered_proposals_ptr = NULL;
     FRCNN_CUDA_CHECK(cudaMalloc(&workspace_ordered_proposals_ptr,
         sizeof(float) * rpn_pre_nms_top_n * 5));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (proposal_workspace_order).\n", sizeof(float) * rpn_pre_nms_top_n * 5 / MB_Size);
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
     Tensor<xpu, 2> workspace_ordered_proposals(workspace_ordered_proposals_ptr,
         Shape2(rpn_pre_nms_top_n, 5));
 
     int* keep;
     FRCNN_CUDA_CHECK(cudaMalloc(&keep, sizeof(int) * rpn_pre_nms_top_n));
 
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (keep).\n", sizeof(int) * rpn_pre_nms_top_n / MB_Size);
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
+
     for (int b = 0; b < num_images; b++) {
         CheckLaunchParam(dimGrid, dimBlock, "CopyScore");
         CopyScoreKernel << <dimGrid, dimBlock >> >(
diff --git a/src/operator/contrib/multibox_detection-inl.h b/src/operator/contrib/multibox_detection-inl.h
index 34099a3..5e181e4 100644
--- a/src/operator/contrib/multibox_detection-inl.h
+++ b/src/operator/contrib/multibox_detection-inl.h
@@ -98,8 +98,13 @@ class MultiBoxDetectionOp : public Operator {
        .get_with_shape<xpu, 2, DType>(Shape2(ashape[1], 4), s);
      Tensor<xpu, 3, DType> out = out_data[mboxdet_enum::kOut]
        .get<xpu, 3, DType>(s);
+#if MXNET_MEMORY_PROFILER_ON
+     Tensor<xpu, 3, DType> temp_space = ctx.requested[mboxdet_enum::kTempSpace]
+       .get_space_typed<xpu, 3, DType>(out.shape_, s, L"workspace:multibox_detection-inl");
+#else
      Tensor<xpu, 3, DType> temp_space = ctx.requested[mboxdet_enum::kTempSpace]
        .get_space_typed<xpu, 3, DType>(out.shape_, s);
+#endif
      out = -1.f;
      MultiBoxDetectionForward(out, cls_prob, loc_pred, anchors, temp_space,
        param_.threshold, param_.clip, param_.variances, param_.nms_threshold,
diff --git a/src/operator/contrib/multibox_target-inl.h b/src/operator/contrib/multibox_target-inl.h
index 872ddde..b574f73 100644
--- a/src/operator/contrib/multibox_target-inl.h
+++ b/src/operator/contrib/multibox_target-inl.h
@@ -116,8 +116,13 @@ class MultiBoxTargetOp : public Operator {
     index_t num_labels = labels.size(1);
     // TODO(zhreshold): use maximum valid ground-truth in batch rather than # in dataset
     Shape<4> temp_shape = Shape4(11, num_batches, num_anchors, num_labels);
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 4, DType> temp_space = ctx.requested[mboxtarget_enum::kTempSpace]
+      .get_space_typed<xpu, 4, DType>(temp_shape, s, L"workspace:multibox_target-inl");
+#else
     Tensor<xpu, 4, DType> temp_space = ctx.requested[mboxtarget_enum::kTempSpace]
       .get_space_typed<xpu, 4, DType>(temp_shape, s);
+#endif
     loc_target = 0.f;
     loc_mask = 0.0f;
     cls_target = param_.ignore_label;
diff --git a/src/operator/contrib/proposal.cc b/src/operator/contrib/proposal.cc
index ccb541a..6e39ed8 100644
--- a/src/operator/contrib/proposal.cc
+++ b/src/operator/contrib/proposal.cc
@@ -315,8 +315,13 @@ class ProposalOp : public Operator{
     int rpn_post_nms_top_n = std::min(param_.rpn_post_nms_top_n, rpn_pre_nms_top_n);
 
     int workspace_size = count * 5 + 2 * count + rpn_pre_nms_top_n * 5 + 3 * rpn_pre_nms_top_n;
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<cpu, 1> workspace = ctx.requested[proposal::kTempResource].get_space<cpu>(
+      Shape1(workspace_size), s, L"workspace:proposal");
+#else
     Tensor<cpu, 1> workspace = ctx.requested[proposal::kTempResource].get_space<cpu>(
       Shape1(workspace_size), s);
+#endif
     int start = 0;
     Tensor<cpu, 2> workspace_proposals(workspace.dptr_ + start, Shape2(count, 5));
     start += count * 5;
diff --git a/src/operator/contrib/proposal.cu b/src/operator/contrib/proposal.cu
index 9f56685..da8e890 100644
--- a/src/operator/contrib/proposal.cu
+++ b/src/operator/contrib/proposal.cu
@@ -51,6 +51,10 @@
     CHECK_EQ(error, cudaSuccess) << " " << cudaGetErrorString(error); \
 } while (0)
 
+#if MXNET_MEMORY_PROFILER_ON
+const int MB_Size = 1024 * 1024;
+#endif
+
 namespace mshadow {
 namespace cuda {
 
@@ -319,6 +323,15 @@ void _nms(const mshadow::Tensor<gpu, 2>& boxes,
   FRCNN_CUDA_CHECK(cudaMalloc(&mask_dev,
                               boxes_num * col_blocks * sizeof(uint64_t)));
 
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (mask_workspace).\n", boxes_num * col_blocks * sizeof(uint64_t) / MB_Size);
+  size_t free_byte;
+  size_t total_byte;
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
   dim3 blocks(DIVUP(boxes_num, threadsPerBlock),
               DIVUP(boxes_num, threadsPerBlock));
   dim3 threads(threadsPerBlock);
@@ -352,6 +365,13 @@ void _nms(const mshadow::Tensor<gpu, 2>& boxes,
   *num_out = num_to_keep;
 
   FRCNN_CUDA_CHECK(cudaFree(mask_dev));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Free %ld MB GPU memory (proposal_mask).\n", boxes_num * col_blocks * sizeof(uint64_t) / MB_Size);
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
 }
 
 // copy proposals to output
@@ -452,6 +472,16 @@ class ProposalGPUOp : public Operator{
     // Copy generated anchors to GPU
     float* workspace_proposals_ptr = NULL;
     FRCNN_CUDA_CHECK(cudaMalloc(&workspace_proposals_ptr, sizeof(float) * count * 5));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (proposal_workspace).\n", sizeof(float) * count * 5 / MB_Size);
+  size_t free_byte;
+  size_t total_byte;
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
     Tensor<xpu, 2> workspace_proposals(workspace_proposals_ptr, Shape2(count, 5));
     FRCNN_CUDA_CHECK(cudaMemcpy(workspace_proposals.dptr_,
                                 &anchors[0], sizeof(float) * anchors.size(),
@@ -502,9 +532,25 @@ class ProposalGPUOp : public Operator{
     // Copy score to a continuous memory
     float* score_ptr = NULL;
     FRCNN_CUDA_CHECK(cudaMalloc(&score_ptr, sizeof(float) * count));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (proposal_score).\n", sizeof(float) * count / MB_Size);
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
     Tensor<xpu, 1> score(score_ptr, Shape1(count));
     int* order_ptr = NULL;
     FRCNN_CUDA_CHECK(cudaMalloc(&order_ptr, sizeof(int) * count));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (proposal_order).\n", sizeof(int) * count / MB_Size);
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
     Tensor<xpu, 1, int> order(order_ptr, Shape1(count));
 
     CheckLaunchParam(dimGrid, dimBlock, "CopyScore");
@@ -524,6 +570,14 @@ class ProposalGPUOp : public Operator{
     float* workspace_ordered_proposals_ptr = NULL;
     FRCNN_CUDA_CHECK(cudaMalloc(&workspace_ordered_proposals_ptr,
                                 sizeof(float) * rpn_pre_nms_top_n * 5));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (proposal_workspace_order).\n", sizeof(float) * rpn_pre_nms_top_n * 5 / MB_Size);
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
     Tensor<xpu, 2> workspace_ordered_proposals(workspace_ordered_proposals_ptr,
                                                Shape2(rpn_pre_nms_top_n, 5));
 
@@ -548,6 +602,14 @@ class ProposalGPUOp : public Operator{
     // copy nms result to gpu
     int* keep;
     FRCNN_CUDA_CHECK(cudaMalloc(&keep, sizeof(int) * _keep.size()));
+
+#if MXNET_MEMORY_PROFILER_ON
+  #include <mxnet/base.h>
+  fprintf(stderr, "Allocate %ld MB GPU memory (keep).\n", sizeof(int) * _keep.size() / MB_Size);
+  cudaMemGetInfo(&free_byte, &total_byte) ;
+  fprintf(stderr, "%ld MB GPU global memory free, total %ld MB.\n", free_byte / MB_Size, total_byte / MB_Size);
+#endif
+
     FRCNN_CUDA_CHECK(cudaMemcpy(keep, &_keep[0], sizeof(int) * _keep.size(),
                                 cudaMemcpyHostToDevice));
 
diff --git a/src/operator/convolution-inl.h b/src/operator/convolution-inl.h
index 5843293..1112547 100644
--- a/src/operator/convolution-inl.h
+++ b/src/operator/convolution-inl.h
@@ -196,8 +196,13 @@ class ConvolutionOp : public Operator {
       }
     } else {
       // allocate workspace for col_buffer
+#if MXNET_MEMORY_PROFILER_ON
+      Tensor<xpu, 1, DType> workspace = ctx.requested[conv::kTempSpace]
+        .get_space_typed<xpu, 1, DType>(Shape1(col_buffer_size_), s, L"workspace:convolution-inl");
+#else
       Tensor<xpu, 1, DType> workspace = ctx.requested[conv::kTempSpace]
         .get_space_typed<xpu, 1, DType>(Shape1(col_buffer_size_), s);
+#endif
       // calculate the shape of col_buffer
       TShape col_buffer_shape(num_spatial_axes_ + 1);
       col_buffer_shape[0] = conv_in_channels_ * param_.kernel.Size();
@@ -281,8 +286,13 @@ class ConvolutionOp : public Operator {
       }
     } else {
       // allocate workspace for col_buffer
+#if MXNET_MEMORY_PROFILER_ON
+      Tensor<xpu, 1, DType> workspace = ctx.requested[conv::kTempSpace]
+        .get_space_typed<xpu, 1, DType>(Shape1(col_buffer_size_), s, L"workspace:convolution-inl");
+#else
       Tensor<xpu, 1, DType> workspace = ctx.requested[conv::kTempSpace]
         .get_space_typed<xpu, 1, DType>(Shape1(col_buffer_size_), s);
+#endif
       // calculate the shape of col_buffer
       TShape col_buffer_shape(num_spatial_axes_ + 1);
       col_buffer_shape[0] = conv_in_channels_ * param_.kernel.Size();
diff --git a/src/operator/convolution_v1-inl.h b/src/operator/convolution_v1-inl.h
index 6b31b15..c3869b1 100644
--- a/src/operator/convolution_v1-inl.h
+++ b/src/operator/convolution_v1-inl.h
@@ -145,9 +145,15 @@ class ConvolutionV1Op : public Operator {
         << "Must init CuBLAS handle in stream";
 #endif
     const index_t nbatch = data.size(0);
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace =
+        ctx.requested[conv_v1::kTempSpace].get_space_typed<xpu, 1, DType>(
+            Shape1(this->InitTemp(data.shape_, out.shape_)), s, L"workspace:convolution_v1-inl");
+#else
     Tensor<xpu, 1, DType> workspace =
         ctx.requested[conv_v1::kTempSpace].get_space_typed<xpu, 1, DType>(
             Shape1(this->InitTemp(data.shape_, out.shape_)), s);
+#endif
     for (index_t i = 0; i < nbatch; i += nstep_) {
       const index_t step = std::min(nstep_, nbatch - i);
       Tensor<xpu, 2, DType> temp_col = Tensor<xpu, 2, DType>(workspace.dptr_,
@@ -234,9 +240,15 @@ class ConvolutionV1Op : public Operator {
         << "Must init CuBLAS handle in stream";
 #endif
     const index_t nbatch = data.size(0);
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace =
+        ctx.requested[conv_v1::kTempSpace].get_space_typed<xpu, 1, DType>(
+            Shape1(this->InitTemp(data.shape_, grad.shape_)), s, L"workspace:convolution_v1-inl");
+#else
     Tensor<xpu, 1, DType> workspace =
         ctx.requested[conv_v1::kTempSpace].get_space_typed<xpu, 1, DType>(
             Shape1(this->InitTemp(data.shape_, grad.shape_)), s);
+#endif
     for (index_t i = 0; i < nbatch; i += nstep_) {
       const index_t step = std::min(nstep_, nbatch - i);
       Tensor<xpu, 2, DType> temp_col = Tensor<xpu, 2, DType>(workspace.dptr_,
diff --git a/src/operator/cudnn_convolution-inl.h b/src/operator/cudnn_convolution-inl.h
index b2b5994..ae57055 100644
--- a/src/operator/cudnn_convolution-inl.h
+++ b/src/operator/cudnn_convolution-inl.h
@@ -861,8 +861,13 @@ class CuDNNConvolutionOp : public Operator {
   mshadow::Tensor<gpu, 1, DType> AllocateTempWorkspace(const OpContext &ctx, size_t size_bytes) {
     mshadow::Stream<gpu> *s = ctx.get_stream<gpu>();
     size_t size_words = size_bytes / sizeof(DType) + 1;
+#if MXNET_MEMORY_PROFILER_ON
+    return ctx.requested[conv::kTempSpace].get_space_typed<gpu, 1, DType>(
+        mshadow::Shape1(size_words), s, L"workspace:cudnn_convolution-inl");
+#else
     return ctx.requested[conv::kTempSpace].get_space_typed<gpu, 1, DType>(
         mshadow::Shape1(size_words), s);
+#endif
   }
 
   // Returns the size in bytes of the 1D Tensor of words.
diff --git a/src/operator/cudnn_deconvolution-inl.h b/src/operator/cudnn_deconvolution-inl.h
index 5e9b7c5..e9bc704 100644
--- a/src/operator/cudnn_deconvolution-inl.h
+++ b/src/operator/cudnn_deconvolution-inl.h
@@ -891,8 +891,13 @@ class CuDNNDeconvolutionOp : public Operator {
   mshadow::Tensor<gpu, 1, DType> AllocateTempWorkspace(const OpContext &ctx, size_t size_bytes) {
     mshadow::Stream<gpu> *s = ctx.get_stream<gpu>();
     size_t size_words = size_bytes / sizeof(DType) + 1;
+#if MXNET_MEMORY_PROFILER_ON
+    return ctx.requested[deconv::kTempSpace].get_space_typed<gpu, 1, DType>(
+        mshadow::Shape1(size_words), s, L"workspace:deconvolution-inl");
+#else
     return ctx.requested[deconv::kTempSpace].get_space_typed<gpu, 1, DType>(
         mshadow::Shape1(size_words), s);
+#endif
   }
 
   // Returns the size in bytes of the 1D Tensor of words.
diff --git a/src/operator/cudnn_rnn-inl.h b/src/operator/cudnn_rnn-inl.h
index a260cb4..9e29865 100644
--- a/src/operator/cudnn_rnn-inl.h
+++ b/src/operator/cudnn_rnn-inl.h
@@ -144,9 +144,15 @@ class CuDNNRNNOp : public Operator {
     }
     // Get temp space
     int temp_size = workspace_size_;
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<gpu, 1, DType> temp_space =
+      ctx.requested[rnn_enum::kTempSpace].get_space_typed<gpu, 1, DType>(
+                              mshadow::Shape1(temp_size), s, L"workspace:cudnn_rnn-inl");
+#else
     Tensor<gpu, 1, DType> temp_space =
       ctx.requested[rnn_enum::kTempSpace].get_space_typed<gpu, 1, DType>(
                               mshadow::Shape1(temp_size), s);
+#endif
     if (ctx.is_train) {
       CUDNN_CALL(cudnnRNNForwardTraining(s->dnn_handle_,
                                          rnn_desc_,
@@ -258,9 +264,15 @@ class CuDNNRNNOp : public Operator {
 
     // Get temp space
     int temp_size = workspace_size_;
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<gpu, 1, DType> temp_space =
+      ctx.requested[rnn_enum::kTempSpace].get_space_typed<gpu, 1, DType>(
+                              mshadow::Shape1(temp_size), s, L"workspace:cudnn_rnn-inl");
+#else
     Tensor<gpu, 1, DType> temp_space =
       ctx.requested[rnn_enum::kTempSpace].get_space_typed<gpu, 1, DType>(
                               mshadow::Shape1(temp_size), s);
+#endif
     CUDNN_CALL(cudnnRNNBackwardData(s->dnn_handle_,
                                     rnn_desc_,
                                     param_.seq_length_,
@@ -447,7 +459,11 @@ class CuDNNRNNOp : public Operator {
       CUDNN_CALL(cudnnDropoutGetStatesSize(s->dnn_handle_,
                                            &dropout_byte_));
       dropout_size_ = dropout_byte_ / sizeof(DType);
+#if MXNET_MEMORY_PROFILER_ON
+      dropout_states_ = Storage::Get()->Alloc(dropout_byte_, Context::GPU(), L"workspace:cudnn_rnn-inl");
+#else
       dropout_states_ = Storage::Get()->Alloc(dropout_byte_, Context::GPU());
+#endif
       CUDNN_CALL(cudnnSetDropoutDescriptor(dropout_desc_,
                                            s->dnn_handle_,
                                            param_.p,  // keep probability
@@ -499,7 +515,11 @@ class CuDNNRNNOp : public Operator {
                                                 &reserve_space_byte_));
       workspace_size_ = workspace_byte_ / sizeof(DType);
       // Allocate the reserve space
+#if MXNET_MEMORY_PROFILER_ON
+      reserve_space_ = Storage::Get()->Alloc(reserve_space_byte_, Context::GPU(), L"workspace:cudnn_rnn-inl");
+#else
       reserve_space_ = Storage::Get()->Alloc(reserve_space_byte_, Context::GPU());
+#endif
 
       // Check that number of params are correct
       size_t cudnn_param_size;
diff --git a/src/operator/deconvolution-inl.h b/src/operator/deconvolution-inl.h
index 41fcf9b..c03fb37 100644
--- a/src/operator/deconvolution-inl.h
+++ b/src/operator/deconvolution-inl.h
@@ -236,9 +236,15 @@ class DeconvolutionOp : public Operator {
         << "Must init CuBLAS handle in stream";
 #endif
     const index_t nbatch = data.size(0);
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace =
+        ctx.requested[deconv::kTempSpace].get_space_typed<xpu, 1, DType>(
+            Shape1(this->InitTemp(out.shape_, data.shape_)), s, L"workspace:deconvolution-inl");
+#else
     Tensor<xpu, 1, DType> workspace =
         ctx.requested[deconv::kTempSpace].get_space_typed<xpu, 1, DType>(
             Shape1(this->InitTemp(out.shape_, data.shape_)), s);
+#endif
     for (index_t i = 0; i < nbatch; i += nstep_) {
       const index_t step = std::min(nstep_, nbatch - i);
       Tensor<xpu, 2, DType> temp_col = Tensor<xpu, 2, DType>(
@@ -346,9 +352,15 @@ class DeconvolutionOp : public Operator {
     param_.InferPad(dshape, o_pad, o_adj);
 
     const index_t nbatch = data.size(0);
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> workspace =
+        ctx.requested[deconv::kTempSpace].get_space_typed<xpu, 1, DType>(
+            Shape1(this->InitTemp(grad.shape_, data.shape_)), s, L"workspace:deconvolution-inl");
+#else
     Tensor<xpu, 1, DType> workspace =
         ctx.requested[deconv::kTempSpace].get_space_typed<xpu, 1, DType>(
             Shape1(this->InitTemp(grad.shape_, data.shape_)), s);
+#endif
     for (index_t i = 0; i < nbatch; i += nstep_) {
       const index_t step = std::min(nstep_, nbatch - i);
       Tensor<xpu, 2, DType> temp_col = Tensor<xpu, 2, DType>(
diff --git a/src/operator/grid_generator-inl.h b/src/operator/grid_generator-inl.h
index 0be6e78..c225529 100644
--- a/src/operator/grid_generator-inl.h
+++ b/src/operator/grid_generator-inl.h
@@ -115,8 +115,13 @@ class GridGeneratorOp : public Operator {
         Tensor<xpu, 4, DType> out = out_data[grid::kOut].get<xpu, 4, DType>(s);
         // grid_dst : (2, H, W)
         Tensor<xpu, 3, DType> grid_dst = out_data[grid::kGridDst].get<xpu, 3, DType>(s);
+#if MXNET_MEMORY_PROFILER_ON
+        Tensor<xpu, 2, DType> workspace = ctx.requested[grid::kTempSpace]
+          .get_space_typed<xpu, 2, DType>(Shape2(2, 1), s, L"workspace:grid_generator-inl");
+#else
         Tensor<xpu, 2, DType> workspace = ctx.requested[grid::kTempSpace]
           .get_space_typed<xpu, 2, DType>(Shape2(2, 1), s);
+#endif
         grid_dst[0] = repmat(range<DType>(0, data.size(3)), data.size(2));
         grid_dst[1] = reshape(range<DType>(0, data.size(2), 1, data.size(3)),
                               Shape2(data.size(2), data.size(3)));
@@ -162,8 +167,13 @@ class GridGeneratorOp : public Operator {
       case grid::kWarp: {
         Tensor<xpu, 4, DType> grad = out_grad[grid::kOut].get<xpu, 4, DType>(s);
         Tensor<xpu, 4, DType> gdata = in_grad[grid::kData].get<xpu, 4, DType>(s);
+#if MXNET_MEMORY_PROFILER_ON
+        Tensor<xpu, 2, DType> workspace = ctx.requested[grid::kTempSpace]
+          .get_space_typed<xpu, 2, DType>(Shape2(2, 1), s, L"workspace:grid_generator-inl");
+#else
         Tensor<xpu, 2, DType> workspace = ctx.requested[grid::kTempSpace]
           .get_space_typed<xpu, 2, DType>(Shape2(2, 1), s);
+#endif
         workspace[0] = scalar<DType>((DType(gdata.size(3)) - 1.0) / 2.0);
         workspace[1] = scalar<DType>((DType(gdata.size(2)) - 1.0) / 2.0);
         Assign(gdata, req[grid::kData],
diff --git a/src/operator/identity_attach_KL_sparse_reg-inl.h b/src/operator/identity_attach_KL_sparse_reg-inl.h
index 2307914..5c40966 100644
--- a/src/operator/identity_attach_KL_sparse_reg-inl.h
+++ b/src/operator/identity_attach_KL_sparse_reg-inl.h
@@ -100,8 +100,13 @@ class IdentityAttachKLSparseRegOp : public Operator {
     Tensor<xpu, 2> data_in = in_data[sparsereg::kData].FlatTo2D<xpu, real_t>(s);
     Tensor<xpu, 2> grad_out = out_grad[sparsereg::kOut].FlatTo2D<xpu, real_t>(s);
     Tensor<xpu, 1> moving_avg = aux_args[sparsereg::kMovingAvg].get<xpu, 1, real_t>(s);
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1> avg = ctx.requested[sparsereg::kTempSpace].get_space<xpu>(
+        mshadow::Shape1(moving_avg.shape_[0]), s, L"workspace:identity_attach_KL_sparse_reg-inl");
+#else
     Tensor<xpu, 1> avg = ctx.requested[sparsereg::kTempSpace].get_space<xpu>(
         mshadow::Shape1(moving_avg.shape_[0]), s);
+#endif
     avg = sumall_except_dim<1>(data_in);
     avg /= data_in.shape_[0];
     moving_avg = param_.momentum * moving_avg + (1 - param_.momentum) * avg;
diff --git a/src/operator/instance_norm-inl.h b/src/operator/instance_norm-inl.h
index 6e78f76..e020607 100644
--- a/src/operator/instance_norm-inl.h
+++ b/src/operator/instance_norm-inl.h
@@ -140,9 +140,15 @@ class InstanceNormOp : public Operator {
     Tensor<xpu, 1> mean =
         out_data[instance_norm::kMean].FlatTo1D<xpu, real_t>(s);
     // Get temp space
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 2> workspace =
+        ctx.requested[instance_norm::kTempSpace].get_space<xpu>(
+            mshadow::Shape2(3, mean.shape_[0]), s, L"workspace:instance_norm-inl");
+#else
     Tensor<xpu, 2> workspace =
         ctx.requested[instance_norm::kTempSpace].get_space<xpu>(
             mshadow::Shape2(3, mean.shape_[0]), s);
+#endif
     Tensor<xpu, 1> gmean = workspace[0];
     Tensor<xpu, 1> gvar = workspace[1];
     Tensor<xpu, 1> tmp = workspace[2];
diff --git a/src/operator/l2_normalization-inl.h b/src/operator/l2_normalization-inl.h
index c1f17ac..9fead04 100644
--- a/src/operator/l2_normalization-inl.h
+++ b/src/operator/l2_normalization-inl.h
@@ -154,8 +154,13 @@ class L2NormalizationOp : public Operator {
       Tensor<xpu, 2> grad_out = out_grad[l2_normalization::kOut]
         .get_with_shape<xpu, 2, real_t>(dshape, s);
       Tensor<xpu, 1> norm = out_data[l2_normalization::kNorm].get<xpu, 1, real_t>(s);
+#if MXNET_MEMORY_PROFILER_ON
+      Tensor<xpu, 1> temp = ctx.requested[l2_normalization::kTempSpace]
+        .get_space<xpu>(mshadow::Shape1(data.shape_[0]), s, L"workspace:l2_normalization-inl");
+#else
       Tensor<xpu, 1> temp = ctx.requested[l2_normalization::kTempSpace]
         .get_space<xpu>(mshadow::Shape1(data.shape_[0]), s);
+#endif
       temp = sumall_except_dim<0>(grad_out * data);
       Assign(grad_in, req[l2_normalization::kData],
         (grad_out - data * broadcast<0>(temp, data.shape_)) /
@@ -173,8 +178,13 @@ class L2NormalizationOp : public Operator {
       Shape<2> norm_shape = Shape2(dshape[0], dshape[2]);
       Tensor<xpu, 2> norm = out_data[l2_normalization::kNorm]
         .get_with_shape<xpu, 2, real_t>(norm_shape, s);
+#if MXNET_MEMORY_PROFILER_ON
+      Tensor<xpu, 2> temp = ctx.requested[l2_normalization::kTempSpace]
+        .get_space<xpu>(mshadow::Shape2(data.shape_[0], data.shape_[2]), s, L"workspace:l2_normalization-inl");
+#else
       Tensor<xpu, 2> temp = ctx.requested[l2_normalization::kTempSpace]
         .get_space<xpu>(mshadow::Shape2(data.shape_[0], data.shape_[2]), s);
+#endif
       temp = reduce_with_axis<red::sum, false>(grad_out * data, 1);
       Assign(grad_in, req[l2_normalization::kData],
         (grad_out - data * broadcast_with_axis(temp, 0, orig_shape[1])) /
@@ -192,8 +202,13 @@ class L2NormalizationOp : public Operator {
       Shape<2> norm_shape = Shape2(dshape[0], dshape[1]);
       Tensor<xpu, 2> norm = out_data[l2_normalization::kNorm]
         .get_with_shape<xpu, 2, real_t>(norm_shape, s);
+#if MXNET_MEMORY_PROFILER_ON
+      Tensor<xpu, 2> temp = ctx.requested[l2_normalization::kTempSpace]
+        .get_space<xpu>(mshadow::Shape2(data.shape_[0], data.shape_[1]), s, L"workspace:l2_normalization-inl");
+#else
       Tensor<xpu, 2> temp = ctx.requested[l2_normalization::kTempSpace]
         .get_space<xpu>(mshadow::Shape2(data.shape_[0], data.shape_[1]), s);
+#endif
       temp = reduce_with_axis<red::sum, false>(grad_out * data, 2);
       Assign(grad_in, req[l2_normalization::kData],
         (grad_out - data * broadcast_with_axis(temp, 1, dshape[2])) /
diff --git a/src/operator/linalg_impl.h b/src/operator/linalg_impl.h
index b1b35cf..3b9f692 100644
--- a/src/operator/linalg_impl.h
+++ b/src/operator/linalg_impl.h
@@ -195,7 +195,42 @@ void linalg_gemm<gpu, mshadow::half::half_t>(const Tensor<gpu, 2, mshadow::half:
 #endif  // CUDA_VERSION >= 7050
 }
 
-
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_BATCH_GEMM(fname, DType) \
+template<> inline \
+void linalg_batch_gemm<gpu, DType>(const Tensor<gpu, 3, DType>& A, const Tensor<gpu, 3, DType>& B, \
+                                   const Tensor<gpu, 3, DType>& C, DType alpha, DType beta, \
+                                   bool tA, bool tB, Stream<gpu> *s) { \
+  using namespace mxnet; \
+  using mshadow::gpu; \
+  CHECK_NOTNULL(s); \
+  linalg_check_batch_size(A.size(0), B.size(0), C.size(0)); \
+  check_gemm(A[0], B[0], C[0], alpha, beta, tA, tB); \
+  Storage::Handle offsetsA, offsetsB, offsetsC; \
+  offsetsA = Storage::Get()->Alloc(sizeof(DType*)*A.size(0), Context::GPU(), L"workspace,linalg_impl"); \
+  offsetsB = Storage::Get()->Alloc(sizeof(DType*)*B.size(0), Context::GPU(), L"workspace,linalg_impl"); \
+  offsetsC = Storage::Get()->Alloc(sizeof(DType*)*C.size(0), Context::GPU(), L"workspace,linalg_impl"); \
+  using namespace mshadow::cuda; \
+  int ngrid = std::min(kMaxGridNum, \
+                       static_cast<int>((A.size(0) + kBaseThreadNum - 1) / kBaseThreadNum)); \
+  linalgCollectBatchOffsetsGPU<<<ngrid, kBaseThreadNum, 0, mshadow::Stream<gpu>::GetStream(s)>>> \
+    (static_cast<DType **>(offsetsA.dptr), A.dptr_, A.size(1)*A.stride_, A.size(0)); \
+  linalgCollectBatchOffsetsGPU<<<ngrid, kBaseThreadNum, 0, mshadow::Stream<gpu>::GetStream(s)>>> \
+    (static_cast<DType **>(offsetsB.dptr), B.dptr_, B.size(1)*B.stride_, B.size(0)); \
+  linalgCollectBatchOffsetsGPU<<<ngrid, kBaseThreadNum, 0, mshadow::Stream<gpu>::GetStream(s)>>> \
+    (static_cast<DType **>(offsetsC.dptr), C.dptr_, C.size(1)*C.stride_, C.size(0)); \
+  CUBLAS_CALL(cublas##fname(Stream<gpu>::GetBlasHandle(s), \
+                            (tB ? CUBLAS_OP_T : CUBLAS_OP_N), \
+                            (tA ? CUBLAS_OP_T : CUBLAS_OP_N), \
+                            C.size(2), C.size(1), (tB ? B.size(2) : B.size(1)), \
+                            &alpha, static_cast<const DType **>(offsetsB.dptr), B.stride_, \
+                            static_cast<const DType **>(offsetsA.dptr),  A.stride_, \
+                            &beta, static_cast<DType **>(offsetsC.dptr), C.stride_, A.size(0))) \
+  Storage::Get()->Free(offsetsA); \
+  Storage::Get()->Free(offsetsB); \
+  Storage::Get()->Free(offsetsC); \
+}
+#else
 #define LINALG_GPU_BATCH_GEMM(fname, DType) \
 template<> inline \
 void linalg_batch_gemm<gpu, DType>(const Tensor<gpu, 3, DType>& A, const Tensor<gpu, 3, DType>& B, \
@@ -230,6 +265,7 @@ void linalg_batch_gemm<gpu, DType>(const Tensor<gpu, 3, DType>& A, const Tensor<
   Storage::Get()->Free(offsetsB); \
   Storage::Get()->Free(offsetsC); \
 }
+#endif
 LINALG_GPU_BATCH_GEMM(SgemmBatched, float)
 LINALG_GPU_BATCH_GEMM(DgemmBatched, double)
 
@@ -321,7 +357,37 @@ void linalg_trsm<gpu, DType>(const Tensor<gpu, 2, DType>& A, const Tensor<gpu, 2
 }
 LINALG_GPU_TRSM(Strsm, float)
 LINALG_GPU_TRSM(Dtrsm, double)
-
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_BATCH_TRSM(fname, DType) \
+template<> inline \
+void linalg_batch_trsm<gpu, DType>(const Tensor<gpu, 3, DType>& A, const Tensor<gpu, 3, DType>& B, \
+                   DType alpha, bool rightside, bool lower, bool transpose, Stream<gpu> *s) { \
+  using namespace mxnet; \
+  using mshadow::gpu; \
+  CHECK_NOTNULL(s); \
+  linalg_check_batch_size(A.size(0), B.size(0), B.size(0)); \
+  check_trsm(A[0], B[0], alpha, rightside, lower, transpose); \
+  Storage::Handle offsetsA, offsetsB; \
+  offsetsA = Storage::Get()->Alloc(sizeof(DType*)*A.size(0), Context::GPU(), L"workspace,linalg_impl"); \
+  offsetsB = Storage::Get()->Alloc(sizeof(DType*)*B.size(0), Context::GPU(), L"workspace,linalg_impl"); \
+  using namespace mshadow::cuda; \
+  int ngrid = std::min(kMaxGridNum, \
+                       static_cast<int>((A.size(0) + kBaseThreadNum - 1) / kBaseThreadNum)); \
+  linalgCollectBatchOffsetsGPU<<<ngrid, kBaseThreadNum, 0, mshadow::Stream<gpu>::GetStream(s)>>> \
+    (static_cast<DType **>(offsetsA.dptr), A.dptr_, A.size(1)*A.stride_, A.size(0)); \
+  linalgCollectBatchOffsetsGPU<<<ngrid, kBaseThreadNum, 0, mshadow::Stream<gpu>::GetStream(s)>>> \
+    (static_cast<DType **>(offsetsB.dptr), B.dptr_, B.size(1)*B.stride_, A.size(0)); \
+  CUBLAS_CALL(cublas##fname(Stream<gpu>::GetBlasHandle(s), \
+                            (rightside ? CUBLAS_SIDE_LEFT : CUBLAS_SIDE_RIGHT), \
+                            (lower ? CUBLAS_FILL_MODE_UPPER : CUBLAS_FILL_MODE_LOWER), \
+                            (transpose ? CUBLAS_OP_T : CUBLAS_OP_N), \
+                            CUBLAS_DIAG_NON_UNIT, B.size(2), B.size(1), &alpha, \
+                            static_cast<const DType **>(offsetsA.dptr), A.stride_, \
+                            static_cast<DType **>(offsetsB.dptr), B.stride_, A.size(0))); \
+  Storage::Get()->Free(offsetsA); \
+  Storage::Get()->Free(offsetsB); \
+}
+#else
 #define LINALG_GPU_BATCH_TRSM(fname, DType) \
 template<> inline \
 void linalg_batch_trsm<gpu, DType>(const Tensor<gpu, 3, DType>& A, const Tensor<gpu, 3, DType>& B, \
@@ -351,6 +417,7 @@ void linalg_batch_trsm<gpu, DType>(const Tensor<gpu, 3, DType>& A, const Tensor<
   Storage::Get()->Free(offsetsA); \
   Storage::Get()->Free(offsetsB); \
 }
+#endif
 LINALG_GPU_BATCH_TRSM(StrsmBatched, float)
 LINALG_GPU_BATCH_TRSM(DtrsmBatched, double)
 
@@ -581,6 +648,25 @@ inline int linalg_potrf_buffsize(const Tensor<gpu, 2, DType>& A, bool lower, Str
 LINALG_GPU_BUFFSIZE_POTRF(DnSpotrf_bufferSize, float)
 LINALG_GPU_BUFFSIZE_POTRF(DnDpotrf_bufferSize, double)
 
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_POTRF(fname, DType) \
+template<> inline \
+void linalg_potrf<gpu, DType>(const Tensor<gpu, 2, DType>& A, bool lower, Stream<gpu> *s) { \
+  using namespace mxnet; \
+  using mshadow::gpu; \
+  CHECK_NOTNULL(s); \
+  check_potrf(A, lower); \
+  int buffsize(linalg_potrf_buffsize(A, lower, s)); \
+  Storage::Handle buffer = Storage::Get()->Alloc(sizeof(DType)*buffsize, Context::GPU(), L"workspace,linalg_impl"); \
+  Storage::Handle info = Storage::Get()->Alloc(sizeof(int), Context::GPU(), L"workspace,linalg_impl"); \
+  CUSOLVER_CALL(cusolver##fname(Stream<gpu>::GetSolverHandle(s), \
+                (lower ? CUBLAS_FILL_MODE_UPPER : CUBLAS_FILL_MODE_LOWER), \
+                A.size(0), A.dptr_, A.stride_, static_cast<DType *>(buffer.dptr), buffsize, \
+                static_cast<int *>(info.dptr))); \
+  Storage::Get()->Free(buffer); \
+  Storage::Get()->Free(info); \
+}
+#else
 #define LINALG_GPU_POTRF(fname, DType) \
 template<> inline \
 void linalg_potrf<gpu, DType>(const Tensor<gpu, 2, DType>& A, bool lower, Stream<gpu> *s) { \
@@ -598,9 +684,32 @@ void linalg_potrf<gpu, DType>(const Tensor<gpu, 2, DType>& A, bool lower, Stream
   Storage::Get()->Free(buffer); \
   Storage::Get()->Free(info); \
 }
+#endif
 LINALG_GPU_POTRF(DnSpotrf, float)
 LINALG_GPU_POTRF(DnDpotrf, double)
 
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_BATCH_POTRF(fname, DType) \
+template<> inline \
+void linalg_batch_potrf<gpu, DType>(const Tensor<gpu, 3, DType>& A, bool lower, Stream<gpu> *s) { \
+  using namespace mxnet; \
+  using mshadow::gpu; \
+  CHECK_NOTNULL(s); \
+  CHECK_GT(A.size(0), 0); \
+  check_potrf(A[0], lower); \
+  int buffsize(linalg_potrf_buffsize(A[0], lower, s)); \
+  Storage::Handle buffer = Storage::Get()->Alloc(sizeof(DType)*buffsize, Context::GPU(), L"workspace:linalg_impl"); \
+  Storage::Handle info = Storage::Get()->Alloc(sizeof(int), Context::GPU(), L"workspace:linalg_impl"); \
+  for (mshadow::index_t i = 0; i < A.size(0); ++i) { \
+    CUSOLVER_CALL(cusolver##fname(Stream<gpu>::GetSolverHandle(s), \
+                 (lower ? CUBLAS_FILL_MODE_UPPER : CUBLAS_FILL_MODE_LOWER), \
+                 A[i].size(0), A[i].dptr_, A[i].stride_, \
+                 static_cast<DType *>(buffer.dptr), buffsize, static_cast<int *>(info.dptr))); \
+  } \
+  Storage::Get()->Free(buffer); \
+  Storage::Get()->Free(info); \
+}
+#else
 #define LINALG_GPU_BATCH_POTRF(fname, DType) \
 template<> inline \
 void linalg_batch_potrf<gpu, DType>(const Tensor<gpu, 3, DType>& A, bool lower, Stream<gpu> *s) { \
@@ -621,6 +730,7 @@ void linalg_batch_potrf<gpu, DType>(const Tensor<gpu, 3, DType>& A, bool lower,
   Storage::Get()->Free(buffer); \
   Storage::Get()->Free(info); \
 }
+#endif
 LINALG_GPU_BATCH_POTRF(DnSpotrf, float)
 LINALG_GPU_BATCH_POTRF(DnDpotrf, double)
 
@@ -672,6 +782,27 @@ __global__ void linalgInitIdentityGPU(DType *a, int stride, int lda, int N) {
 }
 
 // There is no direct support for potri in cuda. We emulate the function by two calls to trsm.
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_POTRI(DType) \
+template<> inline \
+void linalg_potri<gpu, DType>(const Tensor<gpu, 2, DType>& A, bool lower, Stream<gpu> *s) { \
+  using namespace mxnet; \
+  CHECK_NOTNULL(s); \
+  check_potri(A, lower); \
+  Storage::Handle buffer = Storage::Get()->Alloc(sizeof(DType)*A.MSize(), Context::GPU(), L"workspace,linalg_impl"); \
+  using namespace mshadow::cuda; \
+  int ngrid = std::min(kMaxGridNum, \
+                       static_cast<int>((A.MSize() + kBaseThreadNum - 1) / kBaseThreadNum)); \
+  linalgInitIdentityGPU<<<ngrid, kBaseThreadNum, 0, mshadow::Stream<gpu>::GetStream(s)>>> \
+    (static_cast<DType *>(buffer.dptr), A.MSize(), A.stride_, A.MSize());  \
+  Tensor<gpu, 2, DType> B((DType *)buffer.dptr, A.shape_, A.stride_, s); \
+  linalg_trsm(A, B, DType(1.0), false, lower, !lower, s); \
+  linalg_trsm(A, B, DType(1.0), false, lower, lower, s); \
+  Copy(A, B, s); \
+  B.dptr_ = 0; \
+  Storage::Get()->Free(buffer); \
+}
+#else
 #define LINALG_GPU_POTRI(DType) \
 template<> inline \
 void linalg_potri<gpu, DType>(const Tensor<gpu, 2, DType>& A, bool lower, Stream<gpu> *s) { \
@@ -691,9 +822,32 @@ void linalg_potri<gpu, DType>(const Tensor<gpu, 2, DType>& A, bool lower, Stream
   B.dptr_ = 0; \
   Storage::Get()->Free(buffer); \
 }
+#endif
 LINALG_GPU_POTRI(float)
 LINALG_GPU_POTRI(double)
 
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_BATCH_POTRI(DType) \
+template<> inline \
+void linalg_batch_potri<gpu, DType>(const Tensor<gpu, 3, DType>& A, bool lower, Stream<gpu> *s) { \
+  using namespace mxnet; \
+  CHECK_NOTNULL(s); \
+  CHECK_GT(A.size(0), 0); \
+  check_potri(A[0], lower); \
+  Storage::Handle buffer = Storage::Get()->Alloc(sizeof(DType)*A.MSize(), Context::GPU(), L"workspace,linalg_impl"); \
+  using namespace mshadow::cuda; \
+  int ngrid = std::min(kMaxGridNum, \
+                       static_cast<int>((A.MSize() + kBaseThreadNum - 1) / kBaseThreadNum)); \
+  linalgInitIdentityGPU<<<ngrid, kBaseThreadNum, 0, mshadow::Stream<gpu>::GetStream(s)>>> \
+    (static_cast<DType *>(buffer.dptr), A.size(1)*A.stride_, A.stride_, A.MSize()); \
+  Tensor<gpu, 3, DType> B((DType *)buffer.dptr, A.shape_, A.stride_, s); \
+  linalg_batch_trsm(A, B, DType(1.0), false, lower, !lower, s); \
+  linalg_batch_trsm(A, B, DType(1.0), false, lower, lower, s); \
+  Copy(A, B, s); \
+  B.dptr_ = 0; \
+  Storage::Get()->Free(buffer); \
+}
+#else
 #define LINALG_GPU_BATCH_POTRI(DType) \
 template<> inline \
 void linalg_batch_potri<gpu, DType>(const Tensor<gpu, 3, DType>& A, bool lower, Stream<gpu> *s) { \
@@ -714,6 +868,7 @@ void linalg_batch_potri<gpu, DType>(const Tensor<gpu, 3, DType>& A, bool lower,
   B.dptr_ = 0; \
   Storage::Get()->Free(buffer); \
 }
+#endif
 LINALG_GPU_BATCH_POTRI(float)
 LINALG_GPU_BATCH_POTRI(double)
 
@@ -870,7 +1025,26 @@ LINALG_CPU_GELQF_WORKSPACE_QUERY(s, float)
 LINALG_CPU_GELQF_WORKSPACE_QUERY(d, double)
 
 #ifdef __CUDACC__
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_GELQF(fname, DType) \
+template<> inline \
+void linalg_gelqf<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
+                              const Tensor<gpu, 1, DType>& work, \
+                              Stream<gpu> *s) { \
+  using namespace mxnet; \
+  using mshadow::gpu; \
+  CHECK_NOTNULL(s); \
+  check_gelqf(A, work); \
+  int m(A.size(0)); \
+  int lwork(work.size(0) - m); \
+  Storage::Handle info = Storage::Get()->Alloc(sizeof(int), Context::GPU(), L"workspace:linalg_impl"); \
+  CUSOLVER_CALL(cusolver##fname(Stream<gpu>::GetSolverHandle(s), \
+                A.size(1), m, A.dptr_ , A.stride_, work.dptr_, \
+                work.dptr_ + m, lwork, static_cast<int *>(info.dptr))); \
+  Storage::Get()->Free(info); \
+}
 
+#else
 #define LINALG_GPU_GELQF(fname, DType) \
 template<> inline \
 void linalg_gelqf<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
@@ -888,6 +1062,9 @@ void linalg_gelqf<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
                 work.dptr_ + m, lwork, static_cast<int *>(info.dptr))); \
   Storage::Get()->Free(info); \
 }
+
+#endif // MXNET_MEMORY_PROFILER
+
 // Col-major QR-decomposition results in row-major LQ decomposition.
 LINALG_GPU_GELQF(DnSgeqrf, float)
 LINALG_GPU_GELQF(DnDgeqrf, double)
@@ -895,6 +1072,25 @@ LINALG_GPU_GELQF(DnDgeqrf, double)
 // ORGLQ only available with cuda8 or higher.
 #if CUDA_VERSION >= 8000
 
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_ORGLQ(fname, DType) \
+template<> inline \
+void linalg_orglq<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
+                              const Tensor<gpu, 1, DType>& work, \
+                              Stream<gpu> *s) { \
+  using namespace mxnet; \
+  using mshadow::gpu; \
+  CHECK_NOTNULL(s); \
+  check_gelqf(A, work); \
+  int m(A.size(0)); \
+  int lwork(work.size(0) - m); \
+  Storage::Handle info = Storage::Get()->Alloc(sizeof(int), Context::GPU(), L"workspace:linalg_impl"); \
+  CUSOLVER_CALL(cusolver##fname(Stream<gpu>::GetSolverHandle(s), \
+                A.size(1), m, m, A.dptr_ , A.stride_, work.dptr_, \
+                work.dptr_ + m, lwork, static_cast<int *>(info.dptr))); \
+  Storage::Get()->Free(info); \
+}
+#else
 #define LINALG_GPU_ORGLQ(fname, DType) \
 template<> inline \
 void linalg_orglq<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
@@ -912,6 +1108,7 @@ void linalg_orglq<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
                 work.dptr_ + m, lwork, static_cast<int *>(info.dptr))); \
   Storage::Get()->Free(info); \
 }
+#endif // MXNET_MEMORY_PROFILER
 
 #else
 
@@ -930,7 +1127,25 @@ LINALG_GPU_ORGLQ(DnDorgqr, double)
 
 // ORGLQ only available with cuda8 or higher.
 #if CUDA_VERSION >= 8000
-
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_GELQF_WORKSPACE_QUERY(prefix, DType) \
+template<> inline \
+int linalg_gelqf_workspace_query<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
+                                             Stream<gpu> *s) { \
+  using namespace mxnet; \
+  using mshadow::gpu; \
+  int m(A.size(0)); \
+  int work1(0); \
+  CUSOLVER_CALL(cusolverDn##prefix##geqrf_bufferSize(Stream<gpu>::GetSolverHandle(s), \
+                A.size(1), m, A.dptr_ , A.stride_, &work1)); \
+  int work2(0);  \
+  Storage::Handle tau = Storage::Get()->Alloc(sizeof(DType), Context::GPU(), L"workspace:linalg_impl"); \
+  CUSOLVER_CALL(cusolverDn##prefix##orgqr_bufferSize(Stream<gpu>::GetSolverHandle(s), \
+                A.size(1), m, m, A.dptr_ , A.stride_, static_cast<DType *>(tau.dptr), &work2)); \
+  Storage::Get()->Free(tau); \
+  return std::max(work1, work2) + m; \
+}
+#else
 #define LINALG_GPU_GELQF_WORKSPACE_QUERY(prefix, DType) \
 template<> inline \
 int linalg_gelqf_workspace_query<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
@@ -948,6 +1163,8 @@ int linalg_gelqf_workspace_query<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
   Storage::Get()->Free(tau); \
   return std::max(work1, work2) + m; \
 }
+#endif // MXNET_MEMORY_PROFILER
+
 
 #else
 
@@ -1030,6 +1247,26 @@ LINALG_CPU_SYEVD_WORKSPACE_QUERY(dsyevd, double)
 
 // Row-major vs. col-major handled by using upper triangular
 // in cusolver-call.
+#if MXNET_MEMORY_PROFILER_ON
+#define LINALG_GPU_SYEVD(fname, DType) \
+template<> inline \
+void linalg_syevd<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
+                              const Tensor<gpu, 1, DType>& L, \
+                              const Tensor<gpu, 1, DType>& work, \
+                              Stream<gpu> *s) { \
+  using namespace mxnet; \
+  using mshadow::gpu; \
+  CHECK_NOTNULL(s); \
+  check_syevd(A, L); \
+  Storage::Handle info = Storage::Get()->Alloc(sizeof(int), Context::GPU(), L"workspace:linalg_impl"); \
+  CUSOLVER_CALL(cusolver##fname(Stream<gpu>::GetSolverHandle(s), \
+                CUSOLVER_EIG_MODE_VECTOR, CUBLAS_FILL_MODE_UPPER, \
+                A.size(0), A.dptr_ , A.stride_, L.dptr_, work.dptr_, \
+                work.size(0), static_cast<int *>(info.dptr))); \
+  Storage::Get()->Free(info); \
+}
+
+#else
 #define LINALG_GPU_SYEVD(fname, DType) \
 template<> inline \
 void linalg_syevd<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
@@ -1048,6 +1285,8 @@ void linalg_syevd<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
   Storage::Get()->Free(info); \
 }
 
+#endif //MXNET_MEMORY_PROFILER
+
 #define LINALG_GPU_SYEVD_WORKSPACE_QUERY(fname, DType) \
 template<> inline \
 int linalg_syevd_workspace_query<gpu, DType>(const Tensor<gpu, 2, DType>& A, \
diff --git a/src/operator/loss_binary_op-inl.h b/src/operator/loss_binary_op-inl.h
index 8add827..cd3cad2 100644
--- a/src/operator/loss_binary_op-inl.h
+++ b/src/operator/loss_binary_op-inl.h
@@ -62,8 +62,13 @@ void SoftmaxCrossEntropyForward(const nnvm::NodeAttrs& attrs,
     mshadow::Tensor<xpu, 1, DType> out = outputs[0].get<xpu, 1, DType>(s);
     mshadow::Tensor<xpu, 1, DType> mlabel = inputs[1].get<xpu, 1, DType>(s);
     mshadow::Tensor<xpu, 2, DType> mdata = inputs[0].get<xpu, 2, DType>(s);
+#if MXNET_MEMORY_PROFILER_ON
+    mshadow::Tensor<xpu, 1, DType> workspace = ctx.requested[0].get_space_typed<xpu, 1, DType>(
+        mshadow::Shape1(mdata.shape_.Size() + mlabel.size(0)), s, L"workspace:loss_binary_op-inl");
+#else
     mshadow::Tensor<xpu, 1, DType> workspace = ctx.requested[0].get_space_typed<xpu, 1, DType>(
         mshadow::Shape1(mdata.shape_.Size() + mlabel.size(0)), s);
+#endif
     mshadow::Tensor<xpu, 2, DType> temp1(workspace.dptr_, mdata.shape_, s);
     mshadow::Tensor<xpu, 2, DType> temp2(workspace.dptr_ + mdata.shape_.Size(),
         mshadow::Shape2(1, mlabel.size(0)), s);
@@ -95,8 +100,13 @@ void SoftmaxCrossEntropyBackward(const nnvm::NodeAttrs& attrs,
     mshadow::Tensor<xpu, 2, DType> mdata = inputs[1].get<xpu, 2, DType>(s);
     mshadow::Tensor<xpu, 2, DType> mdata_grad = outputs[0].get<xpu, 2, DType>(s);
     mshadow::Tensor<xpu, 1, DType> mscale = inputs[0].get<xpu, 1, DType>(s);
+#if MXNET_MEMORY_PROFILER_ON
+    mshadow::Tensor<xpu, 2, DType> temp = ctx.requested[0].get_space_typed<xpu, 2, DType>(
+        mdata.shape_, s, L"workspace:loss_binary_op-inl");
+#else
     mshadow::Tensor<xpu, 2, DType> temp = ctx.requested[0].get_space_typed<xpu, 2, DType>(
         mdata.shape_, s);
+#endif
     mshadow::Softmax(temp, mdata);
     mshadow::SoftmaxGrad(temp, temp, mlabel);
     ASSIGN_DISPATCH(mdata_grad, req[0], broadcast_scalar(mscale, temp.shape_) * temp);
diff --git a/src/operator/make_loss-inl.h b/src/operator/make_loss-inl.h
index 3f4a993..732589a 100644
--- a/src/operator/make_loss-inl.h
+++ b/src/operator/make_loss-inl.h
@@ -101,8 +101,13 @@ class MakeLossOp : public Operator {
     Tensor<xpu, 2, DType> grad = in_grad[make_loss_enum::kData].FlatTo2D<xpu, DType>(s);
     if (param_.normalization == make_loss_enum::kValid) {
       Tensor<xpu, 2, DType> data = in_data[make_loss_enum::kData].FlatTo2D<xpu, DType>(s);
+#if MXNET_MEMORY_PROFILER_ON
+      Tensor<xpu, 1, DType> temp = ctx.requested[make_loss_enum::kTempSpace]
+        .get_space_typed<xpu, 1, DType>(mshadow::Shape1(1), s, L"workspace:make_loss-inl");
+#else
       Tensor<xpu, 1, DType> temp = ctx.requested[make_loss_enum::kTempSpace]
         .get_space_typed<xpu, 1, DType>(mshadow::Shape1(1), s);
+#endif
       temp = sumall_except_dim<0>(reduce_keepdim<red::sum, false>(
         F<mshadow_op::threshold>(ScalarExp<DType>(param_.valid_thresh), data), 0));
       temp = F<mshadow_op::maximum>(ScalarExp<DType>(1.f), temp);  // avoid zero
diff --git a/src/operator/mkl/mkl_batch_norm-inl.h b/src/operator/mkl/mkl_batch_norm-inl.h
index b5967f4..0d1f328 100644
--- a/src/operator/mkl/mkl_batch_norm-inl.h
+++ b/src/operator/mkl/mkl_batch_norm-inl.h
@@ -102,8 +102,13 @@ class MKLBatchNormOp : public Operator {
     batchNormFwdTraining = NULL;
     batchNormBwdScaleShift = NULL;
     int scaleShift_size = channels_*2*sizeof(DType);
+#if MXNET_MEMORY_PROFILER_ON
+    scaleShift_space = Storage::Get()->Alloc(scaleShift_size, Context::CPU(), L"cpu_workspace:mkl_batch_norm-inl");
+    scaleShiftDiff_space = Storage::Get()->Alloc(scaleShift_size, Context::CPU(), L"cpu_workspace:mkl_batch_norm-inl");
+#else
     scaleShift_space = Storage::Get()->Alloc(scaleShift_size, Context::CPU());
     scaleShiftDiff_space = Storage::Get()->Alloc(scaleShift_size, Context::CPU());
+#endif
     DType * scaleShift_buf = reinterpret_cast<DType*>(scaleShift_space.dptr);
     /*!use_weight_bias_*/
     for (int i = 0; i < channels_; i++) {
diff --git a/src/operator/mkl/mkl_convolution-inl.h b/src/operator/mkl/mkl_convolution-inl.h
index 870e568..702ce64 100644
--- a/src/operator/mkl/mkl_convolution-inl.h
+++ b/src/operator/mkl/mkl_convolution-inl.h
@@ -318,7 +318,11 @@ class MKLConvolutionOp : public Operator {
   }
   void AddToModeAllocAndStoreBuffer(void *src, int blob_size, Storage::Handle *pws) {
     int blob_byte_size = blob_size * sizeof(DType);
+    #if MXNET_MEMORY_PROFILER_ON
+    *pws = Storage::Get()->Alloc(blob_byte_size, Context::CPU(), L"cpu_workspace:mkl_convolution-inl");
+    #else
     *pws = Storage::Get()->Alloc(blob_byte_size, Context::CPU());
+    #endif
     memcpy(pws->dptr, src, blob_byte_size);
   }
   void AddToModeAddAndReleaseBuffer(Storage::Handle *pws, void *dst_, int blob_size) {
diff --git a/src/operator/random/multisample_op.h b/src/operator/random/multisample_op.h
index f0851da..4474974 100644
--- a/src/operator/random/multisample_op.h
+++ b/src/operator/random/multisample_op.h
@@ -178,8 +178,15 @@ void MultiSampleOpForward(const nnvm::NodeAttrs& attrs,
   mshadow::Stream<xpu> *s = ctx.get_stream<xpu>();
   // Generate multiple seeds for the different threads.
   const int nSeeds(OptSampleSeedNum<xpu>(outputs[0].Size()));
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK should I tag the space with 'seed' or some other tag?*/
+  Tensor<xpu, 1, unsigned> seeds
+    = ctx.requested[1].get_space_typed<xpu, 1, unsigned> (Shape1(nSeeds), ctx.get_stream<xpu>(),
+														  L"seed:multisample_op");
+#else
   Tensor<xpu, 1, unsigned> seeds
     = ctx.requested[1].get_space_typed<xpu, 1, unsigned> (Shape1(nSeeds), ctx.get_stream<xpu>());
+#endif
   ctx.requested[0].get_random<xpu, float>(s)->GetRandInt(seeds);
   MSHADOW_TYPE_SWITCH(inputs[0].type_flag_, IType, {
     MSHADOW_REAL_TYPE_SWITCH(outputs[0].type_flag_, OType, {
diff --git a/src/operator/random/sample_multinomial_op.h b/src/operator/random/sample_multinomial_op.h
index 2b01632..558753f 100644
--- a/src/operator/random/sample_multinomial_op.h
+++ b/src/operator/random/sample_multinomial_op.h
@@ -151,8 +151,13 @@ void SampleMultinomialForward(const nnvm::NodeAttrs& attrs,
   Stream<xpu> *s = ctx.get_stream<xpu>();
   MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, {
     Random<xpu, float> *prnd = ctx.requested[0].get_random<xpu, float>(s);
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, float> uniform =
+      ctx.requested[1].get_space_typed<xpu, 1, float>(Shape1(N*M), s, L"weight?:sample_multinomial_op");
+#else
     Tensor<xpu, 1, float> uniform =
       ctx.requested[1].get_space_typed<xpu, 1, float>(Shape1(N*M), s);
+#endif
     prnd->SampleUniform(&uniform, 0, 1);
     Kernel<SampleMultinomialKernel, xpu>::Launch(
       s, N, K, M, inputs[0].dptr<DType>(), uniform.dptr_, outputs[0].dptr<int>(),
diff --git a/src/operator/random/sample_op.h b/src/operator/random/sample_op.h
index d9c3868..cbffe88 100644
--- a/src/operator/random/sample_op.h
+++ b/src/operator/random/sample_op.h
@@ -257,7 +257,11 @@ struct Scalar2Array {
   Storage::Handle array;
   Scalar2Array(DType scalar, const OpContext& ctx) {
     Stream<xpu> *s = ctx.get_stream<xpu>();
+#if MXNET_MEMORY_PROFILER_ON
+    array = Storage::Get()->Alloc(sizeof(DType), AllocContext<xpu>(), L"workspace:sample_op");
+#else
     array = Storage::Get()->Alloc(sizeof(DType), AllocContext<xpu>());
+#endif
     Tensor<xpu, 1, DType> src(Ref(), Shape1(1), s);
     Copy(src, Tensor<cpu, 1, DType>(&scalar, Shape1(1)), s);
   }
@@ -273,8 +277,15 @@ template<typename xpu>
 MSHADOW_FORCE_INLINE Tensor<xpu, 1, unsigned int> GetSeeds(index_t N, const OpContext& ctx) {
   Stream<xpu> *s = ctx.get_stream<xpu>();
   const index_t nSeeds(OptSampleSeedNum<xpu>(N));
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK: should I tag the space with 'seed' or some other tag*/
+  Tensor<xpu, 1, unsigned int> seeds
+    = ctx.requested[1].get_space_typed<xpu, 1, unsigned int>(Shape1(nSeeds), ctx.get_stream<xpu>(),
+                               L"workspace:sample_op");
+#else
   Tensor<xpu, 1, unsigned int> seeds
     = ctx.requested[1].get_space_typed<xpu, 1, unsigned int>(Shape1(nSeeds), ctx.get_stream<xpu>());
+#endif
   ctx.requested[0].get_random<xpu, float>(s)->GetRandInt(seeds);
   return seeds;
 }
@@ -432,7 +443,12 @@ void SampleComputeEx_(const nnvm::NodeAttrs& attrs,
   if (output.storage_type() == kRowSparseStorage) {
     // indices
     nnvm::dim_t nnr = output.shape()[0];
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+    output.CheckAndAlloc({mshadow::Shape1(nnr)}, L"sample_compute_ex:sample_op");
+#else
     output.CheckAndAlloc({mshadow::Shape1(nnr)});
+#endif
     PopulateFullIdxRspImpl(s, &output);
     // data
     TBlob out_blob = output.data();
diff --git a/src/operator/softmax_activation-inl.h b/src/operator/softmax_activation-inl.h
index b1b7693..42d0bb3 100644
--- a/src/operator/softmax_activation-inl.h
+++ b/src/operator/softmax_activation-inl.h
@@ -126,8 +126,13 @@ class SoftmaxActivationOp : public Operator {
     Tensor<xpu, 3> m_in_grad =
       in_grad[softmax_activation::kData].get_with_shape<xpu, 3, real_t>(data_shape, s);
     // get requested temp space
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 2> workspace = ctx.requested[softmax_activation::kTempSpace].get_space<xpu>(
+        Shape2(batch_size, rest_size), s, L"workspace:softmax_activation-inl");
+#else
     Tensor<xpu, 2> workspace = ctx.requested[softmax_activation::kTempSpace].get_space<xpu>(
         Shape2(batch_size, rest_size), s);
+#endif
     workspace = reduce_with_axis<red::sum, false>(m_out_grad * m_out_data, 1);
     Assign(m_in_grad, req[softmax_activation::kData],
         m_out_data * (m_out_grad - broadcast_with_axis(workspace, 0, channel_num)));
diff --git a/src/operator/tensor/broadcast_reduce_op.h b/src/operator/tensor/broadcast_reduce_op.h
index 75f96f9..ffb83a6 100644
--- a/src/operator/tensor/broadcast_reduce_op.h
+++ b/src/operator/tensor/broadcast_reduce_op.h
@@ -425,8 +425,13 @@ void ReduceAxesComputeImpl(const nnvm::NodeAttrs& attrs,
     BROADCAST_NDIM_SWITCH(dst_shape.ndim(), NDim, {
       size_t workspace_size = broadcast::ReduceWorkspaceSize<NDim, DType>(
           s, out_data, req[0], in_data);
+#if MXNET_MEMORY_PROFILER_ON
+      Tensor<xpu, 1, char> workspace =
+          ctx.requested[0].get_space_typed<xpu, 1, char>(Shape1(workspace_size), s, L"workspace:broadcast_reduce_op");
+#else
       Tensor<xpu, 1, char> workspace =
           ctx.requested[0].get_space_typed<xpu, 1, char>(Shape1(workspace_size), s);
+#endif
       broadcast::Reduce<reducer, NDim, DType, mshadow::op::identity>(
           s, out_data, req[0], workspace, in_data);
       if (normalize) {
@@ -613,9 +618,15 @@ void SumCsrImpl(const nnvm::NodeAttrs& attrs, mshadow::Stream<xpu>* s, const OpC
             const IType num_cols = input.shape()[1];
             dim_t num_threads = mxnet_op::get_num_threads<xpu>(16);
             dim_t seg_len = (out_data_size + num_threads - 1) / num_threads;
+#if MXNET_MEMORY_PROFILER_ON
+            mshadow::Tensor<xpu, 1, DType> workspace =
+                ctx.requested[0].get_space_typed<xpu, 1, DType>(
+                    Shape1(2 * out_data_size), s, L"workspace:broadcast_reduce_op");
+#else
             mshadow::Tensor<xpu, 1, DType> workspace =
                 ctx.requested[0].get_space_typed<xpu, 1, DType>(
                     Shape1(2 * out_data_size), s);
+#endif
             mshadow::Tensor<xpu, 1, DType> sum(
                 reinterpret_cast<DType*>(workspace.dptr_),
                 Shape1(out_data_size));
diff --git a/src/operator/tensor/cast_storage-inl.cuh b/src/operator/tensor/cast_storage-inl.cuh
index fb75438..1f51b94 100644
--- a/src/operator/tensor/cast_storage-inl.cuh
+++ b/src/operator/tensor/cast_storage-inl.cuh
@@ -105,8 +105,14 @@ void CastStorageDnsRspGPUImpl_(const OpContext& ctx,
   // Allocate temp storage for marking non-zero rows and for cub's prefix sum
   CHECK_GT(ctx.requested.size(), 0);
   // The resource is located at the end of requested resource array
+#if MXNET_MEMORY_PROFILER_ON
+  mshadow::Tensor<gpu, 1, char> workspace = ctx.requested[ctx.requested.size() - 1]
+    .get_space_typed<gpu, 1, char>(Shape1(num_rows * sizeof(RType) + temp_storage_bytes), s,
+								   L"workspace:cast_storage-inl");
+#else
   mshadow::Tensor<gpu, 1, char> workspace = ctx.requested[ctx.requested.size() - 1]
     .get_space_typed<gpu, 1, char>(Shape1(num_rows * sizeof(RType) + temp_storage_bytes), s);
+#endif
 
   row_flg = reinterpret_cast<RType *>(workspace.dptr_);
   d_temp_storage = workspace.dptr_ + num_rows * sizeof(RType);
@@ -164,7 +170,12 @@ void CastStorageDnsRspGPUImpl_(const OpContext& ctx,
   CUDA_CALL(cudaMemcpy(&nnr, &row_flg[num_rows - 1], sizeof(dim_t), cudaMemcpyDeviceToHost));
 
   // Allocate rsp tensor row index array and fill
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+  rsp->CheckAndAllocAuxData(rowsparse::kIdx, Shape1(nnr), L"rsp_tensor_row:cast_storage-inl");
+#else
   rsp->CheckAndAllocAuxData(rowsparse::kIdx, Shape1(nnr));
+#endif
   if (0 == nnr) return;
   RType *row_idx = rsp->aux_data(rowsparse::kIdx).dptr<RType>();
   num_threads = num_rows;
@@ -174,7 +185,12 @@ void CastStorageDnsRspGPUImpl_(const OpContext& ctx,
   // Construct shape of rsp tensor data, allocate, and fill
   auto storage_shape = dns.shape_;
   storage_shape[0] = nnr;
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+  rsp->CheckAndAllocData(storage_shape, L"rsp_tensor:cast_storage-inl");
+#else
   rsp->CheckAndAllocData(storage_shape);
+#endif
   num_threads = nnr * row_length;
   Kernel<CastDnsRspValsKernel, gpu>::Launch(s, num_threads,
                                             rsp->data().dptr<DType>(), row_idx, dns.dptr<DType>(),
@@ -483,7 +499,12 @@ inline void CastStorageDnsCsrImpl(const OpContext& ctx,
         if (threads_per_warp != 32) {
           LOG(FATAL) << "CastStorageDnsCsrImpl GPU kernels expect warpSize=32";
         }
+#if MXNET_MEMORY_PROFILER_ON
+		/*@ABHISHEK @TODO: Verify that the tag is okay*/
+        csr->CheckAndAllocAuxData(csr::kIndPtr, Shape1(num_rows+1), L"csr_matrix:cast_storage-inl");
+#else
         csr->CheckAndAllocAuxData(csr::kIndPtr, Shape1(num_rows+1));
+#endif
         IType* indptr = csr->aux_data(csr::kIndPtr).dptr<IType>();
         DType* dns_data = dns.dptr<DType>();
 
@@ -538,8 +559,14 @@ inline void CastStorageDnsCsrImpl(const OpContext& ctx,
         // Allocate temporary storage from requested resource.
        CHECK_GT(ctx.requested.size(), 0);
        // The resource is located at the end of requested resource array
+#if MXNET_MEMORY_PROFILER_ON
+       auto workspace = ctx.requested[ctx.requested.size() - 1].
+          get_space_typed<gpu, 1, char>(Shape1(temp_storage_bytes), s,
+										L"workspace:cast_storage-inl");
+#else
        auto workspace = ctx.requested[ctx.requested.size() - 1].
           get_space_typed<gpu, 1, char>(Shape1(temp_storage_bytes), s);
+#endif
        d_temp_storage = workspace.dptr_;
 
         // Compute indptr through inclusive prefix sum
@@ -555,8 +582,14 @@ inline void CastStorageDnsCsrImpl(const OpContext& ctx,
         CUDA_CALL(cudaMemcpy(&nnz, &(indptr[num_rows]), sizeof(IType), cudaMemcpyDeviceToHost));
 
         // Allocate column index array and data array of the csr matrix
+#if MXNET_MEMORY_PROFILER_ON
+		/*@ABHISHEK @TODO: Verify that the tag is okay*/
+        csr->CheckAndAllocAuxData(csr::kIdx, Shape1(static_cast<dim_t>(nnz)), L"csr_column_index_array:cast_storage-inl");
+        csr->CheckAndAllocData(Shape1(static_cast<dim_t>(nnz)), L"csr_data_array:cast_storage-inl");
+#else
         csr->CheckAndAllocAuxData(csr::kIdx, Shape1(static_cast<dim_t>(nnz)));
         csr->CheckAndAllocData(Shape1(static_cast<dim_t>(nnz)));
+#endif
 
         // Compute and fill column index array and data array of the csr matrix
         switch (kernel_version) {
diff --git a/src/operator/tensor/cast_storage-inl.h b/src/operator/tensor/cast_storage-inl.h
index ebe19d4..8a35c0a 100644
--- a/src/operator/tensor/cast_storage-inl.h
+++ b/src/operator/tensor/cast_storage-inl.h
@@ -81,7 +81,12 @@ inline void CastStorageDnsRspImpl(const OpContext& ctx,
     MSHADOW_IDX_TYPE_SWITCH(rsp->aux_type(kIdx), RType, {  // row idx type
       const dim_t num_rows = dns.shape_[0];
       const dim_t row_length = dns.shape_.ProdShape(1, dns.shape_.ndim());
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+      rsp->CheckAndAllocAuxData(kIdx, Shape1(num_rows), L"cpu_rsp:cast_storage-inl");
+#else
       rsp->CheckAndAllocAuxData(kIdx, Shape1(num_rows));
+#endif
       TBlob row_idx_blob = rsp->aux_data(kIdx);
       RType* row_idx = row_idx_blob.dptr<RType>();
       dim_t num_threads = num_rows;
@@ -93,7 +98,12 @@ inline void CastStorageDnsRspImpl(const OpContext& ctx,
       if (0 == nnr) return;
       auto storage_shape = dns.shape_;
       storage_shape[0] = nnr;
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+      rsp->CheckAndAllocData(storage_shape, L"cpu_rsp:cast_storage-inl");
+#else
       rsp->CheckAndAllocData(storage_shape);
+#endif
       auto dns_data = dns.get_with_shape<cpu, 2, DType>(Shape2(num_rows, row_length), s);
       auto rsp_data = rsp->data().get_with_shape<cpu, 2, DType>(Shape2(nnr, row_length), s);
       dim_t idx = 0;
@@ -240,7 +250,12 @@ inline void CastStorageDnsCsrImpl(const OpContext& ctx,
       MSHADOW_IDX_TYPE_SWITCH(csr->aux_type(csr::kIdx), CType, {  // col idx type
         const dim_t num_rows = dns.shape_[0];
         const dim_t num_cols = dns.shape_[1];
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+        csr->CheckAndAllocAuxData(csr::kIndPtr, mshadow::Shape1(num_rows+1), L"cpu_dns_csr:cast_storage-inl");
+#else
         csr->CheckAndAllocAuxData(csr::kIndPtr, mshadow::Shape1(num_rows+1));
+#endif
         IType* indptr = csr->aux_data(csr::kIndPtr).dptr<IType>();
         DType* dns_data = dns.dptr<DType>();
         dim_t num_threads = num_rows;
@@ -253,8 +268,14 @@ inline void CastStorageDnsCsrImpl(const OpContext& ctx,
           indptr[i+1] += indptr[i];
         }
         // allocate column idx array and value array
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+        csr->CheckAndAllocAuxData(csr::kIdx, Shape1(static_cast<index_t>(indptr[num_rows])), L"cpu_dns_csr:cast_storage-inl");
+        csr->CheckAndAllocData(Shape1(static_cast<index_t>(indptr[num_rows])), L"cpu_dns_csr:cast_storage-inl");
+#else
         csr->CheckAndAllocAuxData(csr::kIdx, Shape1(static_cast<index_t>(indptr[num_rows])));
         csr->CheckAndAllocData(Shape1(static_cast<index_t>(indptr[num_rows])));
+#endif
         // fill col_idx and value arrays of the csr
         mxnet_op::Kernel<FillCsrColIdxAndVals, cpu>::Launch(s, num_threads,
             csr->data().dptr<DType>(), csr->aux_data(csr::kIdx).dptr<CType>(),
diff --git a/src/operator/tensor/dot-inl.cuh b/src/operator/tensor/dot-inl.cuh
index fd668b3..4131401 100644
--- a/src/operator/tensor/dot-inl.cuh
+++ b/src/operator/tensor/dot-inl.cuh
@@ -627,9 +627,16 @@ inline void DotCsrDnsRspImpl(const OpContext& ctx,
                                         row_flg_out,
                                         num_cols_l,
                                         mshadow::Stream<gpu>::GetStream(s));
+#if MXNET_MEMORY_PROFILER_ON
+          mshadow::Tensor<gpu, 1, char> workspace = ctx.requested[0]
+              .get_space_typed<gpu, 1, char>(Shape1(num_cols_l * sizeof(dim_t) +
+                                                    temp_storage_bytes), s,
+                          L"workspace:dot-inl");
+#else
           mshadow::Tensor<gpu, 1, char> workspace = ctx.requested[0]
               .get_space_typed<gpu, 1, char>(Shape1(num_cols_l * sizeof(dim_t) +
                                                     temp_storage_bytes), s);
+#endif
           row_flg_out = reinterpret_cast<dim_t*>(workspace.dptr_);
           d_temp_storage = workspace.dptr_ + num_cols_l*sizeof(dim_t);
           num_threads = num_cols_l;
@@ -649,7 +656,12 @@ inline void DotCsrDnsRspImpl(const OpContext& ctx,
                                cudaMemcpyDeviceToHost));
 
           // Allocate output matrix space
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+          ret->CheckAndAlloc({Shape1(nnr_out)}, L"dot_csr_dns_rsp_impl:dot-inl");
+#else
           ret->CheckAndAlloc({Shape1(nnr_out)});
+#endif
           const TBlob data_out_blob = ret->data();
           const TBlob row_idx_out_blob = ret->aux_data(rowsparse::kIdx);
           MSHADOW_IDX_TYPE_SWITCH(row_idx_out_blob.type_flag_, RType, {  // row idx type
@@ -746,9 +758,16 @@ inline void DotCsrRspRspImpl(const OpContext& ctx,
                                           row_flg_out,
                                           num_cols_l,
                                           mshadow::Stream<gpu>::GetStream(s));
+#if MXNET_MEMORY_PROFILER_ON
+            mshadow::Tensor<gpu, 1, char> workspace = ctx.requested[0]
+                .get_space_typed<gpu, 1, char>(Shape1(num_cols_l * sizeof(dim_t) +
+                                                      temp_storage_bytes), s,
+                            L"workspace:dot-inl");
+#else
             mshadow::Tensor<gpu, 1, char> workspace = ctx.requested[0]
                 .get_space_typed<gpu, 1, char>(Shape1(num_cols_l * sizeof(dim_t) +
                                                       temp_storage_bytes), s);
+#endif
             row_flg_out = reinterpret_cast<dim_t*>(workspace.dptr_);
             d_temp_storage = workspace.dptr_ + num_cols_l*sizeof(dim_t);
             num_threads = num_cols_l;
@@ -768,7 +787,12 @@ inline void DotCsrRspRspImpl(const OpContext& ctx,
                                  cudaMemcpyDeviceToHost));
 
             // Allocate output matrix space
+#if MXNET_MEMORY_PROFILER_ON
+      /*@ABHISHEK @TODO: Verify that the tag is okay*/
+            ret->CheckAndAlloc({mshadow::Shape1(nnr_out)}, L"dot_csr_rsp_rsp_impl:dot-inl");
+#else
             ret->CheckAndAlloc({mshadow::Shape1(nnr_out)});
+#endif
             const TBlob data_out_blob = ret->data();
             const TBlob row_idx_out_blob = ret->aux_data(rowsparse::kIdx);
             DType* data_out = data_out_blob.dptr<DType>();
@@ -855,8 +879,15 @@ inline void DotCsrRspDnsImpl(const OpContext& ctx,
           } else {
             // TODO: Consider implementing a vector kernel for SpMV (similar to DotCsrDnsDns)
             // Alloc temp storage for row_flg array
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK: is the tag appropriate?*/
+            RType* row_flg_r = ctx.requested[0]
+                .get_space_typed<gpu, 1, RType>(mshadow::Shape1(rhs.shape()[0]), s,
+                        L"array:dot-inl").dptr_;
+#else
             RType* row_flg_r = ctx.requested[0]
                 .get_space_typed<gpu, 1, RType>(mshadow::Shape1(rhs.shape()[0]), s).dptr_;
+#endif
             num_threads = rhs.shape()[0];
             Kernel<set_zero, gpu>::Launch(s, num_threads, row_flg_r);
             // Set row_flg index array
diff --git a/src/operator/tensor/dot-inl.h b/src/operator/tensor/dot-inl.h
index 26061cb..1a1582e 100644
--- a/src/operator/tensor/dot-inl.h
+++ b/src/operator/tensor/dot-inl.h
@@ -599,7 +599,12 @@ inline void DotCsrDnsRspImpl(const OpContext& ctx,
   const TBlob& data_r = rhs;
 
   // pre-allocate spaces for ret using the dense dimension size
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+  ret->CheckAndAlloc({mshadow::Shape1(lhs.shape()[1])}, L"cpu_csr_dns_rsp:dot-inl");
+#else
   ret->CheckAndAlloc({mshadow::Shape1(lhs.shape()[1])});
+#endif
   const TBlob data_out = ret->data();
   const TBlob row_idx_out = ret->aux_data(rowsparse::kIdx);
 
@@ -740,7 +745,12 @@ inline void DotCsrRspRspImpl(const OpContext& ctx,
 
   // pre-allocate spaces for ret using the dense dimension size
   if (ret->storage_type() == kRowSparseStorage) {
+#if MXNET_MEMORY_PROFILER_ON
+    /*@ABHISHEK @TODO: Verify that the tag is okay*/
+    ret->CheckAndAlloc({mshadow::Shape1(lhs.shape()[1])}, L"cpu_csr_rsp_rsp:dot-inl");
+#else
     ret->CheckAndAlloc({mshadow::Shape1(lhs.shape()[1])});
+#endif
   }
   const TBlob data_out = ret->data();
   const TBlob row_idx_out = ret->aux_data(rowsparse::kIdx);
@@ -926,8 +936,14 @@ void BatchDotForward_(const nnvm::NodeAttrs& attrs,
     mshadow::Tensor<xpu, 3, DType> out = outputs[0].get<xpu, 3, DType>(s);
     mshadow::Tensor<xpu, 3, DType> mlhs = inputs[0].get<xpu, 3, DType>(s);
     mshadow::Tensor<xpu, 3, DType> mrhs = inputs[1].get<xpu, 3, DType>(s);
+#if MXNET_MEMORY_PROFILER_ON
+    mshadow::Tensor<xpu, 1, DType*> workspace =
+      ctx.requested[0].get_space_typed<xpu, 1, DType*>(mshadow::Shape1(3 * out.size(0)), s,
+													   L"workspace:dot-inl");
+#else
     mshadow::Tensor<xpu, 1, DType*> workspace =
       ctx.requested[0].get_space_typed<xpu, 1, DType*>(mshadow::Shape1(3 * out.size(0)), s);
+#endif
     if (kNullOp != req[0]) {
       if (param.transpose_a && param.transpose_b) {
         mshadow::BatchGEMM<true, true>(out, mlhs, mrhs, (DType)1.0f,
@@ -970,9 +986,15 @@ void BatchDotBackward_(const nnvm::NodeAttrs& attrs,
     mshadow::Tensor<xpu, 3, DType> mrhs_data = inputs[2].get<xpu, 3, DType>(s);
     mshadow::Tensor<xpu, 3, DType> mlhs_grad = outputs[0].get<xpu, 3, DType>(s);
     mshadow::Tensor<xpu, 3, DType> mrhs_grad = outputs[1].get<xpu, 3, DType>(s);
+#if MXNET_MEMORY_PROFILER_ON
+    mshadow::Tensor<xpu, 2, DType*> workspace =
+      ctx.requested[0].get_space_typed<xpu, 2, DType*>(
+        mshadow::Shape2(2, 3 * mout_grad.size(0)), s, L"workspace:dot-inl");
+#else
     mshadow::Tensor<xpu, 2, DType*> workspace =
       ctx.requested[0].get_space_typed<xpu, 2, DType*>(
         mshadow::Shape2(2, 3 * mout_grad.size(0)), s);
+#endif
     mshadow::Tensor<xpu, 1, DType*> rhs_workspace = workspace[0];
     mshadow::Tensor<xpu, 1, DType*> lhs_workspace = workspace[1];
     if (param.transpose_a && param.transpose_b) {
diff --git a/src/operator/tensor/elemwise_binary_broadcast_op.h b/src/operator/tensor/elemwise_binary_broadcast_op.h
index 7aae9cc..a3904e5 100644
--- a/src/operator/tensor/elemwise_binary_broadcast_op.h
+++ b/src/operator/tensor/elemwise_binary_broadcast_op.h
@@ -179,8 +179,13 @@ void BinaryBroadcastBackwardUseNone(const nnvm::NodeAttrs& attrs,
         size_t workspace_size_l = ReduceWorkspaceSize<NDim, DType>(s, lhs, req[0], out);
         size_t workspace_size_r = ReduceWorkspaceSize<NDim, DType>(s, rhs, req[1], out);
         size_t workspace_size = std::max(workspace_size_l, workspace_size_r);
+#if MXNET_MEMORY_PROFILER_ON
+        Tensor<xpu, 1, char> workspace =
+          ctx.requested[0].get_space_typed<xpu, 1, char>(Shape1(workspace_size), s, L"workspace:elemwise_binary_broadcast_op");
+#else
         Tensor<xpu, 1, char> workspace =
           ctx.requested[0].get_space_typed<xpu, 1, char>(Shape1(workspace_size), s);
+#endif
         Reduce<red::sum, NDim, DType, LOP>(s, lhs, req[0], workspace, out);
         Reduce<red::sum, NDim, DType, ROP>(s, rhs, req[1], workspace, out);
       });
@@ -208,8 +213,13 @@ inline void BinaryBroadcastBackwardUseInImpl(const OpContext& ctx,
   size_t workspace_size_l = ReduceWorkspaceSize<ndim, DType>(s, lgrad, req[0], ograd, lhs, rhs);
   size_t workspace_size_r = ReduceWorkspaceSize<ndim, DType>(s, rgrad, req[1], ograd, lhs, rhs);
   size_t workspace_size = std::max(workspace_size_l, workspace_size_r);
+#if MXNET_MEMORY_PROFILER_ON
+  Tensor<xpu, 1, char> workspace =
+    ctx.requested[0].get_space_typed<xpu, 1, char>(Shape1(workspace_size), s, L"workspace:elemwise_binary_broadcast_op");
+#else
   Tensor<xpu, 1, char> workspace =
     ctx.requested[0].get_space_typed<xpu, 1, char>(Shape1(workspace_size), s);
+#endif
   Reduce<red::sum, ndim, DType, mshadow::op::mul, LOP>(s, lgrad, req[0], workspace,
     ograd, lhs, rhs);
   Reduce<red::sum, ndim, DType, mshadow::op::mul, ROP>(s, rgrad, req[1], workspace,
diff --git a/src/operator/tensor/elemwise_binary_op-inl.h b/src/operator/tensor/elemwise_binary_op-inl.h
index 3d97040..8fb399d 100644
--- a/src/operator/tensor/elemwise_binary_op-inl.h
+++ b/src/operator/tensor/elemwise_binary_op-inl.h
@@ -79,17 +79,34 @@ void ElemwiseBinaryOp::RspRspOp(mshadow::Stream<cpu> *s,
   const size_t num_rows_l = lhs_is_dense ? lhs.shape()[0] : lhs.aux_shape(rowsparse::kIdx).Size();
   const size_t num_rows_r = rhs_is_dense ? rhs.shape()[0] : rhs.aux_shape(rowsparse::kIdx).Size();
   if (is_dense_result) {
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: No GPU ops should we be tagging here?*/
+    output.CheckAndAlloc(L"rsp_rsp_op:elemwise_binary_op-inl");
+#else
     output.CheckAndAlloc();
+#endif
   } else {
     if (rhs_is_dense || scatter) {
+#if MXNET_MEMORY_PROFILER_ON
+      output.CheckAndAlloc({mshadow::Shape1(num_rows_l)}, L"rsp_rsp_op:elemwise_binary_op-inl");
+#else
       output.CheckAndAlloc({mshadow::Shape1(num_rows_l)});
+#endif
     } else if (lhs_is_dense) {
+#if MXNET_MEMORY_PROFILER_ON
+      output.CheckAndAlloc({mshadow::Shape1(num_rows_r)}, L"rsp_rsp_op:elemwise_binary_op-inl");
+#else
       output.CheckAndAlloc({mshadow::Shape1(num_rows_r)});
+#endif
     } else {
       lhs_in_place = IsSameArray(lhs, output);
       rhs_in_place = IsSameArray(rhs, output);
       if (!lhs_in_place && !rhs_in_place) {
+#if MXNET_MEMORY_PROFILER_ON
+        output.CheckAndAlloc({mshadow::Shape1(num_rows_l + num_rows_r)}, L"rsp_rsp_op:elemwise_binary_op-inl");
+#else
         output.CheckAndAlloc({mshadow::Shape1(num_rows_l + num_rows_r)});
+#endif
       } else {
         CHECK_EQ(allow_inplace, true);
         CHECK_EQ(is_dense_result, false);
@@ -272,15 +289,27 @@ void ElemwiseBinaryOp::CsrCsrOp(mshadow::Stream<cpu> *s,
 
   const size_t output_nnz_guess = same_lhs_rhs ? lhs_nnz : lhs_nnz + rhs_nnz;
 
+#if MXNET_MEMORY_PROFILER_ON
+  output.CheckAndAlloc({mshadow::Shape1(lhs.shape()[0] + 1),
+                        mshadow::Shape1(std::min(output_nnz_guess, lhs.shape().Size()))},
+						L"csr_csr_op:elemwise_binary_op");
+#else
   output.CheckAndAlloc({mshadow::Shape1(lhs.shape()[0] + 1),
                         mshadow::Shape1(std::min(output_nnz_guess, lhs.shape().Size()))});
+#endif
   DCHECK_EQ(output.aux_shape(csr::kIndPtr), lhs.aux_shape(csr::kIndPtr));
 
   const size_t alloc_size = nr_cols * sizeof(IType) + 2 * nr_cols * sizeof(DType);
 
+#if MXNET_MEMORY_PROFILER_ON
+  Tensor<cpu, 1, uint8_t> workspace =
+    ctx.requested[ResourceRequestType::kTempSpace].get_space_typed<cpu, 1, uint8_t>(
+      mshadow::Shape1(alloc_size), s, L"workspace:elemwise_binary_op-inl");
+#else
   Tensor<cpu, 1, uint8_t> workspace =
     ctx.requested[ResourceRequestType::kTempSpace].get_space_typed<cpu, 1, uint8_t>(
       mshadow::Shape1(alloc_size), s);
+#endif
 
   // Allocate temp space and partition into three tensors
   mshadow::Tensor<cpu, 1, IType> next(reinterpret_cast<IType *>(workspace.dptr_),
diff --git a/src/operator/tensor/elemwise_unary_op.h b/src/operator/tensor/elemwise_unary_op.h
index 6e0aeeb..ee4112e 100644
--- a/src/operator/tensor/elemwise_unary_op.h
+++ b/src/operator/tensor/elemwise_unary_op.h
@@ -71,17 +71,34 @@ class OpBase {
     if (req != kNullOp) {
       if (clone_from) {
         const TShape& ishape = clone_from->storage_shape();
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Trace who calls this and pass appropriate tag*/
+        dest->CheckAndAllocData(ishape, L"workspace:elem_wise_unary_op");
+#else
         dest->CheckAndAllocData(ishape);
+#endif
         CHECK_EQ(dest->storage_type(), clone_from->storage_type());
         for (size_t i = 0, n = clone_from->aux_shapes().size(); i < n; ++i) {
+#if MXNET_MEMORY_PROFILER_ON
+          dest->CheckAndAllocAuxData(i, clone_from->aux_shape(i), L"workspace:elem_wise_unary_op");
+#else
           dest->CheckAndAllocAuxData(i, clone_from->aux_shape(i));
+#endif
         }
         DCHECK_EQ(dest->aux_shapes().size(), clone_from->aux_shapes().size());
       } else {
         for (size_t i = 0, n = dest->aux_shapes().size(); i < n; ++i) {
+#if MXNET_MEMORY_PROFILER_ON
+          dest->CheckAndAllocAuxData(i, dest->aux_shape(i), L"allocate_geometry:elem_wise_unary_op");
+#else
           dest->CheckAndAllocAuxData(i, dest->aux_shape(i));
+#endif
         }
+#if MXNET_MEMORY_PROFILER_ON
+        dest->CheckAndAllocData(dest->storage_shape(), L"allocate_geometry:elemwise_unary_op");
+#else
         dest->CheckAndAllocData(dest->storage_shape());
+#endif
       }
     }
   }
@@ -195,7 +212,12 @@ class UnaryOp : public OpBase {
         for (size_t j = 0; j < aux_shape_count; ++j) {
           aux_shapes.emplace_back(input.aux_shape(j));
         }
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+        output->CheckAndAlloc(aux_shapes, L"unary_op:elemwise_unary_op");
+#else
         output->CheckAndAlloc(aux_shapes);
+#endif
         DCHECK_EQ(output->storage_shape(), input.storage_shape());
       }
       return true;
@@ -224,7 +246,12 @@ class UnaryOp : public OpBase {
     CHECK_EQ(inputs[0].storage_type(), outputs[0].storage_type());
     AllocateGeometry(&outputs[0], req[0], &inputs[0]);
     CopyGeometryBlobs<xpu>(ctx.get_stream<xpu>(), &outputs[0], req[0], inputs[0]);
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+    outputs[0].CheckAndAllocData(inputs[0].storage_shape(), L"map_to_f_compute:elemwise_unary_op");
+#else
     outputs[0].CheckAndAllocData(inputs[0].storage_shape());
+#endif
     if (inputs[0].storage_shape().Size()) {
       OpBase::MapToFCompute<xpu>(attrs, ctx, inputs, req, outputs, computer);
     }
diff --git a/src/operator/tensor/indexing_op.h b/src/operator/tensor/indexing_op.h
index 36b6e76..9af4561 100644
--- a/src/operator/tensor/indexing_op.h
+++ b/src/operator/tensor/indexing_op.h
@@ -262,9 +262,15 @@ void AddTakeGradLargeBatchCaller(const OpContext& ctx, mshadow::Tensor<xpu, 2, D
   size_t workspace_size = 2*(index.shape_.Size()*sizeof(int)) + temp_storage_size;
 
   // Request temporary storage
+#if MXNET_MEMORY_PROFILER_ON
+  Tensor<xpu, 1, char> workspace =
+    ctx.requested[embedding::kTempSpace].get_space_typed<xpu, 1, char>(
+      Shape1(workspace_size), s, L"workspace:indexing_op");
+#else
   Tensor<xpu, 1, char> workspace =
     ctx.requested[embedding::kTempSpace].get_space_typed<xpu, 1, char>(
       Shape1(workspace_size), s);
+#endif
 
   // Create tensors
   size_t pos = 0;
diff --git a/src/operator/tensor/init_op.cu b/src/operator/tensor/init_op.cu
index a1982ca..f097533 100644
--- a/src/operator/tensor/init_op.cu
+++ b/src/operator/tensor/init_op.cu
@@ -34,7 +34,12 @@ namespace op {
  */
 void FillZerosCsrImpl(mshadow::Stream<mshadow::gpu> *s, const NDArray& dst) {
   dst.set_aux_shape(csr::kIdx, mshadow::Shape1(0));
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+  dst.CheckAndAllocAuxData(csr::kIndPtr, mshadow::Shape1(dst.shape()[0] + 1), L"fill_zeros_csr:init_op");
+#else
   dst.CheckAndAllocAuxData(csr::kIndPtr, mshadow::Shape1(dst.shape()[0] + 1));
+#endif
   TBlob indptr_data = dst.aux_data(csr::kIndPtr);
   MSHADOW_IDX_TYPE_SWITCH(dst.aux_type(csr::kIndPtr), IType, {
     mxnet_op::Kernel<mxnet_op::set_zero, mshadow::gpu>::Launch(
diff --git a/src/operator/tensor/init_op.h b/src/operator/tensor/init_op.h
index ea4243e..c62ed4e 100644
--- a/src/operator/tensor/init_op.h
+++ b/src/operator/tensor/init_op.h
@@ -199,7 +199,12 @@ inline void FillDnsZerosRspImpl(mshadow::Stream<xpu> *s, NDArray *dst) {
   MSHADOW_REAL_TYPE_SWITCH(dst->dtype(), DType, {
     MSHADOW_IDX_TYPE_SWITCH(dst->aux_type(kIdx), IType, {
       auto num_rows = dst->shape()[0];
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+      dst->CheckAndAlloc({Shape1(num_rows)}, L"dns_zeroes_rsp:init_op");
+#else
       dst->CheckAndAlloc({Shape1(num_rows)});
+#endif
       auto idx = dst->aux_data(kIdx);
       auto val = dst->data();
       Kernel<set_zero, xpu>::Launch(s, val.Size(), val.dptr<DType>());
@@ -214,7 +219,12 @@ void PopulateFullIdxRspImpl(mshadow::Stream<xpu> *s, NDArray *dst) {
   using namespace rowsparse;
   CHECK_EQ(dst->storage_type(), kRowSparseStorage);
   nnvm::dim_t nnr = dst->shape()[0];
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+  dst->CheckAndAllocAuxData(kIdx, mshadow::Shape1(nnr), L"populate_fll_idx_rsp:init_op");
+#else
   dst->CheckAndAllocAuxData(kIdx, mshadow::Shape1(nnr));
+#endif
   MSHADOW_IDX_TYPE_SWITCH(dst->aux_type(kIdx), IType, {
     IType* idx = dst->aux_data(kIdx).dptr<IType>();
     mxnet_op::Kernel<PopulateFullIdxRspKernel, xpu>::Launch(s, nnr, idx);
@@ -242,7 +252,12 @@ void FillZerosRspImpl(mshadow::Stream<xpu> *, const NDArray& dst) {
  */
 inline void FillZerosCsrImpl(mshadow::Stream<mshadow::cpu> *s, const NDArray& dst) {
   dst.set_aux_shape(csr::kIdx, mshadow::Shape1(0));
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+  dst.CheckAndAllocAuxData(csr::kIndPtr, mshadow::Shape1(dst.shape()[0] + 1), L"fill_zeros_csr:init_op");
+#else
   dst.CheckAndAllocAuxData(csr::kIndPtr, mshadow::Shape1(dst.shape()[0] + 1));
+#endif
   TBlob indptr_data = dst.aux_data(csr::kIndPtr);
   MSHADOW_IDX_TYPE_SWITCH(dst.aux_type(csr::kIndPtr), IType, {
     mxnet_op::Kernel<mxnet_op::set_zero, mshadow::cpu>::Launch(
diff --git a/src/operator/tensor/la_op.h b/src/operator/tensor/la_op.h
index a323139..6fac776 100644
--- a/src/operator/tensor/la_op.h
+++ b/src/operator/tensor/la_op.h
@@ -488,8 +488,14 @@ void LaOpBackward(const nnvm::NodeAttrs& attrs,
     std::vector<TBlob> tspace(outputs);
     for ( int i = 0; i < onum; ++i ) {
       if ( req[i] == kAddTo ) {
+#if MXNET_MEMORY_PROFILER_ON
+        tspace[i].dptr_ = ctx.requested[0]
+                             .get_space_typed<xpu, 1, OType>(Shape1(outputs[i].Size()), s,
+															 L"workspace:la_op").dptr_;
+#else
         tspace[i].dptr_ = ctx.requested[0]
                              .get_space_typed<xpu, 1, OType>(Shape1(outputs[i].Size()), s).dptr_;
+#endif
       }
     }
     LaOpCaller<xpu, OType, idim, odim, inum, onum, laop>::op(inputs, tspace,
@@ -538,8 +544,13 @@ void LaOpBackwSyevd(const nnvm::NodeAttrs& attrs,
   MSHADOW_SGL_DBL_TYPE_SWITCH(outputs[0].type_flag_, OType, {
     std::vector<TBlob> tspace(outputs);
     if ( req[0] == kAddTo ) {
+#if MXNET_MEMORY_PROFILER_ON
+      tspace[0].dptr_ = ctx.requested[0]
+        .get_space_typed<xpu, 1, OType>(Shape1(outputs[0].Size()), s, L"workspace:la_op").dptr_;
+#else
       tspace[0].dptr_ = ctx.requested[0]
         .get_space_typed<xpu, 1, OType>(Shape1(outputs[0].Size()), s).dptr_;
+#endif
     }
     laop::op(inputs[0].FlatToKD<xpu, 3, OType>(s),
              inputs[1].FlatToKD<xpu, 2, OType>(s),
diff --git a/src/operator/tensor/la_op_inline.h b/src/operator/tensor/la_op_inline.h
index f372162..13419a2 100644
--- a/src/operator/tensor/la_op_inline.h
+++ b/src/operator/tensor/la_op_inline.h
@@ -258,8 +258,13 @@ struct gelqf {
     // The size is determined by workspace queries, done on the first items
     // of the batch
     int ws_size(linalg_gelqf_workspace_query(Q[0], s));
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> work = ctx.requested[0]
+      .get_space_typed<xpu, 1, DType>(Shape1(ws_size), s, L"workspace:la_op_inline");
+#else
     Tensor<xpu, 1, DType> work = ctx.requested[0]
       .get_space_typed<xpu, 1, DType>(Shape1(ws_size), s);
+#endif
     // Loop over items in batch
     linalg_check_batch_size(A.size(0), Q.size(0), L.size(0));
     int m = Q.size(1);  // Q[i] has shape (m, n)
@@ -326,8 +331,13 @@ struct syevd {
     // From here on, we work on U only
     // Reserve workspace (size determined by query)
     int lwork(linalg_syevd_workspace_query(U[0], L[0], s));
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 1, DType> work = ctx.requested[0]
+      .get_space_typed<xpu, 1, DType>(Shape1(lwork), s, L"workspace:la_op_inline");
+#else
     Tensor<xpu, 1, DType> work = ctx.requested[0]
       .get_space_typed<xpu, 1, DType>(Shape1(lwork), s);
+#endif
     // Loop over items in batch
     for (index_t i = 0; i < U.size(0); ++i) {
       linalg_syevd(U[i], L[i], work, s);
@@ -582,8 +592,13 @@ struct gelqf_backward {
     Stream<xpu> *s = ctx.get_stream<xpu>();
     if (dQ.dptr_ != dA.dptr_) Copy(dA, dQ, s);
     // Need temporal space, same shape as dL
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 3, DType> tempM = ctx.requested[0]
+      .get_space_typed<xpu, 3, DType>(dL.shape_, s, L"temporal_space:la_op_inline");
+#else
     Tensor<xpu, 3, DType> tempM = ctx.requested[0]
       .get_space_typed<xpu, 3, DType>(dL.shape_, s);
+#endif
     Copy(tempM, dL, s);
     trmm::op(L, tempM, DType(1.0), false, true, s);
     gemm::op(dA, Q, tempM, DType(-1.0), DType(1.0), false, true, s);
@@ -653,8 +668,13 @@ struct syevd_backward {
     using namespace mxnet_op;
     Stream<xpu> *s = ctx.get_stream<xpu>();
     // Need temporal space, same shape as dA
+#if MXNET_MEMORY_PROFILER_ON
+    Tensor<xpu, 3, DType> tempM = ctx.requested[0]
+      .get_space_typed<xpu, 3, DType>(dA.shape_, s, L"temporal_space:la_op_inline");
+#else
     Tensor<xpu, 3, DType> tempM = ctx.requested[0]
       .get_space_typed<xpu, 3, DType>(dA.shape_, s);
+#endif
     // This copy is just to make sure there are no invalid values (NaN, infinity) in
     // tempM. gemm multiplies tempM with 0, instead of setting entries to 0.
     Copy(tempM, dU, s);
diff --git a/src/operator/tensor/matrix_op-inl.h b/src/operator/tensor/matrix_op-inl.h
index 1c7016c..27fbd4f 100644
--- a/src/operator/tensor/matrix_op-inl.h
+++ b/src/operator/tensor/matrix_op-inl.h
@@ -570,7 +570,12 @@ void SliceCsrImpl(const SliceParam &param, const OpContext& ctx,
   int end = *param.end[0];
   if (end < 0) end += ishape[0];
   int indptr_len = end - begin + 1;
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+  out.CheckAndAllocAuxData(kIndPtr, Shape1(indptr_len), L"slice_csr:matrix_op-inl");
+#else
   out.CheckAndAllocAuxData(kIndPtr, Shape1(indptr_len));
+#endif
   if (!in.storage_initialized()) {
     out.set_aux_shape(kIndPtr, Shape1(0));
     return;
@@ -586,8 +591,14 @@ void SliceCsrImpl(const SliceParam &param, const OpContext& ctx,
         // retrieve nnz (CPU implementation)
         int nnz = out_indptr[indptr_len - 1];
         // copy indices and values
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+        out.CheckAndAllocAuxData(kIdx, Shape1(nnz), L"slice_csr:matrix_op-inl");
+        out.CheckAndAllocData(Shape1(nnz), L"slice_csr:matrix_op-inl");
+#else
         out.CheckAndAllocAuxData(kIdx, Shape1(nnz));
         out.CheckAndAllocData(Shape1(nnz));
+#endif
         auto in_idx = in.aux_data(kIdx).dptr<IType>();
         auto out_idx = out.aux_data(kIdx).dptr<IType>();
         auto in_data = in.data().dptr<DType>();
@@ -1548,9 +1559,15 @@ void ReverseOpForward(const nnvm::NodeAttrs& attrs,
   }
 
 #ifdef __CUDACC__
+#if MXNET_MEMORY_PROFILER_ON
+  mshadow::Tensor<xpu, 1, uint8_t> workspace =
+    ctx.requested[0].get_space_typed<xpu, 1, uint8_t>(
+      mshadow::Shape1(reverse_index * sizeof(index_t) * 2), s, L"workspace:matrix_op-inl");
+#else
   mshadow::Tensor<xpu, 1, uint8_t> workspace =
     ctx.requested[0].get_space_typed<xpu, 1, uint8_t>(
       mshadow::Shape1(reverse_index * sizeof(index_t) * 2), s);
+#endif
 
   auto stride_workspace = workspace.dptr_;
   auto trailing_workspace = workspace.dptr_ + reverse_index * sizeof(index_t);
diff --git a/src/operator/tensor/ordering_op-inl.h b/src/operator/tensor/ordering_op-inl.h
index 5605541..d8125da 100644
--- a/src/operator/tensor/ordering_op-inl.h
+++ b/src/operator/tensor/ordering_op-inl.h
@@ -197,7 +197,11 @@ void TopKImpl(RunContext ctx,
   if (param.ret_typ == topk_enum::kReturnMask) {
     workspace_size += sizeof(int) * batch_size * k + sizeof(real_t) * batch_size * k;
   }
+#if MXNET_MEMORY_PROFILER_ON
+  workspace = resource.get_space_typed<xpu, 1, char>(Shape1(workspace_size), s, L"workspace:ordering_op-inl");
+#else
   workspace = resource.get_space_typed<xpu, 1, char>(Shape1(workspace_size), s);
+#endif
   char* workspace_curr_ptr = workspace.dptr_;
   sorted_dat = Tensor<xpu, 1, real_t>(reinterpret_cast<real_t*>(workspace_curr_ptr),
                                       Shape1(src.Size()), s);  // contain sorted dat
@@ -370,8 +374,13 @@ void TopKBackward_(const nnvm::NodeAttrs& attrs,
   TShape target_shape;
   ParseTopKParam(outputs[0].shape_, param,
                  &target_shape, &batch_size, &element_num, &axis, &k, &do_transpose, &is_ascend);
+#if MXNET_MEMORY_PROFILER_ON
+  Tensor<xpu, 1, real_t> workspace =
+    ctx.requested[0].get_space_typed<xpu, 1, real_t>(Shape1(batch_size * k * 2 + batch_size), s, L"workspace:ordering_op-inl");
+#else
   Tensor<xpu, 1, real_t> workspace =
     ctx.requested[0].get_space_typed<xpu, 1, real_t>(Shape1(batch_size * k * 2 + batch_size), s);
+#endif
   Tensor<xpu, 1, real_t> sel_indices =
     Tensor<xpu, 1, real_t>(workspace.dptr_, Shape1(batch_size * k), s);
   Tensor<xpu, 1, real_t> batch_shift =
diff --git a/src/operator/tensor/sparse_retain-inl.h b/src/operator/tensor/sparse_retain-inl.h
index 8caa65e..2f1226b 100644
--- a/src/operator/tensor/sparse_retain-inl.h
+++ b/src/operator/tensor/sparse_retain-inl.h
@@ -274,7 +274,12 @@ void SparseRetainOpForwardRspImpl(mshadow::Stream<xpu> *s,
   const TBlob input_data = input_nd.data();
   const TBlob input_idx = input_nd.aux_data(rowsparse::kIdx);
 
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+  output_nd->CheckAndAlloc({mshadow::Shape1(idx_data.Size())}, L"activation_sparse_retain_op_forward:matrix_op-inl");
+#else
   output_nd->CheckAndAlloc({mshadow::Shape1(idx_data.Size())});
+#endif
   TBlob output_data = output_nd->data();
   TBlob output_idx = output_nd->aux_data(rowsparse::kIdx);
   const auto row_length = input_data.shape_.ProdShape(1, input_data.shape_.ndim());
@@ -393,7 +398,12 @@ void SparseRetainOpBackwardEx(const nnvm::NodeAttrs& attrs,
   const TBlob out_grad_data = inputs[sr::kOut].data();
 
   NDArray in_grad_nd = outputs[sr::kArr];
+#if MXNET_MEMORY_PROFILER_ON
+  /*@ABHISHEK @TODO: Verify that the tag is okay*/
+  in_grad_nd.CheckAndAlloc({mshadow::Shape1(idx_data.Size())}, L"gradient_sparse_retain_op_backward:sparse_retain-inl");
+#else
   in_grad_nd.CheckAndAlloc({mshadow::Shape1(idx_data.Size())});
+#endif
   TBlob in_grad_data = in_grad_nd.data();
   TBlob in_grad_idx = in_grad_nd.aux_data(rowsparse::kIdx);
   const auto row_length = out_grad_data.shape_.ProdShape(1, out_grad_data.shape_.ndim());
diff --git a/src/operator/tensor/square_sum-inl.h b/src/operator/tensor/square_sum-inl.h
index 7ce5b1e..5777dbf 100644
--- a/src/operator/tensor/square_sum-inl.h
+++ b/src/operator/tensor/square_sum-inl.h
@@ -290,7 +290,12 @@ void SquareSumRspImpl(const nnvm::NodeAttrs& attrs,
   }
 
   if (output->storage_type() == kRowSparseStorage) {
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+    output->CheckAndAlloc({input.aux_shape(rowsparse::kIdx)}, L"activation_square_sum_rsp:square_sum-inl");
+#else
     output->CheckAndAlloc({input.aux_shape(rowsparse::kIdx)});
+#endif
   }
   const TBlob& out_data = output->data();
   const int64_t nnr = input.storage_shape()[0];
@@ -359,7 +364,12 @@ void SquareSumRspGradImpl(const nnvm::NodeAttrs& attrs,
   // TODO(junwu) change the input of CheckAndAlloc
   // if we want to support differen row idx arrays
   // for ograd and input when they are both row-sparse ndarrays
+#if MXNET_MEMORY_PROFILER_ON
+/*@ABHISHEK @TODO: Verify that the tag is okay*/
+  igrad->CheckAndAlloc({input.aux_shape(rowsparse::kIdx)}, L"gradient_square_sum_rsp:square_sum-inl");
+#else
   igrad->CheckAndAlloc({input.aux_shape(rowsparse::kIdx)});
+#endif
   const int64_t num_cols = input.storage_shape()[1];
   const TBlob& igrad_data = igrad->data();
   const TBlob igrad_row_idx = igrad->aux_data(rowsparse::kIdx);
diff --git a/src/resource.cc b/src/resource.cc
index 4c2dbee..9e74d01 100644
--- a/src/resource.cc
+++ b/src/resource.cc
@@ -60,12 +60,20 @@ struct SpaceAllocator {
       host_handle.size = 0;
     }
   }
+#if MXNET_MEMORY_PROFILER_ON
+  inline void* GetSpace(size_t size, std::wstring name = L"") {
+#else
   inline void* GetSpace(size_t size) {
+#endif
     if (handle.size >= size) return handle.dptr;
     if (handle.size != 0) {
       Storage::Get()->DirectFree(handle);
     }
+#if MXNET_MEMORY_PROFILER_ON
+    handle = Storage::Get()->Alloc(size, ctx, name);
+#else
     handle = Storage::Get()->Alloc(size, ctx);
+#endif
     return handle.dptr;
   }
 
@@ -261,9 +269,15 @@ class ResourceManagerImpl : public ResourceManager {
 };
 }  // namespace resource
 
+#if MXNET_MEMORY_PROFILER_ON
+void* Resource::get_space_internal(size_t size, std::wstring name) const {
+  return static_cast<resource::SpaceAllocator*>(ptr_)->GetSpace(size, name);
+}
+#else
 void* Resource::get_space_internal(size_t size) const {
   return static_cast<resource::SpaceAllocator*>(ptr_)->GetSpace(size);
 }
+#endif
 
 void* Resource::get_host_space_internal(size_t size) const {
   return static_cast<resource::SpaceAllocator*>(ptr_)->GetHostSpace(size);
diff --git a/src/storage/cpu_device_storage.h b/src/storage/cpu_device_storage.h
index ead00da..4379dd5 100644
--- a/src/storage/cpu_device_storage.h
+++ b/src/storage/cpu_device_storage.h
@@ -42,7 +42,11 @@ class CPUDeviceStorage {
    * \param size Size to allocate.
    * \return Pointer to the storage.
    */
+#if MXNET_MEMORY_PROFILER_ON
+  inline static void* Alloc(size_t size, std::wstring name);
+#else
   inline static void* Alloc(size_t size);
+#endif
   /*!
    * \brief Deallocation.
    * \param ptr Pointer to deallocate.
@@ -56,7 +60,11 @@ class CPUDeviceStorage {
   static constexpr size_t alignment_ = 16;
 };  // class CPUDeviceStorage
 
+#if MXNET_MEMORY_PROFILER_ON
+inline void* CPUDeviceStorage::Alloc(size_t size, std::wstring name) {
+#else
 inline void* CPUDeviceStorage::Alloc(size_t size) {
+#endif
   void* ptr;
 #if _MSC_VER
   ptr = _aligned_malloc(size, alignment_);
diff --git a/src/storage/gpu_device_storage.h b/src/storage/gpu_device_storage.h
index 3c4f732..3a2f10c 100644
--- a/src/storage/gpu_device_storage.h
+++ b/src/storage/gpu_device_storage.h
@@ -30,6 +30,9 @@
 #include <cuda_runtime.h>
 #endif  // MXNET_USE_CUDA
 #include <new>
+#include "../common/utils.h"
+
+const int MB_Size = 1024 * 1024;
 
 namespace mxnet {
 namespace storage {
@@ -44,7 +47,11 @@ class GPUDeviceStorage {
    * \param size Size to allocate.
    * \return Pointer to the storage.
    */
+#if MXNET_MEMORY_PROFILER_ON
+  inline static void* Alloc(size_t size, std::wstring name);
+#else
   inline static void* Alloc(size_t size);
+#endif
   /*!
    * \brief Deallocation.
    * \param ptr Pointer to deallocate.
@@ -52,12 +59,35 @@ class GPUDeviceStorage {
   inline static void Free(void* ptr);
 };  // class GPUDeviceStorage
 
+#if MXNET_MEMORY_PROFILER_ON
+inline void* GPUDeviceStorage::Alloc(size_t size, std::wstring name) {
+#else
 inline void* GPUDeviceStorage::Alloc(size_t size) {
+#endif
   void* ret = nullptr;
 #if MXNET_USE_CUDA
+//#if MXNET_MEMORY_PROFILER_ON
+//  #include <mutex>
+//  size_t free_byte_before, free_byte;
+//  std::mutex mtx;
+//  mtx.lock();
+//  cudaMemGetInfo(&free_byte_before, &total_byte_before);
+//#endif
   cudaError_t e = cudaMalloc(&ret, size);
+#if MXNET_MEMORY_PROFILER_ON
+//  if (name==L"warning!,ctx_source_unclear" ||
+//      name==L"untagged")
+//    common::CustomBacktrace();
+//  fprintf(stderr, "[mem] Before allocate, %ld bytes GPU global memory free.\n", free_byte_before);
+  size_t free_byte, total_byte;
+  cudaMemGetInfo(&free_byte, &total_byte);
+  fprintf(stderr, "[mem] Allocate %ld bytes GPU memory (%ls).\n", size, name.c_str());
+  fprintf(stderr, "[mem] %ld MB GPU global memory free, total %ld MB.\n", free_byte/(1024*1024), total_byte/(1024*1024));
+//  mtx.unlock();
+#endif
   if (e != cudaSuccess && e != cudaErrorCudartUnloading)
     throw std::bad_alloc();
+
 #else   // MXNET_USE_CUDA
   LOG(FATAL) << "Please compile with CUDA enabled";
 #endif  // MXNET_USE_CUDA
@@ -72,9 +102,14 @@ inline void GPUDeviceStorage::Free(void* ptr) {
   if (err != cudaSuccess && err != cudaErrorCudartUnloading) {
     LOG(FATAL) << "CUDA: " << cudaGetErrorString(err);
   }
+
 #else   // MXNET_USE_CUDA
   LOG(FATAL) << "Please compile with CUDA enabled";
 #endif  // MXNET_USE_CUDA
+
+#if MXNET_MEMORY_PROFILER_ON
+  fprintf(stderr, "[mem] Release GPU memory.\n");
+#endif
 }
 
 }  // namespace storage
diff --git a/src/storage/naive_storage_manager.h b/src/storage/naive_storage_manager.h
index 731f374..079dc6e 100644
--- a/src/storage/naive_storage_manager.h
+++ b/src/storage/naive_storage_manager.h
@@ -26,6 +26,7 @@
 
 #include "storage_manager.h"
 #include "mxnet/base.h"
+#include "./gpu_device_storage.h"
 
 namespace mxnet {
 namespace storage {
@@ -44,7 +45,11 @@ class NaiveStorageManager final : public StorageManager {
    * \brief Default destructor.
    */
   ~NaiveStorageManager() = default;
+#if MXNET_MEMORY_PROFILER_ON
+  void* Alloc(size_t size, std::wstring name = L"") override;
+#else
   void* Alloc(size_t size) override;
+#endif
   void Free(void* ptr, size_t) override;
 
   void DirectFree(void* ptr, size_t size) override {
@@ -56,9 +61,15 @@ class NaiveStorageManager final : public StorageManager {
 };  // class NaiveStorageManager
 
 template <class DeviceStorage>
+#if MXNET_MEMORY_PROFILER_ON
+void* NaiveStorageManager<DeviceStorage>::Alloc(size_t size, std::wstring name) {
+  return DeviceStorage::Alloc(size, name);
+}
+#else
 void* NaiveStorageManager<DeviceStorage>::Alloc(size_t size) {
   return DeviceStorage::Alloc(size);
 }
+#endif
 
 template <class DeviceStorage>
 void NaiveStorageManager<DeviceStorage>::Free(void* ptr, size_t) {
diff --git a/src/storage/pinned_memory_storage.h b/src/storage/pinned_memory_storage.h
index 69e05f7..7d6dc79 100644
--- a/src/storage/pinned_memory_storage.h
+++ b/src/storage/pinned_memory_storage.h
@@ -39,7 +39,11 @@ class PinnedMemoryStorage {
    * \param size Size to allocate.
    * \return Pointer to the storage.
    */
+#if MXNET_MEMORY_PROFILER_ON
+  inline static void* Alloc(size_t size, std::wstring name);
+#else
   inline static void* Alloc(size_t size);
+#endif
 
   /*!
    * \brief Deallocation.
@@ -48,7 +52,11 @@ class PinnedMemoryStorage {
   inline static void Free(void* ptr);
 };
 
+#if MXNET_MEMORY_PROFILER_ON
+inline void* PinnedMemoryStorage::Alloc(size_t size, std::wstring name) {
+#else
 inline void* PinnedMemoryStorage::Alloc(size_t size) {
+#endif
   void* ret = nullptr;
   // make the memory available across all devices
   CUDA_CALL(cudaHostAlloc(&ret, size, cudaHostAllocPortable));
diff --git a/src/storage/pooled_storage_manager.h b/src/storage/pooled_storage_manager.h
index b2c6633..500182a 100644
--- a/src/storage/pooled_storage_manager.h
+++ b/src/storage/pooled_storage_manager.h
@@ -34,7 +34,7 @@
 #include <new>
 #include "./storage_manager.h"
 #include "../common/cuda_utils.h"
-
+#include "../common/utils.h"
 
 namespace mxnet {
 namespace storage {
@@ -58,7 +58,11 @@ class GPUPooledStorageManager final : public StorageManager {
     ReleaseAll();
   }
 
+#if MXNET_MEMORY_PROFILER_ON
+  void* Alloc(size_t raw_size, std::wstring name) override;
+#else
   void* Alloc(size_t raw_size) override;
+#endif
   void Free(void* ptr, size_t raw_size) override;
 
   void DirectFree(void* ptr, size_t raw_size) override {
@@ -71,7 +75,7 @@ class GPUPooledStorageManager final : public StorageManager {
     used_memory_ -= size;
   }
 
- private:
+    private:
   void ReleaseAll();
   // internal mutex
   std::mutex mutex_;
@@ -86,7 +90,17 @@ class GPUPooledStorageManager final : public StorageManager {
   DISALLOW_COPY_AND_ASSIGN(GPUPooledStorageManager);
 };  // class GPUPooledStorageManager
 
+#if MXNET_MEMORY_PROFILER_ON
+void* GPUPooledStorageManager::Alloc(size_t raw_size, std::wstring name) {
+#else
 void* GPUPooledStorageManager::Alloc(size_t raw_size) {
+#endif
+//#if MXNET_MEMORY_PROFILER_ON
+//  size_t free_byte_before;
+//  size_t total_byte_before;
+//  cudaMemGetInfo(&free_byte_before, &total_byte_before);
+//#endif
+
   std::lock_guard<std::mutex> lock(mutex_);
   size_t size = raw_size + NDEV;
   auto&& reuse_it = memory_pool_.find(size);
@@ -98,6 +112,20 @@ void* GPUPooledStorageManager::Alloc(size_t raw_size) {
 
     void* ret = nullptr;
     cudaError_t e = cudaMalloc(&ret, size);
+
+#if MXNET_MEMORY_PROFILER_ON
+//    if (name==L"warning!,ctx_source_unclear" ||
+//        name==L"untagged")
+//      common::CustomBacktrace();
+//    fprintf(stderr, "[mem] Before allocation, %ld GPU global memory free\n", free_byte_before);
+//    const int MB_size = 1024 * 1024;
+    fprintf(stderr, "[mem] Allocate %ld bytes GPU memory (%ls).\n", size, name.c_str());
+    size_t free_byte;
+    size_t total_byte;
+    cudaMemGetInfo(&free_byte, &total_byte);
+    fprintf(stderr, "[mem] %ld MB GPU global memory free, total %ld MB.\n", free_byte/(1024*1024), total_byte / (1024 * 1024));
+#endif
+
     if (e != cudaSuccess && e != cudaErrorCudartUnloading) {
       LOG(FATAL) << "cudaMalloc failed: " << cudaGetErrorString(e);
     }
diff --git a/src/storage/storage.cc b/src/storage/storage.cc
index fa15a44..b1852e0 100644
--- a/src/storage/storage.cc
+++ b/src/storage/storage.cc
@@ -36,7 +36,11 @@ namespace mxnet {
 // consider change storage as a pure abstract class
 class StorageImpl : public Storage {
  public:
+#if MXNET_MEMORY_PROFILER_ON
+  Handle Alloc(size_t size, Context ctx, std::wstring name = L"") override;
+#else
   Handle Alloc(size_t size, Context ctx) override;
+#endif
   void Free(Handle handle) override;
   void DirectFree(Handle handle) override;
   StorageImpl() {}
@@ -73,7 +77,11 @@ class StorageImpl : public Storage {
 int StorageImpl::num_gpu_device = 0;
 #endif  // MXNET_USE_CUDA
 
+#if MXNET_MEMORY_PROFILER_ON
+Storage::Handle StorageImpl::Alloc(size_t size, Context ctx, std::wstring name) {
+#else
 Storage::Handle StorageImpl::Alloc(size_t size, Context ctx) {
+#endif
   // space already recycled, ignore request
   Handle hd;
   hd.ctx = ctx;
@@ -119,7 +127,11 @@ Storage::Handle StorageImpl::Alloc(size_t size, Context ctx) {
         return ptr;
       });
   this->ActivateDevice(ctx);
+#if MXNET_MEMORY_PROFILER_ON
+  hd.dptr = manager->Alloc(size, name);
+#else
   hd.dptr = manager->Alloc(size);
+#endif
   return hd;
 }
 
diff --git a/src/storage/storage_manager.h b/src/storage/storage_manager.h
index 924d2ed..d06c63d 100644
--- a/src/storage/storage_manager.h
+++ b/src/storage/storage_manager.h
@@ -39,7 +39,11 @@ class StorageManager {
    * \param size Size to allocate.
    * \return Pointer to the storage.
    */
+#if MXNET_MEMORY_PROFILER_ON
+  virtual void* Alloc(size_t size, std::wstring name = L"") = 0;
+#else
   virtual void* Alloc(size_t size) = 0;
+#endif
   /*!
    * \brief Deallocation.
    * \param ptr Pointer to deallocate.
